[{"title": "Registration and Coffee", "location": "Foyer begane grond", "start": "8:30", "end": "9:30"}, {"description": "", "end": "10:50", "subslots": [[{"abstract": "The task of emotion classification has traditionally been addressed using two different ap- proaches: (1) in lexicon-based approaches, a classifier relies on a lexicon that stores, for each word, the emotions the word evokes; (2) in corpus-based approaches, a classifier relies on features extracted from a training set that consists of pairs of sentences and emotion distributions. Both of the approaches have different strengths\u2014lexicon-based approaches typically have a wider coverage of emotion-bearing words, whereas corpus-based approaches learn to use contextual clues. It should not come as a surprise that these approaches have been used jointly, to exploit the strengths of both (Strapparava and Mihalcea, 2008). However, a combination of the two approaches to emotion classification still suffers from the relatively limited size of the available vocabularies. In this work, we introduce a novel variant of the Label Propagation algorithm (Zhu and Ghahramani, 2002) to extend the coverage of an existing emotion lexicon. In order to do so, we construct a fully connected graph wherein words are vertices and the edges are weighted by distributional word representations. The vertices that correspond to words that occur in an emotion lexicon are initialized using the emotion distribution indicated in the lexicon. The label propagation algorithm is then used to derive emotion distributions for words that do not occur in the lexicon. In our experiments, we compare four emotion classifiers: (1) the model of Mohammad and Kiritchenko (2015); (2) a bidirectional LSTM model; (3) a bidirectional LSTM model using an emotion lexicon; (4) a bidirectional LSTM model using the extended emotion lexicon derived through label propagation. Our results show that the model that uses the expanded emotion lexicon consistently outperforms the other models (e.g. it provides an absolute improvement of 6.3 in micro-averaged F1 score compared to Mohammad and Kiritchenko (2015) on the Hashtag Emotion Corpus (Mohammad and Kiritchenko, 2015). Finally, we introduce another variant of label propagation, batched label propagation, that makes label propagation tractable for large vocabularies.", "title": "Semi-supervised emotion lexicon expansion with label propagation", "authors": "Mario Giulianelli and Dani\u00ebl de Kok", "id": 49}, {"abstract": "We discuss the design, development and evaluation of an automated lexical simplification tool for Dutch. Such text simplification tools are useful for a wide range of target populations (e.g. people with an intellectual disability, second language learners, aphasics and children). The ultimate aim of these tools is to adapt texts in such a way that they are easier to read and understand, while maintaining the original meaning as much as possible. To our knowledge, automated text simplification in Dutch has not been researched yet. We use a basic pipeline approach to tackle the problem of lexical simplification. Sentences are first pre-processed (i.e. tokenized, POS tagged and lemmatized) using TreeTagger (Schmid, 1994) and word sense disambiguation is performed using a tool based on support vector machines and trained on the data of the DutchSemCor project (Vossen et al., 2012). This tool links the identified word senses to lexical items in the Cornetto database (Vossen et al., 2013). The difficulty of each token in the input sentence is then estimated using two resources: (a) aggregated data coming from psycholinguistic studies into the average age of acquisition of Dutch words (Brysbaert et al., 2014), and (b) frequency information of Dutch tokens calculated on the basis of a large-scale corpus (of over 1000 million tokens) combining different sources, such as Subtitles2016 (Lison & Tiedemann, 2016), EUBookshop (Skadi\u0146\u0161 et al., 2014), DGT, Europarl and Wikipedia (Tiedemann, 2012), CGN Flemish (Oostdijk et al., 2002) and SONAR500 (Oostdijk et al., 2013). Cornetto is used to identify synonyms of words that have been identified as being difficult. Potential replacements are retrieved, ranked and selected, and a parsed version of the SONAR500 corpus is used to perform reverse lemmatization. The correct inflectional form of the replacement word is selected by matching the Treetagger-tag of the lemma with the SoNaR-tag, and retrieving the corresponding form. A neural language model is used to verify whether the selected replacement word fits the local context. A small development and test set, each consisting of 150 sentences taken from the Flemish newspaper De Standaard, are used to tune the system\u2019s parametersand evaluate its output. In this study, a basic form of human evaluation is performed. Words in the test set are divided into two categories: (a) potentially difficult words, and (b) all other words. It is then verified whether the lexical simplification system changed the appropriate words and whether the proposed changes constitute an improvement in terms of decreasing the degree of difficulty, while maintaining the meaning and respecting grammar rules. We present quantitative results for different versions of the system that focus either on maximizing precision or recall, or on balancing both, and we show how the lexical simplification tool can be integrated in a full-fledged text simplification system for Dutch (Sevens et al., 2017). We expect that the presented research will lead to further developments in the field of augmentative and alternative communication.", "title": "Automating lexical simplification in Dutch", "authors": "Bram Bulte, Leen Sevens and Vincent Vandeghinste", "id": 27}, {"abstract": "Recently several corpora annotated with meaning representations have been developed. Most of this work has been concentrated on English. We present ongoing efforts on building a corpus of Dutch sentences annotated with formal meaning representations. Although there are several resources addressing lexical semantics for Dutch (Postma et al., 2016), as far as we know this is the first effort to produce meaning representations for Dutch that combine predicateargument structure with lexical semantics and typical semantic phenomena such as negation, quantification, modality, and tense. In this paper we outline the method that we use to build the corpus, and present the first results. The method that we employ is based on annotation projection using a parallel English\u2013Dutch corpus (Evang and Bos, 2016). In a nutshell, this works as follows. First we produce a semantic analysis for English with NLP tools tailored for this task (Evang et al., 2013; Lewis and Steedman, 2014; Bos, 2008). Next, taking advantage of automatically aligned words, we produce the syntactic and semantic analysis from English to Dutch. If needed, we manually correct the analysis. The syntactic analysis, both for English and Dutch, is based on Combinatory Categorial Grammar (CCG, Steedman 2001). In order to give a lexical meaning to a word, we use language-independent semantic tagging (Bjerva et al., 2016; Abzianidze and Bos, 2017). Given a word and its lexical CCG category and semantic tag, we produce a lexical meaning in the form of a lambda-expression. As a semantic representation we adopt Discourse Representation Theory (Kamp and Reyle, 1993) with a neo-Davidsonian analysis of events, where the inventory of VerbNet roles (Bonial et al., 2011) provide the thematic roles and WordNet synsets form the predicate symbols. We present a corpus that includes over 300 Dutch documents with gold standard annotations and semantic representations, together with an English counterpart, and in some cases, a German and/or Italian translation as well (http://pmb.let.rug.nl/explorer). Semantic representations of different translations of the same documents can be compared using our semantic matching system D-Match, which detects the degree of overlap between graph structures of meaning representations and calculates standard measures like precision, recall and F1-score. The latter scalar measure can be seen as a complement to a more coarse-grained categorical measure (e.g., equivalence, entailment, neutral, and contradiction) that is based on logical inference over meaning representations. Using the scalar or categorical semantic measure, one can automatically detect potentially bad/good or literal/non-literal translations. During the presentation, we will (i) discuss challenges of the annotation projection from English to Dutch, (ii) show how several semantic phenomena, including quantification, modals, negation and tense, are transferred into semantic representations of Dutch translations and (iii) present semantic similarity scores for several English-Dutch translations.", "title": "Towards Wide-coverage Compositional Semantics for Dutch", "authors": "Johan Bos, Lasha Abzianidze, Hessel Haagsma and Rik van Noord", "id": 65}, {"abstract": "Legal intelligence is a Solr based specialised search engine for Dutch legal professionals, with 70% market share and 100,000+ users. A Google for Dutch lawyers, if you will. Over the years, we have optimised search for our specific group of users by adding domain specific features. This abstract highlights some of these features thereby shining a commercial light on academic topics. Good legal search starts classically by building an inverted index based on appropriate lemmatization and the application of thesauri while indexing and searching, using a combination of domain driven text analysis and query parsing/expanding. Using a set of data driven regular expressions that accounts for writing variations beyond official guidelines 3 , Legal Intelligence recognizes topics from the legal vocabulary, synonyms of courts, references to law articles, legal publications,\u200b \u200band\u200b \u200bmany\u200b \u200bothers.\u200b \u200bThis\u200b \u200bapproach\u200b \u200bresults\u200b \u200bin\u200b \u200ba\u200b \u200bwell\u200b \u200bbalanced\u200b \u200brecall\u200b \u200band\u200b \u200bprecision. For any commercial product, performance and agility are key. As such, the application of thesauri while indexing and searching is based by a custom query parser and analysis chain, including a token filter that takes care of advanced Finite State Transducer based thesaurus matching 4 . The talk discusses some unexpected challenges while implementing. In addition, we will also explain how we are using Solr\u2019s function queries in order to give our users the best possible ranking of search results. For agility goals, the talk will also address our approach to test driven development 5 accounting for an automated test suite for these search related features maintaining\u200b \u200bthe\u200b \u200bquality\u200b \u200bof\u200b \u200bsearch\u200b \u200bwhile\u200b \u200benabling\u200b \u200brapid\u200b \u200bdeployments\u200b \u200bat\u200b \u200bthe\u200b \u200bsame\u200b \u200btime. But search is never stable. New content comes in every day and more search techniques becomes available at each dawn. Therefore Legal Intelligence looks beyond the horizon of excellent search. Therefore this talk will also address two machine learning approaches. The first addresses meta-data enrichment of documents, specifically using supervised classification to add law areas to legal documents. Having such additional meta-data is quintessential for faceted search and alerting mechanism on law area. Using unsupervised learning, the second approach addresses a capability to filter out non legal relevant documents. Known as \u2018the Garbage Collector, this has become an important requirement since clients prefer to search through their own content. But often, the internal content is messy and also including invoices, progress reports\u200b \u200band\u200b \u200beven\u200b \u200bstaff\u200b \u200bassessment\u200b \u200breports\u200b \u200bwhich\u200b \u200ball\u200b \u200blead\u200b \u200bto\u200b \u200bnoise\u200b \u200bin\u200b \u200bsearch\u200b \u200bresults.", "title": "Lawyers that Search? Thesaurus driven legal search using custom analysis and query parsing and beyond", "authors": "Tjerk de Greef and Pieter van Boxtel", "id": 36}, {"abstract": " ", "title": "Overview and short presentations", "authors": "Shared Task", "id": 101}], [{"abstract": "While common sense and connotative knowledge come natural to most people, computers still struggle to perform well on tasks for which such extra-textual information is required. Automatic approaches to sentiment analysis and irony detection have revealed that the lack of such world knowledge undermines classification performance. In this paper, we therefore address the challenge of modelling implicit or prototypical sentiment in the framework of automatic irony detection. Starting from manually annotated connoted situation phrases (e.g. flight delays, sitting the whole day at the doctor's office), we defined the implicit sentiment held towards such situations automatically by using both a lexico-semantic knowledge base and a data-driven method. We further investigate how such implicit sentiment information affects irony detection by assessing a state-of-the-art irony classifier before and after it is informed with implicit sentiment information.", "title": "Using common sense to detect irony on Twitter", "authors": "Cynthia Van Hee, Els Lefever and V\u00e9ronique Hoste", "id": 48}, {"abstract": "In today\u2019s digital age, people with limited reading and writing skills have trouble partaking in online activities. Not being able to access or use information technology is a major form of social exclusion. We present a Pictograph-to-Text translation system for people with an intellectual disability. It provides help in constructing Dutch textual messages, by allowing the user to input a series of pictographs, and translates these messages into natural language text. The main challenge in translating from pictograph languages to natural language text is the fact that a pictograph-for-word correspondence will almost never provide an acceptable output. Pictographs are underspecified, both semantically and grammatically. In the second place, the pictograph input to translation could be ambiguous and unpredictable with respect to pictograph order. Our baseline system for Pictograph-to-Text translation (Sevens et al. 2015) generates natural language from pictographs using language models and does not use any grammatical information in the translation process. When a pictograph is selected, its connected WordNet synset is retrieved, and from this synset, the system retrieves all the synonyms it contains. For each of these synonyms, reverse lemmatisation is applied. The reverse lemmatiser retrieves the full inflectional paradigm of each lemma. Each of these surface forms is a hypothesis for the language model. We propose two types of language models. In our n-gram-based approach, the system performs beam search decoding on an n-gram language model (n\u22645), trained with the CMU toolkit (Clarkson & Rosenfeld 1997) on a Dutch corpus of over 1100M tokens. In our Long Short-Term Memory-based approach, we train a language model with Tensorflow (Abadi et al. 2016) on the Flemish part of the CGN corpus (3.8M tokens) (Oostdijk et al. 2002) and re-rank the natural language hypotheses. The evaluations of the baseline system show that using language models for finding the most likely combination of textual representations is already an improvement over the initial baseline (i.e., pictograph file names), but there is ample room for improvement. In recent experiments, we apply machine translation techniques. Since a parallel corpus of pictograph sequences and well-formed written Dutch text is not available, we explore different approaches toward the creation of a suitable parallel corpus. In our first approach, we automatically translate a large corpus of monolingual Dutch SoNaR subtitles (27.6M tokens) (Oostdijk et al. 2013) into pictographs using the Text-to-Pictograph translation tool (Vandeghinste et al. 2015). In our second approach, we lemmatise the subtitle corpus, and remove all words that are not content words, thus creating a source language corpus that resembles pictograph input. Our phrase-based statistical machine translation approach toward Pictograph-to-Text translation uses the Moses decoder (Koehn et al. 2007), while our neural machine translation approach makes use of the open-source system OpenNMT (Klein et al. 2017). We build different models using a variety of training conditions, including factored models that include part-of-speech and lemma information, and evaluate all systems using automated metrics and human evaluations (adequacy, fluency, and ranking). Our first experiments indicate that the machine translation approaches outperform the baseline system.", "title": "Pictograph-to-Text Translation for Augmented and Alternative Communication", "authors": "Leen Sevens, Vincent Vandeghinste, Lyan Verwimp, Ineke Schuurman, Patrick Wambacq and Frank Van Eynde", "id": 9}, {"abstract": "Within the practice of Topic Modeling, an often heard complaint is the difficulty and inherent subjectivity of evaluating the appropriateness of topics for the document collection they model. On the other hand, word embeddings have proven their worth discerning semantic coherence of a given vocabulary, giving rise to the famous examples of word analogies found without previous world knowledge. The research question we want to address in this paper is to what extent word embeddings can help to discern concepts automatically in topics, and how they can place interpretational boundaries which decreases the subjectivity of interpretation. For this, we start our exposition by giving a short overview of both topic modeling and word vectors, their assets and drawbacks, and show in which way they could be of mutual benefit to each other. Next, we present the results of filtering the keyword clusters found by Latent Dirichlet Allocation, the most widely used topic modeling algorithm, by two sets of pre-trained word embeddings, Word2Vec and GloVe respectively. The corpus serving as case study is a collection of economic documents of the European Commission, spanning from 1958 to 1982. We find that some topics correspond to concepts found by the word embeddings, but that most are collocations of two or more concepts. In this way, we show how topic modeling is a suitable method for extracting the elements of what a text is about, and that word embeddings add conceptual structure to this. We use the embeddings in two ways, one is to find the most related word vectors to a given word, to indicate in which topics closely related words can be found. The other method starts from the topics themselves and puts a hierarchical order on the terms by iteratively extracting the word which according to its vectorial representation has the least features in common with the other words. To round off, we present an overview of the benefits of combining the word of topic modeling with that of word embeddings, rather than resorting to either of both.", "title": "From Topic Modeling to Concept Identification. The Added Value of Word Embeddings to Discern Semantic Coherence in Keyword Clusters.", "authors": "Mathias Coeckelbergs", "id": 24}, {"abstract": "Popularity plays an important role in ranking search results, especially for general interest, high-frequency queries: the more a search result is clicked on by users, the higher it will be ranked in new searches. But when the information searched becomes more specific, popularity becomes less important and less useful as a ranking criterion. Less important because for highly specific queries, relevance is more searcher- specific than for general-domain search; less useful because the amount of click data available from other users is limited for specific search queries. An alternative for search tasks addressing highly specialized topics is to employ the user\u2019s own data for result ranking. Academic search is a key example of domain-specific search. Although academic search is generally defined as a recall-oriented task, precision is also an issue due to ambiguity of search queries \u2013 query terms commonly have different mean- ings across scientific domains. Consider for example the term \u2018search behaviour\u2019, which can refer to (according to Google Scholar): prey search behaviour, job search behaviour, the search behaviour of soccer goalkeepers, and of course information search behaviour. A large body of previous work exists on user profiling in Information Retrieval, but relatively little work addresses personalized academic search. Approaches to user profiling and personalization often incorporate ontological information from a reference ontology. An alternative is to collect a set of documents that are known to be relevant to the user, and generate a user profile from those documents. We propose an academic user profile that consists of topic-specific terms, stored in a graph. Storing the profile as a graph has a number of advantages. First, it is flexible with respect to other node types and relation types to be added. Second, it is a visualization of knowledge that is interpretable by the user. Third, it offers the possibility to view relational characteristics of individual nodes. In the presentation we explain how we generate the graph model from texts authored by the searcher. We show what the proposed personal graph model looks like for academic authors and how the model can be used to to re-rank the scientific publications retrieved by a search engine. We found that re-ranking with the personal graph gives a small but significant improvement over the baseline ranking. In future research, we will address the extension of the author\u2013topic graph with other types of relational information, such as author and conference/journal nodes, citation relations between documents, and behavioral data such as queries and clicks on documents.", "title": "User profiling with personal graphs", "authors": "Suzan Verberne", "id": 21}, {"abstract": " ", "title": " ", "authors": " ", "id": 102}], [{"abstract": "When producing text, people often use shorter or alternative linguistic structures to refer to previously mentioned elements. Though mostly short text is being produced online, people use the same technique when venting opinions on various aspects of a certain product or experience. Whenever discourse entities are mentioned more than once and appear in different discourse contexts they are said to corefer. While it has been claimed that anaphora or coreference resolution plays an important role in opinion mining, it is not clear to what extent coreference resolution actually boosts performance, if at all. In this presentation, we investigate the potential added value of coreference resolution for the aspect-based sentiment analysis of restaurant reviews written in Dutch. We focus on the task of aspect category classification and investigate whether including coreference information prior to classification to resolve implicit aspect mentions is beneficial. Because coreference resolution is not a solved task in NLP, we rely on both automatically-derived and gold-standard coreference relations, allowing us to investigate the true upper bound. By training a classifier on a combination of lexical and semantic features, we show that resolving coreferential relations prior to classification is beneficial in a joint optimization setup on the training data. When validating these optimal models, however, the system is able to achieve a satisfying performance on a held-out test set, regardless of whether coreference information was included or not.", "title": "OMG het was zooo lekker! Can fine-grained sentiment analysis benefit from coreference resolution?", "authors": "Orph\u00e9e De Clercq and Veronique Hoste", "id": 59}, {"abstract": "Natural Language Generation (NLG) is especially interesting for companies with structured data and the need for large amounts of texts or regularly updated texts based on this data. Another important field of application is the demand to provide multilingual content that is usually solved by translation of text. We present a rule-based NLG system with a cross-language abstractional layer that makes it feasible to write texts in multiple languages (currently in 22 languages) at the same time. This cross-language abstraction works independently from data source language or grammatical differences in the output. The system is controlled by a language agnostic markup language that is used on our SaaS platform to generate texts. This markup can be easily edited by a technical editor in the web browser without coding skills or previous knowledge in computational linguistics. In the talk we would like to present our solution for grammatical agreement exemplary for Dutch. One of our clients is able to generate 250.000 different texts about holiday homes in German and Dutch with the same configuration.", "title": "Content Generation in German and Dutch from Structured Data", "authors": "Andreas Madsack and Johanna Heininger", "id": 40}, {"abstract": "Knowledge acquisition is considered as one of major bottlenecks for systems that attempt to automatically detect semantic relations between natural language sentences (Dagan et al., 2013, p. 7). One can employ lexical knowledge resources, e.g., WordNet (Miller, 1995) or FrameNet (Baker et al., 1998), but they are never enough. On the other hand, rule-based textual entailment systems are brittle since a single error in a meaning representation, a missing inference rule, or a lack of piece of lexical knowledge can lead to the failure of capturing a correct semantic relation. In this abstract, I use a rule-based, tableau theorem prover for natural language, called LangPro (Abzianidze, 2015, 2017), and the recognizing textual entailment (RTE) dataset SICK (Marelli et al., 2014), to show how lexical knowledge can be automatically induced from a training set with the help of proof trees built by the prover. For this task we choose the SICK dataset as its problems heavily depend on lexical knowledge. The tableau prover builds proof trees that try to refute a certain semantic relations. Refutation fails if there are no open branches (i.e., all constructed situations are impossible), otherwise refutation succeeds and an open branch in the tableau serves as a counterexample for the semantic relation. For example, the tableau in Figure 1a tries to refute that someone is holding a hedgehog is not entailing someone is holding a small animal ( SICK -4974): it starts to find a situation where the premise 1 is true (T) and the conclusion 2 is false (F). The branch (in blue) is not closed since the prover doesn\u2019t have the information that hedgehog is small animal (see 4 and 8 ). Hence it succeeded to refute the entailment as it found an open branch and failed to classify the problem as entailment. But if the abductive reasoning\u2014inference to the best explanation\u2014is used, based on the open branch in Figure 1a, it is straightforward to identify that the lexical knowledge, hedgehog entailing small animal, is sufficient to close the branch and hence correctly classify SICK -4974 as entailment. Whenever a problem like SICK -4974 occurs in a training set, based on its gold entailment label, LangPro can build a tableau that would have all branches closed if there were no missing lexical knowledge. Then induction of lexical knowledge can be reduced on finding a piece of information that causes all branches of the tableau to close. Let\u2019s explain the latter scenario on the non-trivial entailment problem: ( SICK -5397) a man is removing some food from a box vs the man is putting chicken into the container. Given the gold label CONTRADICTION of the problem, the prover can build the tableau that attempts to refute the contradiction relation in SICK -5397. The built tableau need to have all branches closed, i.e., refutation of the contradiction relation must fail, to comply with the gold label. For this concrete example, the automatically built tableau is open, where the nodes in the only open branch are shown in Figure 1b. Given that chicken entails food based on WordNet, c and f entities can co-refer, which gives us an opportunity to close the branch due to 4 and 8 based on the assumption that put (into) and remove (from) are semantically inconsistent. During the presentation it will be shown to what extent the induced knowledge improves the results of the prover over SICK.", "title": "Lexical Knowledge Acquisition with Theorem Proving", "authors": "Lasha Abzianidze", "id": 82}, {"abstract": "Automatic thesaurus generation is a desired technique for the reason that a thesaurus is a useful tool in NLP, but manually making a thesaurus is expensive and time-consuming. In this project, the process of thesaurus generation was divided up in two parts: term extraction and relation extraction. Term extraction being the process of automatically finding candidate terms for a legal thesaurus and relation extraction is the process of finding which terms are hypernyms of each other. For term extraction different termhood measures are used: Log Likelihood, Kullback Leibler Divergence and the measure as assigned by the TExSIS tool. For relation extraction, different classifiers are trained to classify whether two terms have a hypernym-relation based on word embeddings. In this presentation, I will briefly present my methods and results. I will also discuss the challenges for NLP in the legal domain compared to challenges in other domains for example the biomedical domain. During this research, I experienced first hand that methods from other domains can not be adopted to the legal domain, for the main reason that Law is a domain that differs largely per country. On the contrary, the legal domain provides with great opportunities, for example because large quantities of text are freely available.", "title": "Methods For Automatically Generating a Legal Thesaurus and discussing NLP in the Legal Domain", "authors": "Hugo de Vos", "id": 51}, {"abstract": "May 2018, the new personal data protection \u2013privacy! \u2013 rules (GDPR) will become valid in the EU and all its member states. On the other hand, making your research data available as Open Data is becoming more and more important, for example when applying for a grant (NWO, FWO, EU, ...). Not making them \u2019Open\u2019 is to be justified very well.      What does this mean for linguistic research? Especially when dealing with children, elderly people, people with an intellectual disability, migrants, ... . Or with social media? And what when someone decides that his or her contribution to a corpus can no longer be used? Thus: How are GDPR and Open Data to be reconciled? How are you dealing with such issues? Any tips, tricks, ideas, solutions? Or more topics that should be taken care of? *** One of the pros for all of us of working with Open Data, is that data      will be reusable. For example, we ourselves are working on data written by people with an intellectual disability (ID), data with lots of specific errors (spelling, grammar), or just consisting of pictos. These data can only be used by us. That\u2019s a pity, as it is difficult to get such data, especially digital, for example from social media. Some issues we would like to discuss with you in this session, esp. when you are working with user-generated data (social media, web forums, online reviews, ...). * in general, how would you ask permission from everybody involved? Depending on the type of resource, people may be using nicknames, ... * how would you deal with users with some communicative issues, in general or wrt the language in question (apply text normalization, translation, ...? If so: on request or in general?) * and what with informed consent in such, from the user and/or a carer, guardian, ... And what about the ethical aspects? * how would you act when people recall their permission? * in such a case: what about older versions of your data already made available to other reseachers? * which metadata would you be make available to other researchers? All, or ...? Would you allow them to see the real names, etc? We presume that all people appearing in the data collection will be anonymized or pseudonymized (unless the names of public figures are involved.) !! We will try and get answers to issues arising during this session after\u00a0CLIN 2018 (for example during our ISI-NLP 2 workshop at LREC).", "title": "Open Data, Social Media and other User Generated Text: some topics for a discussion session", "authors": "Ineke Schuurman, Leen Sevens, Vincent Vandeghinste", "id": 100}], [{"abstract": "We present a case study on the categorization of Twitter messages that refer to vaccination, a much-discussed topic on social media. In order to assist the Dutch Rijksinstituut voor Volksgezondheid en Milieu (RIVM) in monitoring this discussion, we aimed to build a system that can detect the stance towards vaccination in these messages. Especially a rapid spread of rumours about the negative effects of vaccination can pose a threat to this institute, which is responsible for the Dutch vaccination program and thereby has interest in a wide vaccination coverage. A system that can detect a sudden increase of messages with a negative stance can alarm the institute and enable them to take timely action if the information that is shared seems to be based on falsive information. Identifying the stance towards vaccination in messages is comparable to identifying the polarity of a message, in the sense that messages are flagged as positive, negative or neutral. However, a tweet that conveys a negative stance towards vaccination might be positive in sentiment and vice versa. Hence, off-the-shelf tools for sentiment analysis do not seem applicable to this problem. Alternatively, we collect Twitter messages that mention a vaccination-related key term and have them annotated by means of three categorizations. First, the annotator is asked whether a message is relevant or irrelevant. This is an important distinction, as the word 'vaccin' might be used as a metafor in a different context or refer to animal vaccination, making the tweet irrelevant. Second, the annotator is asked whether the tweet conveys a positive, negative or neutral stance towards vaccination, or whether the stance is unclear to the reader. Third, the tweet is categorized into one of the fine-grained sentiments 'frustration', 'concern', 'fear', 'information' and 'other'. This latter categorization might assist the detection of tweets with a negative stance. The resulting categorizations are used to train and test a machine learning classifier on detecting tweets with a negative stance. We experimented with a strict and lax usage of the annotated tweets, as well as different categorizations and classifiers. We will present an overview of the different approaches and compare their predictions to those of an off-the-shelf sentiment analysis tool. Based on these outcomes, we will discuss the challenges of the task and avenues for future work.", "title": "Monitoring stance towards vaccination in Twitter messages", "authors": "Florian Kunneman, Liesbeth Mollema, Antal van den Bosch, Mattijs Lambooij, Albert Wong and Hester de Melker", "id": 74}, {"abstract": "Recurrent neural networks have established themselves as a state of the art method for language modeling. Advanced architectures, such as LSTM-style networks, are able to precisely model the sentence\u2019s history in order to capture long-range dependencies. These recurrent neural network models, however, are less suitable for topically coherent text generation: they are able to generate grammatically correct text, but are unable to properly modulate the topical content. In this research, we investigate whether a recurrent neural network architecture might be augmented with topical information from a non-negative matrix factorization model (Lee and Seung, 1999), in order to generate text that is syntactically sound as well as topically coherent. The model\u2019s results are evaluated quantitatively as well as qualitatively, and compared to recent neural network architectures that aim to incorporate topical information using different means (Cao et al., 2015; Lau et al., 2017).", "title": "Generating Topically Coherent Text with Recurrent Neural Networks", "authors": "Tim Van de Cruys", "id": 54}, {"abstract": "Automatic extraction of definitional sentences from text content is often useful in many other tasks such as ontology generation, question answering, and building glossaries and dictionaries. Definition classification can be modelled as a supervised sequence classification problem, and sequence-learning neural networks (namely Long Short-term Memory Networks or LSTMs) have been used successfully to perform this task. In this paper, we propose a simple multi-view extension to the network architecture, to combine the learnings from raw text as well as its corresponding part-of-speech sequence, for both LSTMs as well as Convolutional Neural Networks (CNNs), showing that such a simple multi-view fusion mechanism consistently learns better than a single-view network, as well as furthers the current state-of-the-art F1-score by 5.8%, by obtaining the highest F1-score of 97%.", "title": "Classifying Definitional Sentences using Multi-view Neural Network Architectures", "authors": "Subhradeep Kayal, Sameer Chivukula and Sophia Katrenko", "id": 75}]], "title": "Parallel session no. 1", "sessions": [{"chair": "Veronique Hoste", "title": "Emotion and Sentiment", "location": "Kleine zaal"}, {"chair": "Mari\u00ebt Theune", "title": "Language generation", "location": "Leeuwzaal"}, {"chair": "", "title": "Semantics", "location": "Keizer Karel Foyer"}, {"chair": "", "title": "IR and Knowledge Extraction ", "location": "Soci\u00ebteitskamer"}, {"chair": "", "title": "Shared Task and Discussion on Open data", "location": "Annazaal"}], "start": "9:30"}, {"title": "Coffee Break", "location": "Foyer begane grond", "start": "10:50", "end": "11:10"}, {"title": "Welcome", "location": "Kleine zaal", "start": "11:15", "end": "11:25"}, {"title": "STIL Thesis Prize", "location": "Kleine zaal", "start": "11:25", "end": "11:35"}, {"subslots": [[{"abstract": "Machine Translation (MT) has been used by many people for some time now as a productivity tool, with demonstrable success. Recently, a new paradigm -- Neural MT (NMT) -- has emerged as a contender to replace statistical MT as the new state-of-the-art in the field. While there is no doubt that NMT has enormous potential, we argue that this has been overhyped, a situation which if left unchecked will lead to unrealistic expectations of its capabilities and ultimately to a host of disappointed users. We will demonstrate that this has been seen before with the advent of new approaches to MT, and each time this happens it has the potential to undermine the relationship MT developers need to have with translators, the principal users of the technology. Despite overblown claims regarding its prospects, we argue \u2013 as we have done for many years now \u2013 that the translation community has little to fear from MT, and that where human input is required, the translator remains the most important link in the chain. Indeed, MT developers rely on translators in a number of ways, and if MT is to improve still further, bilateral partnerships need to be formed between both communities for such advances to be made as fast as possible.", "title": "Neural MT: Separating hype from the reality", "authors": "Andy Way", "id": "98"}]], "title": "Martha Larson (Invited speaker)", "location": "Kleine zaal", "start": "11:35", "end": "12:40"}, {"title": "Lunch", "location": "Foyer begane grond", "start": "12:40", "end": "13:30"}, {"subslots": [[{"abstract": "As we all know, text on Twitter (and other social media) contains words that an innocent observer would not expect to be spelled in the way it is spelled there. Some of the variants can clearly be classified as errors. When typing quickly, it is unavoidable that one mistypes one word or another (typographical errors). And, given the enormous number of authors, one can also expect that the proper spelling is sometimes not known (orthographical errors). But here, we already have to be careful in our judgement. More frequent than the spelling errors are deliberately chosen spelling variants, and some of these are phonetic spellings (possibly for confirmation of group membership), which tend to have the same form as orthographical errors. Other deliberate variants include shortened forms and jargon, in reaction to the limit of 140 characters and sometimes derived from SMS), and repeated characters (or longer strings) for emphasis, as a kind of written intonation. For any kind of automatic processing of tweets, if one does want to include the ones containing spelling variants, it would be necessary to somehow deal with all the variant forms. Generally, variants can be recognized in two ways. First, they are related in form to the standard spelling. For spelling errors, there are several traditional methods available; for the variation on Twitter we have extended these with the Viterstein algorithm (van Halteren and Oostdijk, 2012). Second, and we focus on this in our poster, the variants can be expected to occur in the same contexts as the standard spelling (distributional semantics). We have taken several years of Dutch tweets from the TwiNL collection, and derived word similarity measures from these for the about 127,000 most frequent tokens, following several approaches. Our first approach was a traditional corpus linguistic one. We took a random sample for each token, consisting of 2,000 tweets containing the token. We counted context words within each sample, both using a bag-of-words (for semantic similarity) and bi/trigrams around the focus word (for syntactic similarity), and calculated pointwise mutual information to derive a vector for each token. The second approach used the recently developed tools word2vec and GloVe to create vectors on the basis of the complete tweet collection. Vector similarity measures on these vector collections now allow us to measure the similarity between tokens. For the purpose of identifying spelling variants, the syntactic similarity appears to be the most promising, as the variant forms seem to cluster better there.", "title": "Spelling variation in Dutch tweets", "authors": "Hans Van Halteren and Nelleke Oostdijk", "id": "88"}, {"abstract": "That speakers can vary their speaking rate is evident, but how they accomplish this has hardly been studied. The effortful experience of deviating from one's preferred speaking rate might result from the invocation of executive control (EC) processes to modulate the formulation phase of speech planning, namely lexical selection and phonological encoding. A promising way to explore the mechanisms of this control is via simulation of computational models of this formulation phase, and to explore how the parameter sets of these models can be optimised to fit the temporal characteristics of behavioural data as present in the speech signal. We present and test a connectionist model derived from Dell, Burger and Svec\u2019s (1997) model of serial order in language. The model sequentially retrieves syllable-level motor plans in response to activation in a word plan input node. Serial order is achieved by means of a competitive queue embedded in a frame node. The frame nodes might be thought of as encoding metrical structure of words. In our simulation, the model returns four \u2018event times\u2019: the onset and offset times of activation of first and second syllables in disyllabic words. From these event times, we calculate three durations on which the model is tested: the duration of the first syllable, the duration of the second syllable, and the duration of overlap between the two. The model was trained by allowing it to estimate parameter values necessary to approximate the distributions of syllable and overlap durations across words in a Dutch corpus of picture naming in slow, medium and fast cued speaking rate conditions. The corpus contains word and syllable onset and offset times for on average 3,754 disyllabic word tokens in each rate condition. The model learning was performed by a natural selection-inspired algorithm, NSGA-III, which can solve multi-objective problems efficiently. Separate runs of 600 generations were conducted to find optimal parameter settings for each rate condition independently. The fitness of each set of parameter values was evaluated by running the model fifty times, with its parameters set to values sampled from normal distributions defined by the parameter values of the agent. We thus obtained fifty-member distributions for each of the duration objectives, which were compared with the distributions from the corpus by calculating the Kullback- Leibler divergence. The model learnt to fit the data effectively in all the rate conditions, achieving similar Kullback-Leibler divergence scores for each rate (median KL in well-fitting evaluations after 300 generations = 4.98). In preliminary analysis, clear differences emerged in parameter values associated with simulating each rate. These findings suggest the presence of a parameter complex that may be equated with a psychological mechanism controlling speaking rate during the formulation phase of speech production.", "title": "A connectionist model of serial order applied to speaking rate control", "authors": "Joe Rodd, Hans Rutger Bosker, Mirjam Ernestus and Louis Ten Bosch", "id": "87"}, {"abstract": "The computational modelling of dialogue and conversation has been subjected to a paradigm shift in recent years, with an increased focus on data-driven approaches, in particular those based on neural techniques. Although an important force driving this move towards more data-driven methods has been the increased availability of conversational data, publicly available datasets of considerable size containing open-domain dialogue are few. In addition, ad- dressing non-linguistic aspects of conversation, for example contextual factors such as the identification of speaker and addressee (Li et al., 2016), has been shown to have potentially favourable effects on the quality of a (generative) dialogue model. Ergo, we describe an ongoing effort to provide a large annotated dataset consisting of (topically) unrestricted conversations collected from social media platform Reddit for the purpose of data-driven dialogue modelling. Reddit, the self-proclaimed front page of the internet, is a content aggregator and discussion forum. Reddit users can submit content and links to content found elsewhere on the web; these submissions are discussed, with the discourse that follows organized as threaded conversations. From these discussion threads it is possible to extract topically-diverse (open-domain) dialogue spanning multiple turns, both dyadic as well as multi-participant. Our dataset contains more than one billion unique utterances posted to Reddit since January 2015. For each of the comments in this dataset, we provide relevant metadata and automatically generated annotations that may aid the modelling of (non-linguistic) contextual factors. Moreover, we argue that, although Reddit offers an abundance of potentially valuable content to be used for dialogue modelling purposes, it remains important to have a profound understanding of the data and the modelling domain in question. In this regard, in addition to descriptive statistics, we provide a more in-depth analysis of the data contained within the dataset and the possible biases our dataset may exhibit, for example as a result of Reddit\u2019s demographically unbalanced user base.", "title": "Reddit as a Corpus for Data-Driven Dialogue Modelling", "authors": "Bram Willemsen", "id": "86"}, {"abstract": "Unraveling linguistic elements that contribute to a conversational human voice in online dialogues between organisations and stakeholders Nowadays, stakeholders can voice their complaints, compliments, and questions on social media, such as Twitter. This phenomenon is referred to as \u2018electronic Word of Mouth (eWOM). Therefore, organisations actively search consumers\u2019 feedback on social media and try to engage in a conversation with them (i.e., webcare). For example, The Royal Dutch Airlines receive over 100,000 stakeholder messages per week, which makes it undoable to respond all of them manually (Van Manen, 2017). Thus, organisations need automated tools to support them in their online conversations with the stakeholders. An important factor affecting the effectiveness of webcare is the communication style organisations adopt, which is referred to as the Conversational Human Voice (CHV, Kelleher, 2009; Kelleher & Miller, 2006). CHV is a personal and engaging communication style from organisation to stakeholder that reflects attributes, such as being open to dialogue, using emotion and humor, and providing prompt feedback. Research has shown that organisations\u2019 responses containing CHV increase customers\u2019 evaluations of the brand, its reliability and reputation (e.g., Van Noort & Willemsen, 2012; Schamari & Schaefers, 2015). However, contrary effects have been found as well (e.g., Crijns, Cauberghe, Hudders & Claeys, 2017; Gretry, Horv\u00e1th, Belei & van Riel, 2017) which may have been caused by the divergent interpretations of the concept CHV which allows various operationalisations. In this project (funded by a NWO KIEM grant for creative industries) we aim to develop an automated tool that stimulates online conversations with a personal and engaging communication style between organisations and stakeholders. In order to achieve this goal, our first step was to unravel the linguistic elements that contribute to a conversational human voice that can be identified reliably in webcare conversations. An extended literature review was conducted on the most relevant papers from the fist mention of the concept CHV until now (2009-2017), resulting in an identification instrument that characterizes conversational human voice on the basis of three main categories: personalisation (such as greeting the customer), informal language (such as using abbreviations) and invitational rhetoric (such as showing sympathy). Subsequently, we tested the reliability of the identification instrument with a manual corpus analysis of 480 webcare dialogues from Twitter between Dutch municipalities and their stakeholders. Inter coder reliability showed that the identification instrument turned out to be reliable which implies that humans can reliable identify linguistic elements that contribute to the conversational human voice. The next step is to test whether computers are able to identify these linguistic elements reliably which shall be done in collaboration with OBI4wan, a company that develops social media monitoring tools.", "title": "Towards an automatic identification of tone of voice in online conversations", "authors": "Christine Liebrecht and Charlotte Van Hooijdonk", "id": "84"}], [{"abstract": "Sentence representations are a powerful method of extracting fixed-width vectors from sentences, and are useful in domains ranging from machine translation to image retrieval. Evaluating the qualities of sentence representations is often done by employing the representations in standardised semantic prediction tasks. Syntactic properties are to a lesser extent subject to evaluation. Therefore, one model to generate sentence representations has been evaluated; the skip-thoughts model (Kiros et al, 2015). The experiments in this research indicate that syntactic properties are present in skip-thought vectors. Logistic regression trained on the hidden states of the skip-thoughts model is capable of classifying grammatical categories. The trained classifier achieves a test set accuracy of 82.18% in predicting dependency tags. To isolate the accuracy that could be achieved by morphology of words alone, performance has been compared to a similar classifier trained on word embeddings. Compared to this classifier trained on word embeddings, the classifier trained on hidden states achieves a Relative Error Reduction of 40.80% for the classification of dependency tags. The more the grammatical categories depend on the sentence structure, the larger the improvement of the hidden states model over the word embeddings model. Relative Error Reduction in the less syntax-dependent task of predicting part-of-speech tags is lower (23.79%) than in the more syntax-dependent task of predicting dependency tags (40.80%). Furthermore, Relative Error Reduction metrics are higher for grammatical categories that contain more ambiguous words (e.g. nouns and verbs). To estimate the sensitivity of the skip-thoughts model to grammatical sequences, a second prediction task is proposed. In this task accuracy is compared to the classification accuracy that can be obtained by concatenating word embeddings of previous, current and subsequent words in a sentence. [Results of this experiment are still pending.] Despite the lack of word isolation in the hidden states of the skip-thoughts model, predicting grammatical categories has proven to be a robust method of evaluating syntactic properties present in sentence representations. The experimental setup that has been developed can be used to evaluate syntactic properties of other models of obtaining sentence representations, such as the recently proposed CNN-LSTM model (Gan et al, 2017). Future work could focus on standardising an evaluation task, comparable to the tasks that are used in evaluation of semantic properties.", "title": "Syntactic properties of skip-thought vectors", "authors": "Bart Broere", "id": "81"}, {"abstract": "Nowadays, there is a wealth of language resources available annotated in various ways and used to benchmark state-of-the-art research. This rich collection is continuously expanding; keeping track of all newly developed resources and extensions to existing ones is at the same time exciting and challenging. Initiatives such as the Open Language Archives Community, Language Grid, the LRE Map, the ELRA Universal Catalog and the LDC Catalog aim to monitor and promote the use and creation of resources by identifying, describing and distributing them to the research community, making it relatively easy to be informed about individual resources and to obtain them. However, it can still be a hard task to find out how all these resources relate to each other in terms of content (i.e. the documents, sentences, tokens) and annotations. In terms of content, text corpora come into being and evolve through creation (using a set of documents that has not been released elsewhere), selection (selecting documents from existing corpora) and extension (adding an additional set of documents), with selection and extension resulting in content overlap between corpora. In terms of annotations, many resources use similar or complementary frameworks, which have often developed in an organic way as well. There is much to learn and gain from these corpus networks, especially if corpora (partially) overlap in both content and annotations. From a practical point of view, corpora could be enriched by merging complementary annotations. From a theoretical point of view, comparing annotations from similar frameworks (on the same documents) can reveal interesting differences in conceptualization. However, the opportunities for analyzing and merging corpora are currently hampered by a wide range of problems for data sharing. For example, difficulties in tracking the origin of documents arise when they go through several stages of selection and extension, or when corpora use different naming conventions for their files. In addition, alignment of annotations often requires solving mismatches in sentence and/or token identifiers, which may have various causes and are not always trivial to solve. And after solving the identity and provenance of the text documents and identifier mismatches, there is an even harder problem of interoperability: to align the annotations and their interpretations across the data. After all, a label (e.g. \u2018event\u2019) used in one annotation framework does not necessarily represent the same concept as that label in another framework. We outline some of the most common sources of interoperability problems and describe two routes for analyzing data: 1) token-based comparison and 2) type-based comparison. The former requires full interoperability across the content representation of the data, whereas the latter represents an alternative in case there is no (full) content compatibility. We apply these strategies to a set of English text corpora overlapping in content and annotation (events/predicates). In addition, we present a unique visualization of the resources providing insight in their document intersections. Our aim is to make the community aware of the challenges and opportunities and to inspire finding ways to collaboratively improve resource interoperability.", "title": "Content and Annotation Compatibility Across Corpora: The Case of Events", "authors": "Chantal van Son, Lora Aroyo, Oana Inel, Roser Morante and Piek Vossen", "id": "79"}, {"abstract": "Machine\u200b learning\u200b is\u200b used\u200b more\u200b and\u200b more\u200b frequently\u200b in\u200b the\u200b medical\u200b field,\u200b for\u200b financial applications\u200b and\u200b in\u200b business.\u200b Similar\u200b opportunities\u200b arise\u200b in\u200b the\u200b legal\u200b area.\u200b Requirements\u200b for the\u200b focus\u200b on\u200b which\u200b metrics\u200b however,\u200b differ\u200b for\u200b each\u200b field.\u200b When\u200b working\u200b with\u200b medical\u200b cases, high\u200b recall\u200b is\u200b of\u200b great\u200b importance,\u200b as\u200b missing\u200b a \u200b cancerous\u200b nodule\u200b can\u200b be\u200b a \u200b case\u200b of\u200b life\u200b or death. 1 \u200b In\u200b business\u200b and\u200b the\u200b financial\u200b sector\u200b the\u200b risks\u200b are\u200b evaluated\u200b via\u200b different\u200b metrics,\u200b think of\u200b high-frequency\u200b trading.\u200b During\u200b my\u200b thesis\u200b I \u200b want\u200b to\u200b explore\u200b the\u200b focus\u200b of\u200b metrics\u200b which make\u200b that\u200b machine\u200b learning-based\u200b products\u200b can\u200b be\u200b of\u200b value\u200b in\u200b the\u200b legal\u200b domain.\u200b For example\u200b the\u200b Raad\u200b voor\u200b de\u200b Rechtspraak\u200b only\u200b publisher\u200b a \u200b very\u200b small\u200b amount\u200b of\u200b court\u200b cases as\u200b the\u200b anonymization\u200b requires\u200b high\u200b precision. 2 \u200b The\u200b main\u200b question\u200b for\u200b the\u200b thesis\u200b is:\u200b to\u200b which extent\u200b can\u200b we\u200b develop\u200b an\u200b automatic\u200b anonymization\u200b of\u200b documents\u200b differentiating\u200b by recognizing\u200b natural\u200b persons,\u200b legal\u200b entities\u200b and\u200b governing\u200b bodies.\u200b I \u200b will\u200b focus\u200b on\u200b the\u200b related work,\u200b the\u200b aspects\u200b in\u200b which\u200b the\u200b legal\u200b field\u200b differs\u200b from\u200b other\u200b fields\u200b and\u200b will\u200b create\u200b a \u200b benchmark for\u200b anonymizing\u200b in\u200b Dutch\u200b court\u200b cases.", "title": "Semi-Automated Anonymization of Dutch Court Cases", "authors": "Simon Brugman and Arjen P. de Vries", "id": "78"}, {"abstract": "In this paper we present a system that classifies mammography reports in Dutch. A mammogram is a screening of the human breast using low energy X-rays. Radiologist observe these screenings for the detection of visual indicators of breast cancer. These visual indications are described in mammography reports, which are often unstructured and therefore not machine readable. Information therefore has to be retrieved manually, which is time consuming.The main purpose of this paper is to research to which extent a machine can be trained to classify mammography reports. For this experiment a data set containing 17,000 Dutch reports is used, which originate from the Dutch population screening on breast cancer, IBOB, and were provided by A.J.T.Wanders. Each report consists of two elements: the transcript of the radiologist in the form of unstructured text, which amounts to 2 to 4 sentences, and the code labels assigned by an expert. These classes represent the most common abnormalities in mammograms as defined by Dr.A. Wanders. There are four main ab- normalities: mass, calcification, asymmetry and architectural distortion. Each abnormality is then subclassified into types. In total there are up two thirteen unique classifications in the entire dataset. We approach this task as a multilabel classification problem. Data is represented through the bag-of-words model. From the data different sizes of N-gram fea- tures are extracted. Feature selection is done by both the frequency of a term and two tf-idf approaches. As a classifier Support Vector Machines are used. The final results of the experiments are a micro averaged F1-measure of 0.896 on the single label data, and a F1-measure of 0.854 on the multi label data. These results suggest that a machine can be trained to classify these reports. Furthermore our experiments suggest that an increase in training data may also improve the performance.", "title": "Classifying Abnormalities in Mammography Reports in Dutch", "authors": "Tim Loyen and Roser Morante", "id": "70"}], [{"abstract": "Grounding language in the physical world enables humans to use words and sentences in context and to link them to actions. Several recent computer vision studies have worked on the task of expression grounding: learning to select that part of an image that depicts the referent of a multi-word expression. The task is approached by joint processing of the language expression, visual information of individual candidate referents, and in some cases the general visual context, using neural models that combine recurrent and convolutional components (Rohrbach et al., 2016; Hu et al., 2016b,a). However, there is more than just the intended referent by itself that determines how a referring expression is phrased. When referring to an element of a scene, its relations with and contrasts to other elements are taken into account in order to produce an expression that uniquely identifies the intended referent. Inspired by recent work on visual question answering using Relation Networks (Santoro et al., 2017) we build and evaluate models of expression grounding that take in account interactions between elements of the visual scene. We provide an analysis of the performance and the relational representations learned in this setting.", "title": "Modeling relations in a referential game", "authors": "Lieke Gelderloos, Afra Alishahi, Grzegorz Chrupa\u0142a and Raquel Fern\u00e1ndez", "id": "69"}, {"abstract": "Recent advances in computational stylometry have demonstrated that automatically inferring quite an extensive set of personal attributes from text alone (e.g. gender, age, education, socio-economic status, mental health issues) is not only feasible, but can often rely on little supervision. This application opens up potential for both industry and academia to uncover 'hidden' demographics for a large volume of social media accounts. It can be safely assumed that the majority of users of these media are not aware the latent information they are sharing, creating a false sense of privacy that can be easily abused by third parties. Even if they we aware, they would have no countermeasures at their disposal other than self-censorship. One of the proposed computational methods for assisting users in guarding particular attributes is that of author and/or attribute obfuscation, where the goal is to rewrite a particular text in such a way that a classifier trained on detecting an author (or its attributes) is fooled. Most of the work on this topic has focused on rule-based perturbations on text input, demonstrating only minor gains. Our proposal is to use a text encoder-decoder model which learns intermediate representations which are invariant to the protected attributes, and which -- thanks to this property -- is able to rewrite user text in a way which largely preserves its meaning, but which conceals user identity and/or attributes.", "title": "Attribute Obfuscation with Gradient Reversal", "authors": "Chris Emmery, Enrique Manjavacas and Grzegorz Chrupa\u0142a", "id": "67"}, {"abstract": "In last year\u2019s CLIN conference, we showed that Twitter data (in this case the TwiNL collection) can be used to determine isoglosses for some major dialectal alternations, and that these Twiiter-isoglosses match closely with the isoglosses previously suggested by the literature. This year, we intend to produce a map of the (Dutch) Limburgian dialects that uses a wider selection of dialectal forms than the four used last year. The question, now, is how to select these. Following last year\u2019s strategy, we will look to the literature to compile a selection of, say, 20-40 phenomena that are known to vary within the province of Limburg. However, we will also try an alternative strategy: simply using counts of all character trigrams in all the tweets. Our research question is to which degree a knowledge poor approach (character trigrams) can replace a knowledge rich approach (established phenomena) for the creation of dialect maps. If the knowledge poor approach does perform satisfactorily, this would greatly facilitate mapping further regions with strong dialect use. The answer to the research question will be found in the confrontation of the two created maps, with each other as well as with knowledge from the existing literature.", "title": "Dialect mapped: knowledge rich or knowledge poor?", "authors": "Hans van Halteren, Roeland van Hout and Romy Roumans", "id": "66"}, {"abstract": "To this day, students\u2019 running-text answers to open exam questions are still overwhelmingly graded manually by lecturers or teaching assistants. This is the case even though approaches to automatise the grading process have already been developed since the 1960s (e.g., Page, 1966). While earlier approaches were based on simple features such as average word length or the total number of words, more advanced approaches to automatic grading have been developed in recent years. One avenue of research is based on comparing students\u2019 answers to a \u2018perfect\u2019 reference answer by means of semantic similarity (e.g., Noorbehbahani & Kardan, 2011). Still, such approaches generally are not considered trustworthy enough to actually be implemented in higher education. The current project aims to make a contribution to the development of techniques for automatic grading. I will compare different ways of operationalising semantic similarity (LDA, LSA, and a baseline approach based on term frequencies only) for automatically grading student answers. The dataset consists of 693 pairs of students\u2019 answers (402 English, 293 Dutch) to a first year Psychology exam question at Radboud University, and the grade that the lecturer assigned to that answer (here used as the ground truth). After preprocessing the raw data (tokenisation, stop word removal, lemmatisation) , implementation consists of two further steps. Firstly, for each method, I will obtain the similarity score between the student\u2019s answer and the reference answer. For example, for the LDA model I have calculated the cosine similarity between the topic vectors of the student\u2019s and the reference answer. In the second step, I will try out different algorithms to map similarity scores to grades. The two steps described above both will be implemented using a subset of the dataset as training data. In this subset, I will also explore the various parameter settings, such as the number of topics for the LDA. The final performance of our different approaches will be evaluated by obtaining the predicted grades for the test set, and correlating them with the grades assigned by the lecturer. Apart from contrasting the different approaches, I will also explore further ways to improve the accuracy of grade prediction. Among other things, I will compare the use of only one reference answer to the use of multiple reference answers. In addition, I will train LDA models for Dutch and English answers separately and combined, with the expectation that the topics will (mostly) contain words of one language only. At the time of writing this abstract, the research described above has not yet fully been implemented, and results are not yet available. However, the research will have been completed by the time CLIN28 takes place.", "title": "Automatically grading students\u2019 answers to exam questions", "authors": "Johanna de Vos", "id": "62"}], [{"abstract": "One of the main characteristics of individuals with autism spectrum disorder (ASD) is a deficit in social communication. In line with this, a lot of research has already been done about language and ASD. In this study, we investigate whether texts of Dutch speaking adolescents with ASD (aged 12-18 years) are (automatically) distinguishable from texts written by typically developing peers. The aim of this exploratory study is twofold: on the one hand we want to reveal whether there are specific characteristics in texts of adolescents with ASD. The challenge is to create a method that automatically extracts linguistic features from texts, instead of annotating them by hand. We look for simple linguistic features, but also for syntactic, semantic as well as discourse features. On the other hand, we examine the possibility to use these features in an automated classification task for diagnostic purposes. Our work differs from most other works on language and ASD in multiple ways: firstly, we examine written language, while most studies focus on oral narrations. Secondly, we develop a method for automatically extracting features for Dutch texts. To our knowledge, no such approaches have been investigated for this language regarding ASD. Also, we use a notably larger dataset than most other studies. We created this balanced dataset ourselves by gathering school assignments. Finally, we use machine learning techniques to automatically classify the analyzed texts. Most works only focus on finding differences in language use, but not on using these differences for identifying ASD. The aspects that are examined comprise a wide range of linguistic features, going from n-grams and simple features like average word and sentence length to more sophisticated features capturing syntactic complexity (like part of speech tag frequencies, patterns of constituents, etc.), semantic diversity (frequency of word categories, propositional idea density) and discourse structure (with a main focus on cohesion).We test the differences between the ASD group and control group for statistical significance and show that mainly syntactic features are different among the groups. The classification task reveals that we can increase the baseline accuracy by combining our features with word and character n-grams, but only to a limited extent. When our features aren\u2019t combined with word and character n-grams, accuracy is still significantly higher than chance.", "title": "Stylometric Text Analysis for Adolescents with Autism Spectrum Disorder", "authors": "Luna De Bruyne and Walter Daelemans", "id": "61"}, {"abstract": "The present paper is dedicated to evaluating the resources used to train a con- versational agent. A conversational agent, or a chatbot, is a computer program that communicates with people via texting or talking to them. This has become a very popular phenomenon in the recent years. Chatbots are often used to perform specific tasks, such as order a pizza, give a weather forecast or consult customers on some topics (Nakano et al., 2000). They are also used to simply entertain people and communicate with them on broad topics. The last type of chatbot is called \u201copen domain conversational agent\u201d (Shibata et al., 2009). Most of the open domain chat- bots are built using recurrent neural networks, when a neural network model tries to mimic human communication based on given question-answer pairs (Vinyals and Le, 2015). Obviously, this approach requires huge amounts of training data and is highly dependent on the available data. There are several sources from which open data can be extracted for training a chatbot. These sources include subtitles (dialogues from TV series and films), threads from various forums, data from social networks (messages in various com- munities, user walls and grids), and personal communication (Serban et al., 2015). At the same time there was no attempt to evaluate whether all these corpora are equally suitable for developing a chatbot. It is unknown how characteristics of a training corpus influence the resulting model and therefore the behavior and functioning of a chatbot. It can be hypothesized that every type of a corpus fits better to a specific dialogue situation that is closer to the situations present in the corpus itself. In my research I collect different dialogue corpora for the Russian language such as data from the communities in the Vkontakte social network, subtitles for Russian films and TV series, my personal communication in Vkontakte. Based on these data I train neural network sequence-to-sequence models that are used to produce replies to user queries. Then I integrate the resulting models within a chatbot framework provided by the Telegram messenger. The chatbots have been evaluated in two ways: with the help of human evaluators and with linguistic metrics. The human evaluators were asked to rate different aspects of their communication with chatbots on the scale from 1 to 5. Judging by the overall mean value (which is the mean of all the measures listed in the table), first_smatrbot and third_smartbot demonstrate similar level, while second_smartbot stands down by all the parameters except for the meaningfulness of its answers. Another approach towards chatbot evaluation is using linguistic characteristics of generated replies. The designed metrics aim at determining how grammatically and syntactically valid are the conversational turns produced by chatbots. It is important to note that we do not plan to evaluate the semantic validity of the answers. The adopted characteristics are the following: \u2022 Syntactic validity of a sentence (the ability of a syntactic parser to process the sentence correctly); \u2022 Presence of agreeing chunks in the reply. Because automatic evaluation is not perfect, I also annotated chatbot\u2019s replies by hand. I determined whether a sentence is syntactically valid and whether it contains agreeing chunks longer than 1 word. Annotated by hand, the sentence above receives 0 for syntactic validity and 1 for the presence of agreeing chunks. The evaluation procedure was as follows: 1. A chatbot was queried 10 times; 2. 10 replies were processed to determine their syntactic validity and presence of chunks; 3. the metric for each parameter rages from 0 if no positive cases were found to 10 if all the bot\u2019s replies were correct in the respective way. 4. the same 10 replies were also annotated manually; 5. this was repeated 10 times; 6. the resulting measure is the mean value of metrics rendered by 10 measurements. The main conclusions of the present research are the following: \u2022 different training corpora indeed lead to different performance of chatbots. It seems to be that the natural dialogue data is more suitable for this task. Artificial and mixed up dialogue from subtitles did not prove to perform on sufficient level. \u2022 automatic evaluation aimed at determining the syntactic and grammatic correctness of a sentence seems to be promising though some adjustments to the tools have to be carried out.", "title": "Evaluation of Different Dialogue Corpora for Training a Chatbot", "authors": "Elizaveta Kuzmenko", "id": "60"}, {"abstract": "FoLiA, i.e. Format for Linguistic Annotation, has been in development for over six year to accommodate the need to represent a wide variety of linguistic annotations in a unified, formalised and usable format that is defined independently of any linguistic vocabulary or language (van Gompel and Reynaert, 2013). FoLiA is aimed at language resource exchange and corpus storage. The format is currently funded by CLARIAH (Odijk, 2016), previously CLARIN-NL (Odijk, 2010), and has been adopted by partners all over the Netherlands and Flanders. This dual-poster presentation and demo session focusses on FoLiA itself on one hand, detailing the abilities and advancements made in the recent two years, and on the rich infrastructure of tools and libraries on the other end. The main highlight here is the FoLiA Linguistic Annotation Tool, known as FLAT. This is a web-based annotation environment that allows human annotators to create or redact linguistic annotations of many kinds. FLAT aims to eventually provide full coverage for FoLiA and has been progressing steadily over the past two years and has already been used in various research projects, notable here are the international PARSEME project and the current CLIN shared task. Current developments in FoLiA include a tighter formal framework, offering better integration with Linked Open Data, something we want to raise awareness for. We pursue tighter and more extensive validation. Various new linguistic annotations types have also been made available. In the spirit of interoperability there has been work on conversions to from/to other formats, from format such as ReStructuredText to FoLiA alternatives such as NAF (Fokkens et al., 2014) and TEI (Burnard and Bauman, 2007). With FoLiA we have always focussed on a establishing a practical infrastructure, easily accessible to researchers, data scientists, and developers alike. We believe the success of a format is determined in a bottom-up fashion by the tools and documentation around it; accessibility is key. Programming libraries for both Python and C++, along with an extensive set of command line tools, bring the format to the developer or data scientist.", "title": "A practical linguistic annotation infrastucture: FoLiA, its tools, and FLAT", "authors": "Maarten Van Gompel", "id": "53"}, {"abstract": "We present TICCLAT, one of the four projects awarded in the joint CLARIAH and eScience Center call `Accelerating Scientific Discovery in the Arts and Humanities' (ADAH). The Text-Induced Corpus Clean-up tool TICCL, integral part of the CLARIN infrastructure, is globally unique in utilizing the corpus-derived word form statistics to attempt to fully-automatically post-correct large corpora digitized by means of Optical Character Recognition. In 2018, NWO 'Groot' project Nederlab will deliver a uniformly processed and linguistically enriched diachronic corpus of Dutch containing an estimated 15 billion word tokens. We aim to extend TICCL's correction capabilities with classification facilities based on specific data collected from the full Nederlab corpus: word statistics and word dispersion information, document and time references and linguistic annotations, i.e. lemmata as well as Part-of-Speech and Named-Entity labels. These data will complement a solid, renewed basis composed of the available validated lexicons and name lists for Dutch. In this, TICCL as a post-correction tool will be transformed into TICCLAT, a lexical assessment tool capable of delivering not only correction candidates, but also e.g. more accurately dated diachronic Dutch word forms, more securely classified person and place names. To achieve this on scale, the TICCLAT project will seek a successful merger of TICCL's anagram hashing with bit-vectorization techniques. TICCLAT's capabilities will also be evaluated in comparison to human performance by an expert psycholinguist (Emmanuel Keuleers, Tilburg University). The data collected will be exportable for storage in a data repository, as RDF triples, for broad reuse. The project will greatly contribute to a more comprehensive overview of the lexicon of Dutch since its earliest days and of the person and place names that share its history. Its partners are the Dutch experts in Lexicology (Katrien Depuydt, INT) , Person Names (Gerrit Bloothooft, UU) and Toponyms (Rombert Stapel, IISH). Jisk Attema is the coordinator for the eScience Center side of the project.", "title": "TICCLAT: Text-Induced Corpus Correction and Lexical Assessment Tool", "authors": "Martin Reynaert", "id": "52"}], [{"abstract": "The Dark Web is the place on the internet that is not indexed by search engines, and is accessible only via a special router. Its goal is anonymity for the users. Although the dark web in itself is not illegal, many illegal activities take place on the dark web. Examples are the trade in weapons, drugs, cyber-attacks etc. Law enforcement agencies are aware of these activities and intervene more and more. Recently, the Hansa Market has been taken over and shut down by the Dutch Police (Underground Hansa Market taken over and shut down, 2017). We are interested in the effects of such actions. Where do the buyers and sellers of a shutdown marketplace go? Can we identify them when they resurface with different user names 1 ? For that purpose it is important to be able to track the active users on the marketplaces and forums. This is where authorship attribution comes in and what this abstract is about. Since the goal of the dark web is to remain anonymous and we only have surface information such as the posts of a user, it is difficult to track users between marketplaces and forums as often they use multiple usernames. Therefore we aim to connect users between forums and marketplaces based on their language use. For this purpose we create a similar profile for each user as in (Spitters, Klaver, Koot, & van Staalduinen, 2015) . The profile contains features related to time of posting, character n-grams, and linguistic features such as number of function words, etc. Using this profile we compare four approaches to modeling the similarity between users: 1) The first is based on a cosine-similarity measure , where each user is compared to every other user and ranking determines which usernames most likely belong to the same individual 2) The second is based on a multi-label SVM classification where the model predicts the user that has written a set of posts. 3) The third is based on a binary SVM classification that predicts whether two given user profiles belong to the same individual. 4) The final one is a Siamese Deep Learning network that is particularly suited for finding similarity relationships These methods are evaluated on a set of scraped Dark Web forums and markets. The results show that the cosine similarity measure can be difficult to interpret, the multilabel SVM classification method does not scale well with many users and finally the binary SVM shows promising results. We will compare the results of these traditional approaches to the Siamese Deep Learning approach. By connecting usernames to underlying individuals we can see which users are active on multiple markets and forums and how their activity changes after a market is taken down. This helps to understand the consequences of interventions by law enforcement agencies on the Dark Web", "title": "Authorship Attribution on the Dark Web", "authors": "Maya Sappelli and Stephan Raaijmakers", "id": "47"}, {"abstract": "Low linguistic ability has been associated with low cognitive reserve, which might result in the development of Alzheimer\u2019s disease. As a result, a lower propositional idea density (PID) in early life might predict cognitive decline in late life (Snowdon et al., 2000). PID is a measure for the density (i.e. proportion) of propositions or different ideas in text, and can be interpreted as a measure of linguistic ability. This paper uses a literary case study to explore the differences in propositional behavior between healthy individuals and people with Alzheimer\u2019s disease. Based on the CPIDR 3 program for English (Brown et al., 2008), we developed a computerized propositional idea density rater for Dutch texts which is publicly available. We measured the PID of the works of one author who was diagnosed with Alzheimer\u2019s disease (i.e. Hugo Claus) and one comparable control author (i.e. Willem Elsschot). Longitudinal changes in PID of both authors were compared, as well as the initial differences in PID in early life. Our analysis shows that the propositional idea density of Elsschot was significantly higher than that of Claus. The PID of Elsschot also significantly increased over time. This change in PID differed from the slight decrease in PID of Claus. To conclude, this study provided some support to the hypothesis that a low PID in early life and a slight decrease in PID over time might be a predictor of cognitive decline in late life.", "title": "The Claus Case: Exploring the Use of Propositional Idea Density for Alzheimer Detection", "authors": "Silke Marckx, Ben Verhoeven and Walter Daelemans", "id": "46"}, {"abstract": "The National Library of the Netherlands (KB) is an active partner in national and international cooperative efforts to develop new knowledge and technology. With this poster, we will showcase what the KB can offer researchers in the field of computational linguistics. We will present our digitised datasets, current research projects and the services of the KB Lab. The Koninklijke Bibliotheek (KB), National Library of the Netherlands, is a research library with a broad collection in the fields of Dutch history, culture and society. As a national library we collect and store all (digital) publications that appear in the Netherlands, as well as a part of the international publications about the Netherlands. The KB has planned to have digitised and OCRed its entire collection of books, periodicals and newspapers from 1470 onward by the year 2030. Over 60 million book-, newspaper- and magazine pages are currently available via the search portal www.delpher.nl. Via DNBL (www.dbnl.nl) high quality digitized works of Dutch literature, linguistics and Cultural History can be accessed. To further improve the usability of our content we aim to have all relevant names of persons, locations and organisations in our digital content reusable as linked (open) data in the near future. For this, a team of researchers at the KB Research Department is developing methods to automatically extract these types of information from the unstructured text of e.g. historical newspaper articles and use it to improve the findability and usability of our digital content. Next to this, most of our datasets (such as historical newspapers, books, periodicals and catalogues) are freely available for research purposes via our Dataservices department and we welcome and encourage experiments and new applications. The virtual KB Lab shows some of such applications and invites researchers to experiment with our data, new technologies and innovative prototypes. The KB also collaborates with academic researchers in research projects or (junior) fellowships to learn from their research in order to improve our services. Example outcomes of research projects using computational linguistic methods are the Frame Generator and Genre Classifier. This poster will present the various datasets that the KB has available for research, the activities we undertake to work together with scholars in research projects, the services that we offer those who wish to work with our material and demos of the tools available in our KB Research Lab.", "title": "Computational linguistic opportunities at the Koninklijke Bibliotheek", "authors": "Martijn Kleppe, Lotte Wilms and Steven Claeyssens", "id": "45"}, {"abstract": "Sentiment Analysis is the task of automatically analyzing text documents using computational methods to detect the opinions about specific objects e.g. products, people, events, companies. Today the web has turned into a great source of opinions about entities, especially with the increased popularity of social media. People are expressing their opinions in reviews, forum discussions, blogs, twits, comments, and postings on social network sites. These opinions are being extensively used by organizations and individuals during the decision-making process. It is common for all of us to check the reviews of others before we decide to purchase a product, visit a place for holidays and reserve a hotel room, read a book or even deciding to watch a movie. Reviews are good means for deciding, while it's a time-consuming task to read them line-by-line and to finally decide if it's a positive review or a negative one. What if the decision was automated? What if a machine can suggest us something based on the reviews it processed so far? It would make our life a lot easier, isn't it? This paper manifest our original unpublished thesis work, where we tried to focus on extracting opinions from corpus or domain specific reviews such as reviews about books, movies, apps etc. We have developed a system that uses Binarized Multinomial Na\u00efve Bayes Classifier to classify the reviews as either \u201cPOSITIVE\u201d or \u201cNEGATIVE\u201d which is formally known as Sentiment Polarity Detection and also rated the Intensity of the Polarity showing how much positive or negative a review is. The evaluation is done with two well-known sentiment dataset, one of which is the \u201cSentiment polarity dataset (v.2.0) By Bo Pang & Lillian Lee\u201d which consists of 2000 movie reviews and another was \u201cStanford dataset\u201d consisting of a large collection of 50000 movie reviews both from IMDB. Our system gave 80~82% accuracy outperforming most of the current baseline methods.", "title": "Sentiment Analysis: Skipping Mumbo Jumbo and Revealing How We Actually Do It!!", "authors": "Md Ataur Rahman and Rudra Pratap Deb Nath", "id": "43"}], [{"abstract": "Trainable approaches to data-to-text systems offer several benefits to the developer: they enable easy transfers between domains, can generalize for unseen instances and do not need the input of experts. Furthermore, traditional rule-based systems may not be able to account for all distinct rules if the domain is complex (e.g. the sports domain). Therefore, the need for a trainable approach to content selection and realization has arisen in the past few years. We present a project wherein we attempt such an approach for a data-to-text system in the sports domain.", "title": "Exploring automated template generation for data-to-text systems", "authors": "Chris van der Lee, Emiel Krahmer and Sander Wubben", "id": "33"}, {"abstract": "Sense disambiguation of potentially idiomatic expressions (PIEs, for short) is the task of distinguishing between different uses of a potentially idiomatic phrase. This entails, for example, distinguishing the literal use of make hay in \u2018Only in Speyside did the number of farms making hay drop to 75%\u2019 from its idiomatic use, as in \u2018Investment banks made hay while takeovers shone\u2019. Distinguishing these senses is a crucial step in the correct interpretation of idiomatic expressions in text. Not dealing with this problem poses a challenge for various NLP applications (Sag, Baldwin, Bond, Copestake & Flickinger, 2002), including machine translation (Salton, Ross & Kelleher, 2014; Isabelle, Cherry & Foster, 2017), and sentiment analysis (Williams, Bannister, Arribas-Ayllon, Preece & Spasi\u0107, 2015). There are a number of existing approaches to PIE sense disambiguation (Fazly, Cook & Stevenson, 2009; Sporleder & Li, 2009; Gharbieh, Bhavsar & Cook, 2016; Salton, Ross & Kelleher, 2016, among others), but these often rely on large amounts of training data for each individual expression. Unsupervised methods do not have this drawback, but the performance of existing unsupervised approaches leaves significant room for improvement. In addition, most systems use different evaluation settings, which hampers comparability. As such, we focus on improving the perfor- mance of unsupervised methods and using a more comprehensive evaluation setting. An additional advantage of unsupervised methods is that they are applicable to all PIE types equally, even to unseen types. Our starting point is the lexical cohesion-based approach pioneered by Sporleder and Li (2009). This approach relies on the assumption that literally used PIEs are more cohesive with the words in the surrounding context than idiomatically used ones. For example, make hay is likely to be literal when the context contains words like grass, cattle, and farm. We extend this method by using larger context windows, different words in those context, and improved measures of word similarity. In addition, we take into account cohesion between the context and the idiomatic meaning of the PIE. For example, we test whether knowing that words like profit and success are cohesive with the idiomatic meaning of make hay is a significant clue for classifying its usage as idiomatic. Evaluation is done on all available existing corpora, rather than just one: the VNC-Tokens dataset (Fazly et al., 2009), the IDIX corpus (Sporleder, Li, Gorinski & Koch, 2010), the SemEval-2013 Task5b dataset (Korkontzelos, Zesch, Zanzotto & Biemann, 2013), and a new dataset developed as part of this work. The main benefit of this new dataset is that it covers a larger number of PIE types covered, 278, compared to the other corpora, which contain around 50 to 60 types. Both F1-score and accuracy are used for evaluation, in micro-average and macro-average-over-types forms. We also examine performance on individual PIE types, in order to better assess the performance on infrequent and unseen types.", "title": "Improving Sense Disambiguation of Potentially Idiomatic Expressions", "authors": "Hessel Haagsma", "id": "32"}, {"abstract": "One of the tasks faced by young children is the segmentation of a continuous stream of speech into discrete linguistic units. A possible strategy for bootstrapping the segmentation process is the wholesale storage of syllable chunks within an early proto-\u00ad\u2010lexicon. Stored chunks can then be compared to one another and to incoming input in order to identify units such as phonemes and morphemes. Here, we investigate what types of chunks are most likely to be stored by children. Our method involves sampling syllabified utterances from transcribed child-\u00ad\u2010directed speech as units that children could potentially store as under-\u00ad\u2010segmented chunks. We vary sampled utterances according to (a) their frequency in child-\u00ad\u2010directed speech and (b) the mutual predictability of their syllables (coherence). In the first of two experiments, we then ask whether it is easier to classify single-\u00ad\u2010word chunks on the basis of (i) their frequency or (ii) their coherence. Following this, in experiment two, we attempt to ascertain whether children are more likely to store coherent or frequent chunks. We argue that comparing chunks to one another and to incoming speech should quickly result in identifying words which are contained in a large number of stored chunks. Consequently, we consider it more probable that children store chunk samples that perform well at predicting when their constituent words are learned, and we consider it less likely that children store poorly performing samples. Our results show that especially coherent chunks are more likely to correspond to single words, and that coherent chunks samples consistently perform better at predicting when their component words are learned. These patterns suggest that children's early proto-\u00ad\u2010lexica are more likely to contain coherent rather than frequent undersegmented chunks.", "title": "Children are more likely to store coherent rather than frequent under-segmented chunks: Quantitative evidence from a corpus study", "authors": "Robert Grimm, Steven Gillis and Walter Daelemans", "id": "31"}, {"abstract": "The dt-mistake is a persistent problem in Dutch writing. This grammatical error occurs when writing conjugated verbs due to the difference between how a verb sounds and how it is written. Unfortunately, it seems impossible to prevent the dt-mistake from happening. Therefore spell checkers and correctors try to detect and correct the mistakes afterwards. Current spell tools are working insufficiently with regard to dt-mistakes, however. The tools mostly rely on word lists, which are able to detect non-word errors, but fail to detect real-word errors, where the real problem of the dt-mistakes lies. In this work, a new approach for developing spell tools is presented that is particularly effective for detecting and correcting real-word dt-mistakes. The approach relies on recurrent neural networks, which are powerful in representing the context of a sentence. Given a sentence and a verb that may need correction, the neural network predicts a probability distribution over edit-rules. Examples of such rules are: replacing the last d by a t, adding a t, doing nothing (in case the verb was spelled correctly), etc. In total, we distinguished 15 edit rules. To predict an edit rule, the model builds a representation of the sentence and the verb that may need correction using three recurrent neural networks (RNNs). One RNN encodes the verb on the character-level, the other two RNN\u2019s encode the rest of the sentence on word-level. These three representations are concatenated and fed to a softmax layer which outputs a probability distribution over the possible edit rules. To train and evaluate the dt-correction model, we automatically constructed a large dataset of dt-mistakes with the possibility to change the proportion of the different dt-mistakes in the resulting dataset. After comparison with a majority class baseline model, it is clear that the dt-correction model really learns to detect and correct dt-mistakes. We also compared the model against two current spell tools: the built-in spell checker of Microsoft Word and the ILT spell checker, the latter was the best performing system according to recent empirical tests. Each tool was evaluated on the same test set of 200 sentences with annotated dt-mistakes. Results showed that our dt-correction model was the only system that could find and correct real-word errors. The existing systems were better in finding non-word errors, however. None of the systems predicted false positives (i.e., predicting corrections for correctly spelled words). We conclude that the performance of the proposed dt-correction model is not yet sufficient to replace existing tools altogether, but since it finds different dt-mistakes and predicted no false positives, a hybrid model between an existing system and our dt-correction model would improve the performance of current spell tools. A suggestion for future work is adding the mechanism of attention to the model. With this, an RNN can learn to focus on specific parts of the sentence, which in the case of dt-mistakes would be very useful because the subject of the sentences is the determinative factor.", "title": "Automatic Detection and Correction of dt-mistakes Using Recurrent Neural Networks", "authors": "Yannick Laevaert, Geert Heyman and Marie-Francine Moens", "id": "30"}], [{"abstract": "We discuss the use of the Corpus Gesproken Nederlands (Corpus of Spoken Dutch, CGN, Oostdijk et al., 2002), and various tools that are available to query it, as a source of information on diachronic language change, using the so-called apparent-time method. This method, pioneered by Labov (1963), is based on the observation that differences in language use between age groups may be due to an ongoing language change (Boberg, 2004). In this method, the synchronic linguistic behaviour of different generations is compared, and it is assumed that the generations reflect different stages of the language. The CGN can be used as a data source for this method because speakers of different ages are sampled, and ages are included as metadata. The phenomenon we investigate using this method is the word order variation of Dutch two-verb clusters. Synchronically, these clusters occur both in the order auxiliary \u2013 participle (1-2) and in the order participle \u2013 auxiliary (2-1). Diachronic studies have reported increasing use of the 1-2 order starting as early as the 15th century (Couss\u00e9, 2008; Coup\u00e9, 2015). As diachronic change typically leads to synchronic variation, we hypothesize that the order variation we observe in modern Dutch can be related to this diachronic development. As verb clusters are more than just linear sequences of verbs, extracting them from a corpus is not trivial. To retrieve all verb clusters from a corpus it needs to be queried in terms of dependency relations or constituent structure. However, only a small part of the CGN is syntactically annotated, the rest of it is only annotated with part-of-speech tags. Therefore, we searched the CGN in two stages. First, we used two web-based tools that operate on the syntactically annotated part of the corpus in order to obtain a smaller initial set of verb clusters, and to identify factors that correlate with the use of different varieties besides the age of the speakers. We used GrETEL (Augustinus et al., 2013) for query composition, in an example-based way which is accessible to non-technical users, and PaQu (Odijk, 2015) for querying and result extraction. In the second stage, we take a more labour-intensive semi-automatic approach to retrieve more instances of verb clusters from other parts of the CGN, using the CGN Corpus Exploitation Software (COREX, Kilpatrick and Hellwig, 2002). This tool was used to query particular text types and speaker types in the remaining part of the CGN, on the basis of part-of-speech tags and word forms. Our results show that younger speakers indeed use the 1-2 order more frequently, following the trend of older diachronic developments. It appears that a diachronic change is ongoing, enabling synchronic intraspeaker variation. We also show that such intraspeaker variation is present in the corpus: many speakers make use of both possible word orders, even if they only produced a few verb clusters in total.", "title": "Language change in the Corpus Gesproken Nederlands: An apparent-time study of verb cluster order", "authors": "Marieke Olthof, Maud Westendorp and Jelke Bloem", "id": "29"}, {"abstract": "Although earlier corpus studies have been attempted on the P ERFECT (e.g. Nishiyama and Koenig (2010) for English, van der Klis et al. (2017) for a cross-linguistic analysis), it has not been attested whether all verbs appear equally in the P ERFECT . As Goldberg et al. (2004) state, high-frequency collexemes could facilitate language learners to note correlations between meaning of the word and the construction itself. We set out to find whether these collexemes exist for English, and whether there is any variation in genre or across languages. The English present perfect is constructed using by an auxiliary H AVE followed by a past participle (e.g. \u201che has left\u201d). However, the auxiliary and the past participle need not be directly adjacent to each other (e.g. \u201che has just left\u201d). Another caveat is that the past participle \u201cbeen\u201d might start a passive (\u201chas been read\u201d) or progressive (\u201chas been reading\u201d) construction. We therefore used an extraction algorithm by van der Klis et al. (2015) to extract present perfect occurrences from the British National Corpus, including the passive, but excluding progressive forms. We retrieved co-occurrence frequencies for all verbs (i.e. the lemmata of the past participle) and compared that with the total frequency of these verbs across the corpus in a collostructional analysis (Stefanowitsch and Gries, 2003). We found (atelic) state verbs (like \u2018be\u2019 and \u2018have\u2019) scored high on Attraction (the relative frequency of the lemma given the P ERFECT ), but low on Reliance (the relative frequency of the P ERFECT given the lemma). Conversely, (telic) achievement verbs like \u2018finish\u2019 and \u2018lose\u2019 scored high on Reliance, but low on Attraction. Correcting for contingency information using the Fischer-Yates exact test reveals (telic) accomplishments (\u2018show\u2019, \u2018prove\u2019) and perception verbs (\u2018see\u2019, \u2018hear\u2019) are most typical for the English present perfect. Interestingly, we find some variation between genres. Newspaper items attract reporting verbs like \u2018agree\u2019 or \u2018decide\u2019, but repel cognitive state verbs like \u2018think\u2019, \u2018believe\u2019 and \u2018know\u2019. Fiction as well as the spoken section repels the same verbs, but attracts verbs like \u2018happen\u2019 and \u2018change\u2019, verbs that denote progression. We ran a similar analysis on the Europarl parallel corpus (Tiedemann, 2012), comparing French and English. From the literature, we know the distribution of the P ERFECT is rather different between these languages, especially in narration, where the French pass\u00e9 compos\u00e9 is licensed, but the English present perfect is not (e.g. de Swart (2007), Schaden (2009)). In our collostructional analysis, we find that French, like English, repels cognitive state verbs, but, unlike English, strongly attracts verbs like \u2018voter\u2019 (to vote) and \u2018dire\u2019 (to say). Note that in this analysis, we used the French and English parts of Europarl as comparable monolingual corpora rather than aligned corpora. All in all, the role of the lexicon in the P ERFECT is not to be dismissed: not all verbs are equally willing to appear in a P ERFECT , and these dissimilarities could well prove to be of interest for getting closer to a semantics of the P ERFECT .", "title": "Lexical preferences in the Perfect construction", "authors": "Martijn van der Klis", "id": "26"}, {"abstract": "Massive automatic comparison of languages by using parallel corpora will greatly enhance and speed up research in the field of comparative syntax. The goal of this research is to develop a theoretical model of the syntactic properties that all languages have in common and the range and limits of syntactic variation.It only makes sense to compare parallel translated sentences that are sufficiently similar syntactically. A method and measure is needed to filter out parallel sentences that are syntactically too different. In this paper I compare three possible filters with each other and with a baseline, (relative) Levenshtein (Levenshtein, 1966) distance based on POS-tags. The first filter uses sentence-length ratio between aligned sentences as a measure for syntactic comparability. Alignments in which one of the two sentences is twice as long as the other are most probably noise, and should not be used for syntactic comparison. Sentence-length ratio as a measure will be computationally cheap, though it might not be fine-grained enough to adequately grasp syntactic differences caused by differences in syntactic hierarchies. The second filter uses dependency parses. Hierarchical input, such as parses, should give better results than the baseline, which takes a linear string as input. I evaluate the usefulness of Oommen et al.\u2019s (1996) or Zhang and Shasha\u2019s (1989) algorithms to measure distance between trees. A weakness of using dependency parses is that a parser must be available for all languages involved. The third filter uses sentence vectors. We will see whether Mikolov et al.\u2019s (2013) method to translate between two vector spaces determines whether two aligned sentences are syntactically comparable. The three filters will be evaluated with a manually labeled set of syntactically comparable parallel sentences in English, Swedish and Dutch. We will conclude by showing which method or combination of methods provides the best filter.", "title": "A filter for syntactically comparable parallel sentences", "authors": "Martin Kroon and Sjef Barbiers", "id": "25"}, {"abstract": "A central part of discourse processing is the inferring of coherence relations. These relations exist between units in a text and are typically classified into senses, such as contrast, elaboration or cause. They can either be left implicit, as in example (1), or signaled explicitly through a word or phrase referred to as discourse connective, such as in example (2). 1. Jonathan was angry. He failed his exam. 2. Jonathan was angry because he failed his exam. An example of an inventory of relations these connectives can represent is the PDTB Relation Hierarchy (Prasad et al., 2008). The reliable identification of discourse connectives is an important step in Discourse Parsing. In the 2016 CONLL shared task data on Discourse Parsing, 1 about 45% of all relations are explicit relations. 2 Additionally, because of the pipeline setup exploited by recent State of the Art approaches to discourse parsing (Lin et al., 2010), (Wang and Lan, 2015), (Oepen et al., 2016), mistakes in discourse connective identification (typically the first step in the pipeline) can have significant impact on downstream tasks. Identifying discourse connectives is not a simple task, since connectives are not a homogeneous class and contain conjunctions, adverbials as well as prepositions. In addition, they are often ambiguous between having a discourse function or not, as well as with regard to the type of relation they mark, as is exemplified by examples (3)-(5). 3. Sophie was tired and happy. - no discourse relation 4. Sophie was tired and she wanted to go to sleep. - discourse relation (sense: Expansion.Conjunction) 5. Sophie was tired, and Jonathan was wide awake. - discourse relation (sense: Comparison.Contrast) A lexicon of discourse connectives can be a very useful resource when applying discourse parsing to a particular language. Several resources exist for German (Stede, 2002), French (Roze et al., 2012), English (Prasad et al., 2008) and Italian (Feltracco et al., 2016), but to the best of our knowledge, such a resource does not yet exist for Dutch. Our work seeks to fill this gap. Besides the use for discourse parsing, a bi-lingual lexicon can be beneficial to both translators (human or machine) and (human) language learners, as for many connectives, there is no straightforward m:n mapping between languages. A bi-lingual lexicon can assist in contrastive analyses of how different languages lexicalize the set of coherence relations. For these reasons, we present an attempt to automatically construct a bi-lingual Dutch-German Discourse Lexicon, based on DiMLex (Stede, 2002). The Dutch connectives are obtained through creating word-alignments (Gao and Vogel, 2008) for two sentence-aligned corpora: the German-Dutch subset of Europarl (Koehn, 2005) and News-Commentary11 (Tiedemann, 2012). This data-driven approach is less time-consuming than manually translating the German connective lexicon, and is likely to produce a more complete list than automatically translating the German lexicon because an MT engine is less likely to output low-frequent translations. We use DiMLex as a starting point because it aims to be an exhaustive list of German connectives, and focus on German-Dutch because the structural similarities between these two languages are likely to produce relatively reliable word-alignment results. In our presentation, we will introduce the bi-lingual lexicon and discuss the challenges and design decisions made along the way.", "title": "A Lexicon of Dutch Connectives", "authors": "Peter Bourgonje, Jacqueline Evers-Vermeul, Jet Hoek, Gisela Redeker, Ted Sanders and Manfred Stede", "id": "23"}], [{"abstract": "One of the major challenges of using historical document collections for research is the fact that Optical Character Recognition (OCR) on these documents is far from perfect. One way the quality of OCR text can be improved is by applying OCR post-correction. OCR post-correction involves manipulating the (textual) output of the OCR process. Existing approaches for OCR post-correction generally make use of extensive dictionaries to replace words in the OCR text that do not occur in the dictionary with words that do. The main problem with these existing approaches is that they do not take into account the context in which words occur. By training character based language models using long short term memory networks, this context can be taken into account. The poster presents ochre, an open source software package for training character-based language models that can be used for OCR post-correction 1 . Ochre is based on Python deep learning library Keras 2 and uses common workflow language (CWL) 3 to ensure reproducibility and enable reuse. The first results of using LSTMs to correct OCR errors in the VU DNC corpus 4 are promising; the average character error rate per newspaper article drops from 3.72% to 1.34% using sequence to sequence, a neural network architecture that is also used for machine translation. In addition to functionality for training LSTMs, ochre provides an overview of OCR post-correction data sets, and tools and workflows to preprocess these datasets, do post-correction, assess the performance of the OCR post-correction, and analyze the types of errors that are corrected.", "title": "Ochre, a Toolbox for OCR Post-Correction", "authors": "Janneke Van Der Zwaan and Lotte Wilms", "id": "22"}, {"abstract": "In\u200b May\u200b 2017,\u200b a \u200b lawsuit\u200b was\u200b filed\u200b against\u200b 22\u200b people\u200b for\u200b posting\u200b offensive\u200b comments\u200b on\u200b Facebook. Twenty\u200b of\u200b these\u200b people\u200b were\u200b convicted\u200b for\u200b a \u200b \u201csingular\u200b insult\u201d,\u200b following\u200b article\u200b 266\u200b of\u200b the\u200b Dutch Penal\u200b Code\u200b (Wetboek\u200b van\u200b Strafrecht,\u200b 2017).\u200b This\u200b conviction\u200b was\u200b controversial\u200b because\u200b it\u200b was\u200b the largest\u200b lawsuit\u200b regarding\u200b insulting\u200b language\u200b in\u200b the\u200b Netherlands\u200b to\u200b date.\u200b Because\u200b of\u200b this\u200b lawsuit, there\u200b has\u200b been\u200b a \u200b renewed\u200b interest\u200b on\u200b offensive\u200b language\u200b on\u200b social\u200b media.\u200b In\u200b this\u200b study,\u200b an automated\u200b recognition\u200b tool\u200b is\u200b proposed\u200b to\u200b detect\u200b this\u200b type\u200b of\u200b language.\u200b \u200b While\u200b other\u200b researchers only\u200b use\u200b a \u200b dictionary-based\u200b approach\u200b for\u200b similar\u200b tasks\u200b (e.g.\u200b Tulkens\u200b et\u200b al\u200b 2016),\u200b this\u200b tool\u200b also incorporates\u200b other\u200b types\u200b of\u200b linguistic\u200b features,\u200b based\u200b on\u200b a \u200b stylistic\u200b analysis\u200b of\u200b offensive\u200b language compared\u200b to\u200b non-offensive\u200b language\u200b in\u200b Facebook\u200b comments.\u200b The\u200b method\u200b of\u200b stylistic\u200b analysis\u200b is based\u200b on\u200b Grant\u200b (2012),\u200b in\u200b which\u200b two\u200b separate\u200b data\u200b sets\u200b are\u200b looked\u200b at\u200b to\u200b determine\u200b what\u200b features are\u200b specific\u200b to\u200b each\u200b set.\u200b This\u200b analysis\u200b is\u200b done\u200b on\u200b a \u200b corpus\u200b of\u200b roughly\u200b 1200\u200b downloaded\u200b Facebook comments\u200b that\u200b were\u200b posted\u200b on\u200b the\u200b public\u200b pages\u200b of\u200b five\u200b political\u200b parties\u200b and\u200b the\u200b public\u200b page\u200b where the\u200b comments\u200b were\u200b posted\u200b that\u200b were\u200b brought\u200b to\u200b court\u200b in\u200b May.\u200b The\u200b comments\u200b were\u200b marked\u200b by three\u200b annotators\u200b as\u200b either\u200b offensive\u200b or\u200b non-offensive. Following\u200b Grant\u2019s\u200b method,\u200b if\u200b a \u200b certain\u200b element\u200b is\u200b used\u200b more\u200b than\u200b twice\u200b as\u200b often\u200b in\u200b one data\u200b set\u200b than\u200b in\u200b the\u200b other,\u200b and\u200b if\u200b it\u200b is\u200b used\u200b frequently\u200b enough\u200b (n\u200b >=\u200b 10),\u200b the\u200b element\u200b can\u200b be included\u200b as\u200b a \u200b feature\u200b that\u200b can\u200b help\u200b distinguish\u200b between\u200b offensive\u200b and\u200b non-offensive\u200b comments. Following\u200b this\u200b method,\u200b eleven\u200b features\u200b were\u200b found.\u200b These\u200b features\u200b include\u200b the\u200b use\u200b of\u200b offensive words,\u200b demonstrative\u200b determiners\u200b and\u200b certain\u200b pronouns.\u200b All\u200b features\u200b are\u200b binary:\u200b either\u200b the\u200b feature is\u200b included\u200b in\u200b a \u200b comment\u200b or\u200b it\u200b is\u200b not.\u200b The\u200b tool\u200b can\u200b extract\u200b the\u200b features\u200b from\u200b unseen\u200b comments and\u200b uses\u200b Jaccard\u2019s\u200b coefficient\u200b to\u200b determine\u200b whether\u200b there\u200b are\u200b enough\u200b similarities\u200b to\u200b an\u200b offensive language\u200b baseline.\u200b This\u200b results\u200b in\u200b a \u200b correct\u200b evaluation\u200b of\u200b 77%\u200b of\u200b the\u200b testing\u200b data,\u200b where\u200b most\u200b of the\u200b misjudged\u200b comments\u200b were\u200b falsely\u200b marked\u200b as\u200b insulting.\u200b This\u200b corresponds\u200b to\u200b a \u200b F1\u200b of\u200b 0.55.\u200b Error analysis\u200b shows\u200b that\u200b most\u200b of\u200b the\u200b errors\u200b are\u200b not\u200b due\u200b to\u200b incorrect\u200b feature\u200b extraction,\u200b but\u200b due\u200b to errors\u200b in\u200b misclassifications\u200b (i.e.\u200b non-offensive\u200b comments\u200b using\u200b features\u200b that\u200b are\u200b usually\u200b found\u200b in offensive\u200b comments).\u200b We\u200b are\u200b currently\u200b working\u200b on\u200b finding\u200b and\u200b implementing\u200b more\u200b features\u200b in order\u200b to\u200b reduce\u200b the\u200b number\u200b of\u200b misclassified\u200b comments.", "title": "Automated detection of offensive language on social media", "authors": "Finn R\u00fcbsaam, Roser Morante and Fleur van der Houwen", "id": "20"}, {"abstract": "Over 60.000 Dutch archaeological reports are available online, and this number is growing by around 4.000 a year. The main reason for the existence of these reports is the Malta Convention; a European agreement, aimed at protecting archaeological remains. Every research project that is performed has to be documented and deposited according to Dutch law, which has created a collection of grey literature too vast to comprehend. Many of these reports are threatened to end up in a proverbial graveyard, unread and unknown. Currently it is only possible to search through the metadata of these documents, mainly via the DANS (Data Archiving and Networking Services) repository. However, these metadata are often of poor and inconsistent quality, and generally do not describe the contents of a report well. Also, an archaeologist will generally want to search more fine- grained, and might be interested in what is known as the \u2018by-catch opportunity\u2019; i.e. a single Bronze Age find in a Medieval excavation, not mentioned in the metadata. There is a strong need for a better way to search through these documents. Also, archaeologists are eager to use multiple aspects in their searches; an example query might be to find all documents relating to the Iron Age, from a particular geographical area, that mention cremations. This is currently possible via DANS, but it is difficult and inaccurate. To effectively index these texts, Named Entity Recogni- tion (NER) is needed to correctly identify and distinguish between entities. Standard approaches to NER, and NER in related fields such as history, are insufficient to deal with the peculiarities of archaeological concepts and the wealth of potential classes. Some of the challenges include non-standard naming, extensive polysemy & synonymy, and complex word formation, including different spellings and concepts including capitals, numbers and symbols. This is particularly true for archaeological time periods, which can be expressed in numerous ways. For example, the following entities all equate to roughly the same time period: Neolithic, Swifterbant culture, Early Neolithic, New Stone Age, 3500 v.Chr, 5000 to 4000 BP and 4915 \u00b1 40 Cal BC. Some research has already been done on NER in archaeological texts in e.g. English and Dutch, but these are not combined with full-text search, or tend to focus on limited entity types, and not the full breadth of archaeological concepts, which includes artefact, time period, place, material, ground context and monument. This means that currently there is no working system in place for Dutch archaeology. In this presentation, we will present the first phase of AGNES (Archaeological Grey literature Named Entity Search), in which machine learning is used to perform NER. The initial experiments use Conditional Random Fields and a feature set fine-tuned to archeological concepts. The identified entities are combined with a full-text index to create an effective online search, allowing researchers to answer research questions that are currently impossible to solve.", "title": "Named Entity Recognition in Dutch Archaeological Research Reports", "authors": "Alex Brandsen, Suzan Verberne and Milco Wansleeben", "id": "19"}, {"abstract": "Nearly all houses that are for sale are advertised on the internet. At the time of writing, there are over 130,000 residential properties listed on the biggest Dutch real estate website (funda.nl). Each listing usually consists of a free text description (436 words on average), some structured properties like the plot area and year of construction, and media files such as photos, a floor plan and videos. Each listing also obviously has an asking price. This project aims to automatically predict the asking price of a residential offer based on its textual description and its structured properties. The main advantage of an automated process is that it is faster and more objective than doing the same by hand. An automated system also allows for real time insight into the market, and could help both the seller in setting an appropriate asking price and the buyer in spotting out-of-range requests. We downloaded approximately 5,000 listings from funda.nl, including all municipalities in The Netherlands, for both houses and flats. For each listing, all available information, namely the description, the structured properties, and media was acquired and stored. The collected data was partitioned into training, development, and test sets. We approached asking price prediction as a regression problem, and we used mean absolute error, as well as relative error, to assess the results. A linear regression model and a basic neural network were trained to predict the asking price based on the description (sparse matrix) and the structural properties (dense matrix). In addition to these properties some external information was added related to the municipality the offer belongs to, such as local average income, unemployment rate, and degree of urbanness. We developed several regression models which combined the various features we consider, and observed that the contribution of the textual description is crucial towards lowering the error. We also found that some properties, such as plot area, year of construction and the bathroom count are substantially more informative than others. The best models we tested yielded a prediction that was off the actual asking price by approximately 20-25%. In order to further lower the error, we exploited the combination of different models. We did so in two major ways: one is multitask learning, were auxiliary tasks such as price per square meter or average income are learnt together with the main asking price task. The other one is stacking, where the output of the linear regressor and the output of the neural network are both informing a stacked model. We show that the combined models, especially when the views are truly different, yield the highest error reduction. In the presentation, ample space will be devoted to the explanation of such experiments, and also to some insights into data analysis.", "title": "Predicting the asking price of real estate offers", "authors": "Hendrik Prins and Malvina Nissim", "id": "18"}], [{"abstract": "We compare the output of commercial NMT, SMT and RBMT systems for English-to-Dutch, providing a fine-grained error analysis. Accordingly, we want to discover if the findings for other language pairs can be confirmed for English-to-Dutch as well. Additionally, we explore the actual improvements that NMT systems bring to automated translation, uncovering its potential shortcomings. Other than previous research, using engines developed in research institutes or building test suites for evaluation, our approach aims for a different angle; by using commercial MT systems and real-life texts from different genres we bring more ecological validity into the field. First we enriched the SCATE corpus of MT errors which differentiates between fluency (assessing the well-formedness of the target language) and accuracy errors (concerning the transfer of source content) with the error annotated translations of Google Translate (NMT). We adjusted the SCATE error taxonomy to suit NMT output as well. This allowed a fair comparison of the performance of the three paradigms for translation of non-fiction, external communication and journal articles. NMT in general outperforms SMT and RBMT, making less errors and generating more correct sentences. The NMT output is especially fluent. On the other side of the coin, accuracy errors can be fluently disguised. Omissions, for example, are more problematic than before, since the target sentence does not always contain traces or clues of content being missing. This has repercussions for quality estimation and gisting, operating only at monolingual level. Mistranslations remain a very prominent error category, especially word sense disambiguation remains a though nut to crack. But a new subcategory might be needed for mistranslations for which the target word(s) could never be a plausible translation of the given source word(s). These mismatches of unrelated source and target sometimes lead to hallucinating translations. In these cases the key to understanding can only be found in the source text, which will not always be accessible to the people using NMT for assimilation purposes. Error profiles applied to the text genres separately, will reveal if NMT still outperforms SMT and RBMT in specific domains as well. For instance the capability to translate named entities and terminology, a known strength of SMT, will be checked for NMT only taking into account the external communication texts in our corpus.", "title": "Comparing MT errors in NMT, SMT and RBMT output for English-Dutch", "authors": "Laura Van Brussel, Arda Tezcan and Lieve Macken", "id": "16"}, {"abstract": "Machine reading comprehension is a form of question answering in which the correct answer to a query can be found by reading a document. In this talk, we present our work in machine comprehension by focusing on the creation of a large resource that allows us to study reading comprehension in the clinical domain. We analyze the performance of both human and machine readers on a gap- filling task, and examine what skills are required to understand the supporting text and to provide the right answers. We also position our dataset and task formulation with respect to other popular reading comprehension works.", "title": "Clinical Machine Comprehension with Case Reports", "authors": "Simon Suster and Walter Daelemans", "id": "15"}, {"abstract": "We describe the Circumstantial Event Ontology (CEO), a newly developed ontology for calamity events in newswire that models semantic circumstantial relations between event classes, where we define circumstantial as inferred and implicit causal relations. For instance, the class ceo:Fire can be circumstantial with ceo:Extinguishing, based on a shared assertion where we define that the existence of fire is true while there is a fire and that it is true before some extinguishing process starts. The circumstantial relations are inferred from overlapping assertions in the formalization of the event classes; we do not define explicitly the relation itself as the relation may or may not be realised when some calamity is described. For example: not every fire gives rise to e.g. extinguishing, evacuations, casualties and rebuilding some place. However, if we find these events, a circumstantial relation is made. This way, we chain events and track back why something happened and what participants were involved. CEO Contents. The CEO is a manually built OWL2 ontology, which adopts the Event and Implied Situation Ontology model [Segers et al, 2016], and currently comprises 249 event classes, 298 class assertions, 88 roles and 105 properties. Further, CEO is fully mapped to the SUMO ontology [Niles et al., 2001] on class level (426 mappings) and to FrameNet [Ruppenhofer et al., 2006] on Frame level (560 mappings) and Frame element level (438 mappings). CEO runs on Semantic Role Labeling output. Evaluation. For the evaluation of CEO, we created a manually annotated corpus based on 18 topics describing calamities in ECB+ [Cybulska et al., 2014]. The eighteen topics cover about 370 articles. For each article, we annotated the mentions of the calamities, the coreference sets and the circumstantial relation between instances of the event mentions. At the time of submission, the annotation of this corpus, called ECB+/CEO is nearing completion. The ontology, the corpus and all mappings are made publicly available via a CC-BY-SA license. For reviewing purposes, the resources can be visited at: https://github.com/newsreader/eso-and-ceo.", "title": "The Circumstantial Event Ontology (CEO): A New Resource for Tracking Event Chains based on Implicit Semantic Relations.", "authors": "Roxane Segers, Piek Vossen and Tommaso Caselli", "id": "14"}, {"abstract": "With the increasing availability of large corpora, quantitative corpus analysis is becoming more and more popular as a method for doing linguistic research. The important insights that can be gained from larger-scale corpus studies, however, require programming skills that a linguistics researcher or student may not have. Our project, ACAD, addresses this caveat by developing an environment in which computationally naive linguists can carry out quantitative corpus analyses. The environment will be optimized for studies focusing on discourse coherence, but can be used for other types of searches as well. Our interface consists of a web-application \u2018Cesar\u2019, which provides an abstract layer for the user to specify searches. The user's searches are converted into Xquery and sent to the back-end of the CLARIN CorpusStudio web application (Komen, forthcoming). The Cesar interface gives access to several existing large syntactically annotated Dutch corpora (e.g., the SoNaR part of Lassy (van Noord, 2008, Oostdijk et al, 2013), CGN (van der Wouden 2002). We are adding a syntactically annotated version of the VU- DNC corpus (Vis, 2011), as well as two new corpora, consisting of, respectively, contemporary news articles (taken from the NRC) and WhatsApp messages. Cesar allows researchers to define search-projects equipped with data-dependent hit conditions and output features using a graphical interface that does not require computational expertise. Users will be able to share their own projects and copy projects shared by others, so that completely starting from scratch will often not be necessary. Furthermore, the share function allows instructors to set up a general project, in which students can make further specifications. Cesar comes with a research project that locates causal connectives (e.g., omdat \u2018because\u2019, want \u2018because\u2019, dus \u2018so\u2019) and identifies the discourse segments they relate to each other, see (1), in which the two discourse segments are set off by brackets. The project is also able to identify the direction of the causal relation (i.e., antecedent- consequent, as in (1), or consequent-antecedent), and whether the segments contain evaluative words, indicators of subjective claims and conclusions, which allows researchers to investigate whether a specific causal connective is used to mark subjective or more objective relations. (1) Omdat [klimaat nu eenmaal iets heel anders is dan het weer,] S1 [blijft het lastig om de opwarming van de aarde de schuld te geven van een zware regenbui of een extreem warme dag.] S2 [4] Because [climate is not the same as the weather,] S1 [it is hard to blame global warming for a heavy downpour or an extremely hot day.] S2 The causal-connective project can, for instance, be adapted to research other types of coherence connectives (e.g., signalling contrastive or additive relations), to examine the prototypical positioning of various connectives (i.e., before both segments or between the two segments), to compare the frequency of connectives between genres, and to research many other questions related to discourse coherence. In this poster, we give an overview of our project, discuss how we have adapted specific XML corpus queries to a format that can be used, modified, and recreated by computationally naive users, and showcase the environment we have developed.", "title": "Automating Coherence Analysis for the Computationally Challenged", "authors": "Erwin R. Komen and Jet Hoek", "id": "11"}], [{"abstract": "In\u200b the\u200b area\u200b of\u200b literary\u200b studies,\u200b there\u200b are\u200b currently\u200b two\u200b seemingly\u200b opposing\u200b approaches.\u200b Close reading\u200b deals\u200b with\u200b the\u200b detailed\u200b analysis\u200b of\u200b texts,\u200b whereas\u200b distant\u200b reading\u200b (Moretti,\u200b 2013)\u200b tries to\u200b capture\u200b the\u200b overarching\u200b trends\u200b in\u200b the\u200b texts\u200b (Stronks,\u200b 2013).\u200b On\u200b the\u200b one\u200b hand,\u200b given\u200b the huge\u200b amounts\u200b of\u200b texts\u200b available\u200b online,\u200b distant\u200b reading\u200b has\u200b several\u200b advantages\u200b as\u200b this approach\u200b allows\u200b for\u200b the\u200b automatic\u200b analysis\u200b of\u200b the\u200b texts.\u200b \u200b On\u200b the\u200b other\u200b hand,\u200b close\u200b reading reveals\u200b interesting\u200b details\u200b present\u200b in\u200b the\u200b text\u200b that\u200b are\u200b generalized\u200b over\u200b when\u200b using\u200b the distant\u200b reading\u200b approach. Both\u200b close\u200b and\u200b distant\u200b reading\u200b approaches\u200b have\u200b their\u200b benefits\u200b and\u200b problems.\u200b If\u200b we\u200b are interested\u200b in\u200b close\u200b reading\u200b results,\u200b but\u200b have\u200b to\u200b deal\u200b with\u200b large\u200b amounts\u200b of\u200b text,\u200b a \u200b combination of\u200b close\u200b and\u200b distant\u200b reading\u200b techniques\u200b may\u200b prove\u200b useful.\u200b By\u200b clustering\u200b similar\u200b texts\u200b using\u200b a distant\u200b reading\u200b technique,\u200b we\u200b can\u200b apply\u200b close\u200b reading\u200b techniques\u200b to\u200b only\u200b a \u200b selection\u200b of interesting\u200b texts. To\u200b cluster\u200b texts,\u200b we\u200b use\u200b LDA\u200b topic\u200b modeling\u200b (Blei\u200b et\u200b al.,\u200b 2003)\u200b as\u200b our\u200b distant\u200b reading technique.\u200b \u200b At\u200b the\u200b same\u200b time,\u200b we\u200b use\u200b manual\u200b close\u200b reading\u200b classification.\u200b If\u200b the\u200b documents grouped\u200b by\u200b LDA\u200b classes\u200b corresponds\u200b to\u200b the\u200b clusters\u200b formed\u200b using\u200b close\u200b reading,\u200b we\u200b can reduce\u200b the\u200b amount\u200b of\u200b manual\u200b labor\u200b and\u200b concentrate\u200b on\u200b close\u200b reading\u200b of\u200b those\u200b texts identified\u200b as\u200b interesting\u200b using\u200b LDA.\u200b To\u200b know\u200b whether\u200b this\u200b works,\u200b we\u200b need\u200b to\u200b evaluate\u200b how well\u200b the\u200b LDA\u200b classes\u200b correspond\u200b to\u200b the\u200b manually\u200b assigned\u200b close\u200b reading\u200b classes. Additionally,\u200b we\u200b need\u200b to\u200b find\u200b a \u200b way\u200b to\u200b identify\u200b how\u200b many\u200b LDA\u200b classes\u200b will\u200b need\u200b to\u200b be selected. In\u200b this\u200b presentation\u200b we\u200b will\u200b explain\u200b how\u200b we\u200b do\u200b this\u200b and\u200b show\u200b results\u200b on\u200b a \u200b dataset\u200b extracted from\u200b the\u200b online\u200b forum\u200b Reddit.\u200b \u200b The\u200b results\u200b show\u200b that\u200b LDA\u200b does\u200b lead\u200b to\u200b some\u200b overlap\u200b of\u200b the manually\u200b assigned\u200b classes,\u200b but\u200b the\u200b manual\u200b classes\u200b have\u200b different\u200b functions\u200b (not\u200b only\u200b topic based),\u200b leading\u200b to\u200b LDA\u200b being\u200b a \u200b technique\u200b that\u200b can\u200b only\u200b provide\u200b partially\u200b useful\u200b results\u200b for\u200b the identification\u200b of\u200b clusters\u200b of\u200b text\u200b that\u200b are\u200b required\u200b for\u200b close\u200b reading\u200b analysis.", "title": "From\u200b distant\u200b to close reading:\u200b evaluating\u200b the\u200b use\u200b of\u200b LDA\u200b topic modeling", "authors": "Christoph Aurnhammer, Iris Cuppen, Inge van de Ven and Menno van Zaanen", "id": "10"}, {"abstract": "This talk provides an overview of the different resources and technologies that are available for the creation of Dutch image description systems. I will discuss efforts to collect two corpora of Dutch image descriptions, and highlight differences between Dutch, English, and German image descriptions. These differences show us the limitations of translation-based approaches to build image description systems for other languages. Furthermore, I will discuss requirements for building useful image description technology, based on a survey of potential users of this technology.", "title": "Automatic image description for Dutch", "authors": "Emiel van Miltenburg", "id": "8"}, {"abstract": "Both monolingual automatic term extraction (ATE) and multilingual automatic term extraction from comparable corpora (ATECC) are productive fields of research within natural language processing. However, the evaluation of both tasks is still problematic, with many protocols and language-, domain- and task-dependent gold standards. The goal of this research was to construct a comprehensive, multilingual gold standard for ATECC. Since the first step of ATECC is monolingual ATE, monolingual gold standards were created first. Terms were manually annotated in three comparable corpora in three domains (medical \u2013 heart failure, juridical - corruption and technical \u2013 wind energy), each in three languages (English, French and Dutch). Termhood was defined based on lexicon-specificity and domain-specificity. This resulted in three domain- and language-independent term labels: Specific Terms, Common Terms and Out-of-Domain Terms. The annotation scheme allowed intuitive and consistent annotation as shown by the inter-annotator agreement scores, and, ultimately, a fine-grained evaluation which could be tuned for different applications. Based on the medical corpus, in which all terms were annotated in all three languages, a multilingual gold standard for ATECC was created. In ATECC, lists of monolingually extracted term candidates in each language are used to find term equivalents. For each term candidate in the source language, an ordered list of equivalents is suggested, chosen from the term candidates in the target language. To acquire all information from the corpus that is necessary to evaluate this process, each term was manually linked to all its variations, all possible term equivalents in the other languages and several other related terms, such as hypernyms. With the resulting gold standard, both precision and recall of ATECC systems can be accurately evaluated. Moreover, any mistake can be traced back to its origin: when a system suggests a wrong translation equivalent, either it didn\u2019t find the correct equivalent or the correct equivalent wasn\u2019t present in the provided corpus and the system isn\u2019t to blame. In summary, the gold standard allows a comprehensive and detailed evaluation of ATECC. Furthermore, the volume and detail of the annotations make the gold standard a rich source of information about terms. It sheds light on term structure (term length and part-of-speech patterns) and how that differs between different languages and domains. It reveals how much term variation exists, even in a clean corpus of published medical abstracts and short papers, and how terms vary in different languages. Additionally, it shows how many term equivalents one can expect to find in an unaligned, comparable corpus. In conclusion, in answer to the need for a re-usable evaluation protocol for ATECC, we developed a detailed and complete gold standard in three languages. Besides being a valuable tool for evaluation, this gold standard also provides a rich source of information about terms.", "title": "Gold Standard for Multilingual Automatic Term Extraction from Comparable Corpora", "authors": "Ayla Rigouts Terryn, Veronique Hoste and Els Lefever", "id": "7"}, {"abstract": "The usefulness of the content on social media for a particular statistical use depends on the characteristics of the users that contribute to generating it. Thus, the level of our understanding of the user characteristics affects the quality and the completeness, or selectivity, of the outcome. As such, every social media use case needs to be aware of the characteristics of the users in the tweets collected and analyzed. Especially when social media messages are used as an input source for official Dutch statistics, the composition of the population producing these messages should -by definition-be located in the Netherlands. The language in which a tweet is written is an important criterion for that, but the language of a tweet as detected by Twitter is not enough. The study reported here focusses on classifying users that write tweets in the Dutch language, in one of its two main dialects: 1) the language used in the Netherlands (simply called Dutch from hereon) and 2) the language used in Flanders (Flemish). So, to be absolutely clear, this study only focusses on discerning Twitter messages written in Flemish from those written by inhabitants of the Netherlands; no other dialects were included here. The study consists of two steps. The first step is to use the data in the location field of a Twitter user profile to determine the location of a user. This is often a municipality. The goal of the first step is the creation of a set of users for which it is certain that they are located in either the Netherlands or Flanders. The next step is to use the text of the tweets to identify the dialect of a user. Our sample is all users from 2016 on TwiNL (Tjong Kim Sang and van den Bosch, 2013). Place recognition was performed using Pyparsing (McGuire, 2007) on the profile location of a Twitter user. We developed grammars that enable disambiguation of place names by using language specific rules. The rules contain definitions about the context of a place name in the text as well. The context is defined in terms of separator punctuation, conjunctives, determiners, directions or affixes. The result of all of this is the identification of 1,484,427 unique location strings, of which 54,301 could be parsed and were either located in the Netherlands or Belgium. The recognized place names occur in the location field of 825,373 (14%) of the users that have a location string on their profile. This is 9% of the whole population. We trained a support vector machine (SVM) classifier using the Corpus Gesproken Nederlands (CGN) and tweet text data that is collected from users that states their location on their profile. We extracted unigram and bigram bag-of-words (BoW) features and weight them with a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus (TF-IDF). The classifier accuracy and F1 on a test set that is created based on user profile location are 0.97 and 0.98 respectively. In addition to the performance metrics, we will present a detailed performance and error analysis of the results of location parsing and tweet text classification.", "title": "Using Location and Dialect to Classify Twitter Users from the Netherlands and Flanders", "authors": "Ali H\u00fcrriyeto\u011flu and Piet Daas", "id": "4"}]], "title": "Poster Session", "location": "Foyer begane grond", "start": "13:30", "end": "14:30"}, {"description": "", "end": "15:30", "subslots": [[{"abstract": "Representation learning techniques have been used extensively within and outside the clinical domain to learn the semantics of words, phrases, and documents. We apply such techniques to create a patient semantic space where \u201csimilar\u201d patients should have similar vectors. We aim to capture patient similarity on multiple dimensions which would encapsulate a holistic view of the patients. To address the cases where there is a dearth of labeled data despite the abundance of unlabeled data, we create unsupervised dense patient representations from clinical notes in the freely available MIMIC-III database. We explore the usage of two neural representation learning architectures\u2014a stacked denoising autoencoder (SDAE), and a paragraph vector architecture\u2014for unsupervised learning. We evaluate these representations by predicting patient mortality, the primary diagnostic and procedural categories for the patients, and the gender of the patients. We compare their performance with sparse representations obtained from the bag-of-words and the bag-of-medical-concepts models. We observe that the generalized representations outperform the sparse representations when we have very few positive instances to learn from, and there is an absence of strong lexical features. It is critical to understand model behavior when statistical outputs influence clinical decisions. However, dense representations often capture semantics at a loss of interpretability. To interpret the dense representations learned by the SDAE, we calculate the mean squared reconstruction error of every feature after training the first layer of the SDAE. We observe a very high positive correlation between feature frequency and the reconstruction error, which we believe may be due to the high entropy of the frequent terms. Further, we calculate feature sensitivity across networks to extract the most influential features for the classification output on using the SDAE representations as the input. We find that the classifiers give high importance to sensible frequent features for most of the tasks. Although we input a bag-of-words representation to the SDAE, co-occurrence of different terms is reflected in the extracted features. We further observe that there is a minimal overlap between the sets of important features for the different tasks, which reflects the task-independent nature of the representations.", "title": "Unsupervised dense patient representations with interpretable classification decisions", "authors": "Madhumita Sushil, Simon Suster, Kim Luyckx and Walter Daelemans", "id": 41}, {"abstract": "t is generally accepted that the form of a text is influenced by various factors, foremost among which are a) who wrote it (author), b) what it is about (topic), and c) which form was chosen to optimally reach the intended effect in the intended target audience (communicative situation, more or less also covered by the terms genre/register). For each of these three factors, there are NLP tasks to identify them for a specific text. From these topic detection and authorship recognition are most likely known to the CLIN audience. Genre recognition has drawn less attention so far, and is often limited to high level distinctions, with broad classes like fiction and academic writing. However, that genre/register do influence language use in a text will be clear to anyone attempting authorship recognition, where both genre/register and topic are strong confounding factors in the desired text classification. We would like to advance all three classification tasks by mapping which classification features are bound to which of the three factors, and to which degree. Before attacking this ultimate question, however, we first need some more clarity regarding genre/register. Especially, we would like to employ much more fine-grained groupings, such as romance fiction and medical texts. This means we have to determine whether such labels lead to groups of texts which have enough internal consistency, and enough contrast with other groups, that they can be recognized. In this talk, we start our investigation with romance fiction, a genre for which we most expect to find clear stylistic properties. Using texts from the British National Corpus, we look at various stylistic features, and investigate how these differ between romance fiction and other texts, but also how they vary within the genre, e.g. between authors.", "title": "Fine-grained genre/register recognition: romance fiction in the BNC", "authors": "Hans Van Halteren and Nelleke Oostdijk", "id": 64}, {"abstract": "Whereas parser performance on news texts keeps getting closer to human performance, parsers still perform drastically worse out-of-domain. Multiple approaches have already been explored for domain adaptation; up-training, weighing of training data and integrating disfluency detection are some common approaches. In this work we will focus on another approach: normalization. More concretely, we examine different approaches on integrating normalization into a neural network dependency parser. We will focus on Twitter data; we annotated a small treebank using the Universal Dependencies format for evaluation purposes. Our baseline parser is the UUParser(de Lhoneux et al, 2017), which is an Arc-Hybrid BiLSTM parser. This parser exploits character embeddings, and includes an option to initialize with pre-trained word embeddings. We use word2vec to train word embeddings on a big Twitter corpus. Both the character embeddings and the pre-trained model increased the performance of the parser. In a domain adaptation setup, where we train on Wall Street Journal and test on Twitter, the performance improvement is even bigger. This is probably mainly an effect of solving the unknown word problem, which is also adressed by a normalization-based approach. This leads to our main research question: can we make use of normalization to increase performance beyond the use of character level models and pre-trained word embeddings? We use an existing normalization model, which does normalization on the word level. It generates candidates using the Aspell spell checker, word embeddings and a lookup list. Features are directly taken from the generation step and supplemented with N-gram probabilities. A random forest classifier is then trained to predict the probability that a candidate belongs to the \u2018correct\u2019 class; this enables the system to output a top-n list of candidates and their probabilities. When using normalization as a straightforward pre-processing component, we observe a small increase in LAS. However, the normalization component makes mistakes, these propagate directly to the parser. And even if we would have access to a perfect normalization sequence, it might still be informative to take the original token into account during parsing. To fully exploit the potential of the normalization model we combine the vectors of the top-n normalization candidates into one vector. We weight the vector of each candidate by the probably from the normalization model, and then sum the vectors of all candidates.It should be noted that this approach can also be generalized to other neural network parsers, and even to other tasks. In our initial experiments normalization improved parser performance, even when using charactel embeddings, and pre-trained word embeddings. The in- tegration of multiple normalization candidates improved performance even further, indicating that the top-n candidates are also informative. More in-depth evaluation will be included in the presentation.", "title": "Lexical Normalization for Neural Network Parsing", "authors": "Rob van der Goot and Gertjan Van Noord", "id": 38}, {"abstract": "Due to the small corpus of biblical Hebrew, dating texts based on linguistic properties is tedious. Young, Rezetko, and Ehrensv\u00e4rd (2008) collected 88 grammatical properties from literature that could be typical of late biblical Hebrew, but conclude that they are (almost) all fuzzy and/or rare, making it hard, if not impossible, to date texts based on them. We translate these properties to queries to the ETCBC treebank of the Hebrew Bible (Roorda et al. 2017) and train a classifier on them to distinguish early and late biblical Hebrew. This approach allows us to consider many linguistic properties at once, which helps resolve issues of fuzziness and scarceness. Considering the size of the corpus (ca. 420,000 words), we cannot expect a classifier to date texts with very high granularity or accuracy. However, inspecting the models after training provides insights into the patterns behind language variation. Working with small corpora is part and parcel of the study of many dead languages. Com- putational methods can prove useful, but must be used carefully to avoid over-fitting. While developing and applying them, linguistic expertise must be consulted continuously. We show what methods, in particular sliding windowing and leave-one-out cross validation, can help with scarcity of data and rare features. We discuss which pitfalls one has to avoid to keep the analysis sound.", "title": "Combining grammatical features for dating texts in small-corpus languages", "authors": "Camil Staps", "id": 42}], [{"abstract": "Neural Machine Translation (NMT) models have recently become the state-of-the art in the field of Machine Translation (Bahdanau et al. 2014, Cho et al. 2014, Kalchbrenner et al. 2014, Sutskever et al. 2014). Compared to Statistical Machine Translation (SMT), the previous state- of-the-art, NMT performs particularly well when it comes to word-reorderings and translations involving morphologically rich languages (Bentivogli et al. 2016). Although NMT seems to partially learn or generalize some patterns related to syntax from the raw, sentence-aligned parallel data, more complex phenomena (e.g. prepositional-phrase attachment) remain problematic (Bentivogli et al. 2016). More recent work showed that explicitly modeling extra syntactic information into an NMT system on the source (and/or target) side improves the translation quality 1 : Sennrich and Haddow (2016) integrated morphological information, POS-tags and dependency labels in the form of features on the source side of the NMT model while Nadejde et al. (2017) introduced syntactic information in the form of CCG supertags on both the source and the target side. Moreover, Nadejde et al. (2017) showed that a shared embedding space, where syntax information and words are tightly coupled, is more effective than multitask training. When integrating linguistic information into an MT system, following the central role assigned to syntax by many linguists, the focus has been mainly on the integration of syntactic features. Although there has been some work on semantic features for SMT (Banchs and Costa-Juss\u00e0 2011), so far, no work has been done on enriching NMT systems with more general semantic features at the word-level. This might be explained by the fact that NMT models already have means of learning word-embeddings since words are represented in a common vector space. However, making some level of semantics more explicitly available at the word level can provide the translation system with a higher level of abstraction and generalization beneficial to learn more complex constructions. Furthermore, a combination of both syntactic and semantic features would provide the NMT system with a way of learning semantico-syntactic patterns. To apply semantic abstractions at the word-level that enable a characterisation beyond that what can be superficially derived, coarse-grained semantic classes can be used. Inspired by Named Entity Recognition (NER) which provides such abstractions for a limited set of words, supersense- tagging uses an inventory of more general semantic classes (for nouns and verbs) for domain-independent settings (Schneider and Smith 2015). We investigate the effect of integrating super-sense features (26 for nouns, 15 for verbs) into an NMT system. To obtain these features, we used theAMALGrAM 2.0 tool (Schneider et al. 2014, Schneider and Smith 2015) which analyses the input sentence for multi-word expressions as well as noun and verb supersenses. The features are integrated using the framework of Sennrich et al. (2016), replicating the tags for every subword unit obtained by byte-pair encoding (BPE). We further experiment with a combination of semantic supersenses and syntactic supertag features (CCG syntactic categories (Steedman 2000) using EasySRL (Lewis et al. 2015)) and less complex features such as POS-tags, assuming that supersense-tags have the potential to be useful especially in combination with syntactic information.", "title": "SuperNMT: Integrating Supersense and Supertag Features into Neural Machine Translation", "authors": "Eva Vanmassenhove and Andy Way", "id": 71}, {"abstract": "This presentation introduces BasiScript, a 9-million-word corpus of contemporary Dutchtexts written by primary school children. The data were collected over three years with manychildren contributing texts throughout this period. Each word token in the corpus isannotated with the correct orthographical form, the associated lemma and the part-of-speech.The most frequent polysemous words have been annotated for word meaning, while allwords in the lexicon have been annotated for corpus and subcorpora frequency, distribution,length, family size, family frequency, orthographic neighborhood size, and orthographicneighborhood frequency. Images of the texts are available. The present article first describesthe corpus. Then it presents some results from an initial exploration of the corpus, namely, acomparison of BasiScript with BasiLex (a Dutch corpus with texts primary school childrenread) by means of frequency profiling, and an analysis of spelling errors children makeregarding diphthongs and verb forms.", "title": "BasiScript: a 9 million word corpus of texts written in primary school", "authors": "Agnes Tellings, Nelleke Oostdijk, Iris Monster, Franc Grootjen and Antal van den Bosch", "id": 2}, {"abstract": "This work addresses the phenomenon of (ir)regularity in the past tense forms of verbs, a subject widely studied in linguistics (Pinker and Ullman, 2002; Pinker, 2001; Prasada and Pinker, 1993; Bybee and Moder, 1983). It is a common assumption, claimed by, among others, Kim et al. (1991), that the formation of either a regular or an irregular preterite form is exclusively due to phonological or orthographic form. The works of Baayen and Moscoso del Prado Mart\u0131\u0301n (2005) and Tabak et al. (2005) allow to question this assumption by stressing the additional importance of semantic density for past tense regularity. Inspired by their work, I present a memory-based model of past tense regularity of Dutch simplex verbs. The computational model investigates the importance of phonological similarity and semantic density of verbs for the regularity of their past tense forms. The dataset of Baayen and Moscoso del Prado Mart\u0131\u0301n (2005) is modeled using memory-based classification. This is implemented by the IB1 algorithm in TiMBL (Daelemans et al., 2004). The classifier is presented with, first, phonological transcriptions of verbs, second, semantic density measures and, last, a combination of the two. Using the combined feature set results in the highest classification accuracy (85,8%). Classification turns out most difficult on the irregular class. An analysis of classification errors shows that adding semantic density information resolves erroneous regularization. The computational model thus replicates the findings of Baayen and Moscoso del Prado Mart\u0131\u0301n (2005) and Tabak et al. (2005) by demonstrating the importance of semantic density in Dutch verb's past tense regularity. Beyond that, it demonstrates the potential of purely memory-based language processing to model regular -- irregular distinctions.", "title": "Memory Based Modelling of Dutch Past Tense Regularity", "authors": "Christoph Aurnhammer", "id": 1}, {"abstract": "The Nederlab projects makes available a large diachronic corpus of Dutch, from the 6th until the 21st century in a virtual research environ- ment which allows extensive search of the texts. The corpus is based on available digital text material. The text material is converted into one format, FoLiA (van Gompel, 2012), and special attention is given to uniform metadata. To try to enhance the quality of the OCR\u2019ed material in the corpus, automatic post-correction is applied with TICCL (Reynaert, 2010). To allow for better searching, the text material is annotated with Part of Speech information (PoS) and lemma. Another annotation layer is the links from named entities to an external resource like Wikipedia. In this talk we will focus on the linguistic annotation with PoS and lemma and more in particular, the various challenges we have been facing, tools for linguistic annotation need training and training material. We deliver the Nederlab enrichment pipeline is delivered as part of PICCL. One of the main challenges is that historical training material is scarce and hard to come by in sufficient quantity, and even more so in the case of Nederlab, where a very large time span is covered. From the start of the project, it was therefore decided to work with Frog (Van den Bosch et al., 2007) trained on a corpus of approximately 11 million word forms annotated with the tagset for the Corpus Gesproken Nederlands (van Eynde, 2004) and to see how we could improve the tagging of historical Dutch by adding a modern Dutch translation. The experiment was performed on 17th Cen- tury Dutch [Sang, 2016]. Exploratory evaluation of the results of this ap- proach brought to light several issues: a) the CGN tagset is not entirely suitable for historical Dutch b) our current tokenisers cannot deal with the fact that words in historical Dutch have spaces, and c) the approach by which tagging is improved by translation of historical Dutch is only applicable for text material until 1800. After having made an inventory of existing training material and a division into different language phases, it was decided to go for a mixed approach for linguistic annotation, implying conversion and training of existing golden standards. But it was also decided to produce enough evaluation material for each language phase for the annotation with main PoS and lemma, so as to be able to assess the quality of the approach taken. The evaluation material will be made publicly available. In this way, we hope to come to solid recommendations for future work.", "title": "Nederlab: Progress and challenges in linguistic enrichment of historical Dutch Texts", "authors": "Katrien Depuydt, Maarten Van Gompel, Hennie Brugman, Jesse de Does and Gosse Bouma", "id": 57}], [{"abstract": "Distributed\u200b word\u200b embeddings\u200b offer\u200b continuous\u200b vector\u200b representations\u200b that\u200b can\u200b capture\u200b rich contextual\u200b semantics.They\u200b have\u200b become\u200b the\u200b standard\u200b component\u200b in\u200b downstream\u200b NLP\u200b systems, and\u200b help\u200b models\u200b with\u200b modern\u200b neural\u200b network\u200b architectures\u200b achieve\u200b state\u200b of\u200b the\u200b art\u200b results\u200b on most\u200b NLP\u200b tasks.\u200b In\u200b addition\u200b to\u200b improving\u200b the\u200b performance\u200b on\u200b monolingual\u200b NLP\u200b tasks,\u200b shared representations\u200b of\u200b words\u200b across\u200b languages\u200b offers\u200b intriguing\u200b possibilities.\u200b Crosslingual embedding\u200b can\u200b potentially\u200b enrich\u200b the\u200b monolingual\u200b learning\u200b in\u200b languages\u200b with\u200b less\u200b training\u200b data. For\u200b example,\u200b we\u200b could\u200b borrow\u200b data\u200b from\u200b a language\u200b with\u200b sufficient\u200b training\u200b data\u200b to\u200b help\u200b train\u200b a model\u200b in\u200b another\u200b language\u200b where\u200b only\u200b limited\u200b resources\u200b are\u200b available.\u200b And\u200b one\u200b step\u200b further,\u200b we might\u200b be\u200b able\u200b to\u200b deploy\u200b models\u200b trained\u200b in\u200b one\u200b language\u200b to\u200b work\u200b on\u200b problems\u200b in\u200b other languages. Motivated\u200b by\u200b these\u200b insights,\u200b this\u200b project\u200b studied\u200b techniques\u200b for\u200b crosslingual\u200b embedding,\u200b and used\u200b them\u200b in\u200b a recurrent\u200b neural\u200b network\u200b sequence\u200b labelling\u200b model.\u200b Pioneer\u200b works in\u200b this fields\u200b focused\u200b on\u200b bilingual\u200b word\u200b embeddings\u200b from\u200b a pair\u200b of\u200b languages.\u200b Recently,\u200b there\u200b are\u200b also studies\u200b to\u200b align\u200b real\u200b crosslingual\u200b embeddings.\u200b In\u200b line\u200b with\u200b these\u200b studies,\u200b we\u200b propose\u200b a canonical\u200b correlation\u200b analysis\u200b (CCA)\u200b based\u200b transformation\u200b method\u200b to\u200b learn\u200b shared\u200b semantic representations\u200b by\u200b aligning\u200b word\u200b embeddings\u200b from\u200b multiple\u200b languages. The\u200b embedding\u200b is\u200b then\u200b used\u200b in\u200b the\u200b input\u200b layer\u200b of\u200b a bidirectional\u200b LSTM\u200b network.\u200b While\u200b our previous\u200b HMM/CRF\u200b based\u200b model\u200b has\u200b used\u200b hand-engineered\u200b features\u200b that\u200b are\u200b hard\u200b to\u200b be generalized\u200b across\u200b languages,\u200b the\u200b neural\u200b network\u200b model\u200b is\u200b able\u200b to\u200b learn\u200b relevant\u200b features automatically\u200b with\u200b better\u200b generalization,\u200b and\u200b enables\u200b rapid\u200b development\u200b of\u200b new\u200b language models\u200b at\u200b lower\u200b cost. In\u200b the\u200b present\u200b work,\u200b we\u200b aligned\u200b word\u200b embeddings\u200b of\u200b 4 languages\u200b (English,\u200b German,\u200b French, and\u200b Dutch).\u200b We\u200b evaluated\u200b the\u200b quality\u200b of\u200b the\u200b crosslingual\u200b embeddings\u200b by\u200b checking\u200b the neighbouring\u200b words\u200b of\u200b a \u200bword\u200b from\u200b another\u200b language,\u200b for\u200b example,\u200b the\u200b top\u200b 5 similar\u200b French words\u200b of\u200b a given\u200b English\u200b word.\u200b The\u200b experiment\u200b also\u200b shows\u200b that\u200b using\u200b the\u200b crosslingual embedding,\u200b a model\u200b trained\u200b on\u200b only\u200b 10%\u200b of\u200b the\u200b German\u200b training\u200b data\u200b combined\u200b with\u200b the\u200b English data\u200b performs\u200b similar\u200b to\u200b the\u200b baseline\u200b model\u200b trained\u200b purely\u200b on\u200b 100%\u200b German\u200b training\u200b data.", "title": "Towards multilingual sequence labelling using cross-lingual word embeddings", "authors": "Chao Li, Carsten van Weelden, Luigi Lorato, Carsten L. Hansen, Mihai Rotaru and Jakub Zavrel", "id": 50}, {"abstract": "We present the results of a study into interpolation factors of a Bayesian skipgram language model based on the hierarchical Pitman-Yor processes (HPYPLM). We apply our language models in Dutch and English to two tasks, namely the intrinsic task of language modelling and the extrinsic task of automatic speech recognition. In the first task, we show that skipgram language models outperform n-gram language models, in and across different text genres, with a large drop in perplexity. We confirm this behaviour in a series of experiments in the second task, where we show a reduction in word error rate. Additionally, we show that we can improve over the naive skipgram baseline by some simple heuristics in the form of backoff strategies and interpolation factors.", "title": "Reducing perplexities and word error rates by improving the interpolation factors of a Bayesian skipgram language model", "authors": "Louis Onrust, Hugo Van Hamme and Antal van Den Bosch", "id": 72}, {"abstract": "Translation memories (TM) are tools which store sentences and their translation and allow for retrieving fuzzy matches based on similarity. In translation workflows, TMs are often used independently from tools for machine translation (MT), in spite of accumulating evidence that the integration of both tool types can lead to gains in translation efficiency. In the SCATE project (Smart Computer-Aided Translation Environment), we develop a method for integrating TM and MT which constrains MT output by pre-translating parts of the input sentence using consistently aligned sub-segments stemming from one or more TM matches. Pre-translation takes place using the XML markup functionality in the Moses toolkit for statistical MT. Our integrated system adopts two sliders to associate a fuzzy match score range with one of three strategies: the first one selects MT output, the second one selects the translation of the best fuzzy match (TM output), and the third one chooses the output resulting from pre-translation. In case of high-scoring fuzzy matches, an additional strategy \u201crepairs\u201d the translation of a match through small changes. Our first baseline consists of pure MT output for all test sentences, our second baseline of pure TM output. We tune our integrated system using a hill climber and evaluate translations by means of three metrics (BLEU, METEOR and TER). We perform tests on the TM of the Directorate-General for Translation of the European Commission, for ten language pairs involving different families (English to Dutch, German, French, Polish and Hungarian, and vice versa). Some of these pairs are known to be challenging for MT due to differences in word order and richness of the morphological system. For all language pairs, we observe significantly better scores when applying our integrated system. It has been incorporated into the prototype of the SCATE project, which demonstrates an interface for professional translators and presents different types of translation suggestions. The integration of TM and MT being work in progress, we discuss further research related to the construction and evaluation of a neural MT system for the above TM and language pairs.", "title": "TM-MT: the SCATE sliders", "authors": "Tom Vanallemeersch, Vincent Vandeghinste and Bram Bult\u00e9", "id": 28}, {"abstract": "he\u200b availability\u200b of\u200b large\u200b text\u200b archives\u200b such\u200b as\u200b Delpher\u200b present\u200b humanities\u200b researchers\u200b with\u200b a wealth\u200b of information.\u200b However,\u200b such\u200b archives\u200b do\u200b not\u200b provide\u200b researchers\u200b with\u200b a precisely\u200b scoped\u200b collection\u200b that fits\u200b their\u200b research\u200b question.\u200b The\u200b current\u200b research\u200b practice\u200b involves\u200b sending\u200b a query\u200b to\u200b an\u200b archive interface,\u200b click\u200b every\u200b document\u200b link,\u200b read\u200b the\u200b document\u200b and\u200b record\u200b whether\u200b it\u200b is\u200b relevant\u200b to\u200b their research\u200b question\u200b or\u200b not.\u200b Automating\u200b this\u200b step\u200b would\u200b leave\u200b researchers\u200b with\u200b more\u200b time\u200b to\u200b analyse\u200b only relevant\u200b documents\u200b to\u200b answer\u200b their\u200b research\u200b question. The\u200b CLARIAH\u200b SERPENS\u200b pilot\u200b project\u200b aims\u200b to\u200b aid\u200b historical\u200b ecologists\u200b in\u200b finding\u200b information\u200b from\u200b the Dutch\u200b National\u200b Library\u2019s\u200b newspaper\u200b collection\u200b by\u200b employing\u200b semantic\u200b web\u200b and\u200b language\u200b technology. Historical\u200b ecology\u200b is\u200b an\u200b interdisciplinary\u200b research\u200b field\u200b that\u200b investigates\u200b the\u200b interactions\u200b between humans\u200b and\u200b their\u200b environment.\u200b The\u200b research\u200b in\u200b SERPENS\u200b is\u200b driven\u200b by\u200b the\u200b question\u200b how\u200b the\u200b public perception\u200b of\u200b certain\u200b species\u200b changes\u200b over\u200b time\u200b (Lenders, 2014).\u200b To\u200b this\u200b end,\u200b we\u200b obtained\u200b a set\u200b of\u200b documents\u200b from the\u200b Dutch\u200b National\u200b Library\u2019s\u200b newspaper\u200b collection\u200b by\u200b querying\u200b for\u200b \u201cLynx\u201d\u200b and\u200b \u201cBunzing\u201d\u200b (European polecat).\u200b However,\u200b we\u200b find\u200b that\u200b many\u200b returned\u200b articles\u200b do\u200b not\u200b concern\u200b animals\u200b but\u200b for\u200b example\u200b a magazine\u200b or\u200b ship\u200b whose\u200b name\u200b is\u200b derived\u200b from\u200b an\u200b animal\u200b name. During\u200b the\u200b first\u200b phase\u200b of\u200b the\u200b project,\u200b we\u200b identified\u200b the\u200b issues\u200b that\u200b complicate\u200b collecting\u200b relevant documents,\u200b we\u200b defined\u200b relevant\u200b document\u200b categories\u200b and\u200b performed\u200b a manual\u200b annotation\u200b of\u200b 7,902 documents\u200b to\u200b classify\u200b documents\u200b into\u200b one\u200b of\u200b these\u200b categories\u200b (\u200bvan\u200b Erp et al., 2017). In\u200b our\u200b first\u200b classification\u200b experiments,\u200b we\u200b focus\u200b on\u200b identification\u200b of\u200b OCR\u200b problems\u200b using\u200b a lexicon-based method\u200b as\u200b well\u200b as\u200b automatic\u200b classification\u200b experiments\u200b using\u200b the\u200b twelve\u200b categories\u200b defined\u200b in\u200b (\u200bvan\u200b Erp et al., 2017).\u200b In our\u200b first\u200b series\u200b of\u200b experiments,\u200b we\u200b achieve\u200b F1\u200b measures\u200b of\u200b 77.62\u200b (fastText)\u200b and\u200b 86.24\u200b (mallet)\u200b but\u200b more research\u200b is\u200b needed\u200b to\u200b overcome\u200b the\u200b class\u200b imbalance\u200b and\u200b classify\u200b the\u200b most\u200b important\u200b categories\u200b to ecological\u200b historians\u200b correctly. In\u200b the\u200b coming\u200b months,\u200b we\u200b will\u200b test\u200b unsupervised\u200b techniques\u200b such\u200b as\u200b LDA\u200b to\u200b try\u200b to\u200b strengthen\u200b our\u200b grip on\u200b the\u200b material,\u200b and\u200b test\u200b our\u200b techniques\u200b on\u200b a larger\u200b set\u200b of\u200b species.", "title": "Good lynx, bad lynx: document enrichment for historical ecologists", "authors": "Marieke van Erp, Jesse De Does, Thomas van Goethem and Katrien Depuydt", "id": 63}]], "title": "Parallel session no. 2", "sessions": [{"chair": "Grzegorz Chrupala", "title": "Deep learning", "location": "Kleine zaal"}, {"chair": "", "title": "Language variation", "location": "Leeuwzaal"}, {"chair": "", "title": "Parsing and Grammar", "location": "Soci\u00ebteitskamer"}, {"chair": "", "title": "NLP for historical text", "location": "Annazaal "}], "start": "14:30"}, {"title": "Coffee and beverages", "location": "Foyer begane grond", "start": "15:30", "end": "15:55"}, {"description": "", "end": "17:15", "subslots": [[{"abstract": "TBA", "title": "TBA", "authors": "Winner STIL Thesis Prize", "id": 99}, {"abstract": "Writing academic texts is a skill that students often find hard to acquire. Whereas several tools exist that support low-level text writing aspects, such as spelling correction systems, not many tools are available that support students in constructing the high-level structure of academic texts, or the sentence-level rhetorical structures from which arguments are constructed. To help students improve their writing, it is important to provide them with feedback that goes beyond low-level surface properties such as spelling, grammar, and mechanics, by giving feedback on high-level, textual structure and coherence. In order to develop a feedback system that helps students write coherent, well-structured academic texts, we first need to know what \u2018good\u2019 academic texts look like. For this, we collected texts written by students in two disciplines: Law and Science. These texts have been analyzed at the level of rhetorical moves, which show the function of sentences in the text. The extracted rhetorical moves are: attitude, background, contrast, contribution, emphasis, novelty, question, surprise, trend, and very important sentence. This type of information does not aim to describe the overall structure of the text, but indicates the relationships between and functions of the sentences in the text. Sequences of rhetorical moves of a text can indicate the relationship between sentences, which provides information on the overall structure of the text. Information found in these sequences may be a stronger indicator of well structured and coherent texts. Most work on analyzing and evaluating writing products has focused on frequencies of textual features. While this may provide interesting and useful information, it does not allow for the uncovering of higher level sequential information as context is not taken into account. In this presentation, we will introduce a technique that identifies patterns (sequences of rhetorical moves) that can be found in texts from the Law and Science disciplines. The technique relies on notions from the field of information retrieval. Based on the information found by the technique, we show that we can identify differences in structure of the text between the different disciplines. The \u2018differential\u2019 patterns that characterize each of the two disciplines can be explained from the nature of the information that is described in the texts. The ultimate aim of the technique is to provide insight into the patterns of rhetorical moves that are preferred in texts in a specific discipline. This allows for the development of an automatic feedback system that can help students with the high-level structure in the texts they write.", "title": "Patterns of rhetorical structure in student\u2019s writing in different text genres", "authors": "Rianne Conijn, Simon Knight, Roberto Martinez and Menno van Zaanen", "id": 12}, {"abstract": "Neural\u200b translation\u200b systems\u200b typically\u200b use\u200b a \u200b limited\u200b vocabulary\u200b in\u200b the\u200b target\u200b language\u200b to\u200b reduce computational\u200b demand\u200b during\u200b training.\u200b Because\u200b of\u200b this,\u200b neural\u200b models\u200b tend\u200b to\u200b generate more\u200b unknown\u200b words\u200b ('<unk>')\u200b during\u200b translation\u200b than\u200b statistical\u200b or\u200b phrase-based\u200b models. Neural\u200b network\u200b translation\u200b has\u200b repeatedly\u200b shown\u200b its\u200b worth\u200b in\u200b recent\u200b years,\u200b but\u200b vocabulary limitations\u200b and\u200b the\u200b unknown\u200b word\u200b problem\u200b remain\u200b a \u200b hurdle\u200b for\u200b further\u200b improvement. Named\u200b entities\u200b contribute\u200b a \u200b large\u200b portion\u200b of\u200b these\u200b unknown\u200b words;\u200b robust\u200b named\u200b entity resolution\u200b can\u200b immediately\u200b improve\u200b any\u200b neural\u200b system.\u200b This\u200b work\u200b explores\u200b the\u200b feasibility\u200b of\u200b a full-size\u200b neural\u200b model\u200b augmented\u200b with\u200b dedicated\u200b named\u200b entity\u200b resolution\u200b components\u200b for English\u200b to\u200b Chinese\u200b translation. We\u200b focus\u200b on\u200b the\u200b transcription\u200b of\u200b personal\u200b and\u200b place\u200b names\u200b from\u200b English\u200b into\u200b Chinese. Resolving\u200b such\u200b entities\u200b in\u200b language\u200b pairs\u200b using\u200b different\u200b writing\u200b systems\u200b is\u200b challenging,\u200b but important\u200b for\u200b a \u200b commercial\u200b translation\u200b system.\u200b Specifically,\u200b we\u200b show\u200b a \u200b method\u200b to automatically\u200b transcribe\u200b English\u200b names\u200b (or\u200b any\u200b name\u200b in\u200b the\u200b Latin\u200b alphabet)\u200b into\u200b Chinese characters.\u200b This\u200b approach\u200b consists\u200b of\u200b two\u200b components:\u200b a \u200b neural\u200b network\u200b trained\u200b for character-based\u200b transcription\u200b and\u200b a \u200b lookup\u200b dictionary\u200b or\u200b gazetteer.\u200b The\u200b gazetteer\u200b handles names\u200b not\u200b fit\u200b to\u200b be\u200b transcribed\u200b automatically. We\u200b derive\u200b a \u200b list\u200b of\u200b English-Chinese\u200b name\u200b pairs\u200b from\u200b a \u200b corpus\u200b of\u200b wikipedia\u200b article\u200b titles\u200b using a\u200b custom-made\u200b method.\u200b Part\u200b of\u200b this\u200b list\u200b becomes\u200b training\u200b data\u200b for\u200b the\u200b transcription\u200b network, the\u200b other\u200b becomes\u200b the\u200b gazetteer\u200b list.\u200b We\u200b find\u200b our\u200b transcription\u200b network\u200b generates\u200b useable transcriptions\u200b despite\u200b its\u200b simplicity. We\u200b train\u200b three\u200b full-size\u200b neural\u200b models\u200b using\u200b the\u200b OpenNMT\u200b framework:\u200b one\u200b baseline,\u200b one where\u200b named\u200b entities\u200b have\u200b been\u200b normalized,\u200b and\u200b one\u200b where\u200b entities\u200b consisting\u200b of\u200b multiple tokens\u200b are\u200b concatenated.\u200b Random\u200b artefact\u200b generation\u200b in\u200b the\u200b normalized\u200b model\u200b point\u200b to undesirable\u200b interference\u200b between\u200b our\u200b normalization\u200b methods\u200b and\u200b OpenNMT\u200b parameters;\u200b we correct\u200b for\u200b this.\u200b We\u200b augment\u200b both\u200b model\u200b with\u200b our\u200b phrase\u200b table\u200b system\u200b and\u200b transcription module\u200b and\u200b compare\u200b the\u200b results.", "title": "Improving named entity resolution for English to Chinese neural machine translation", "authors": "Camiel Colruyt", "id": 17}, {"abstract": "Translation plays a key role in interlingual communication. With the advent of big data there is a growing demand for translation, which cannot be dealt with by human translators alone. An efficient solution is machine-assisted translation, in which machine translation (MT) assists the human translator. However, machine-assisted translation of literary texts such as novels remains challenging because literary translation must preserve not only the meaning of the source text but also its reading experience. As a result, novels are still translated manually. While the perceived wisdom is that MT is of no use for novels, empirical evidence to support this thinking is thin and fragmented and the topic remains largely unexplored. Due to the emergence of a radically new paradigm to automated translation, neural MT (NMT), which has raised the expectations of what MT can offer, we deem it timely to revisit the question of whether MT can be useful to assist with the translation of literary text. This gives us the opportunity to test NMT on a particularly challenging use case, which we hope should contribute to shed further light on the real potential of this newly introduced approach to MT. This work builds upon our recent study (Toral, 2017), in which we built an NMT system tailored to novels for the English-to-Catalan language direction and evaluated it on a set of 12 widely known novels against a system based on the \"classic\" statistical phrase-based approach (PBMT). NMT outperformed PBMT for every novel, resulting overall in 11% relative improvement according to the BLEU evaluation metric. In this work we use this NMT system to assist professional translators. To this end we use the post-editing workflow, so the novel is first translated automatically using an MT system, and subsequently this MT output is edited by translators. Our case study is Warbreaker, a popular fantasy novel originally written in English, which we translate into Catalan. We translated one chapter of the novel with the NMT and PBMT systems built in our previous work. In the post- editing experiment, two professional translators translate subsets of this novel (contiguous passages of 10 sentences) under three conditions: from scratch (the norm in the novel translation industry), post-editing PBMT and post-editing NMT. We record all the keystrokes as well as the time taken to translate each sentence. Based on these measurements we aim to answer two questions regarding (i) temporal and (ii) technical effort, i.e. whether post-editing results in (i) faster translations, and (ii) a smaller number of keystrokes, compared to translating from scratch. Both MT approaches significantly reduce the translation time required, the reduction achieved by NMT being considerably bigger than that by PBMT. In addition, NMT results in a significant reduction in keystrokes, while the reduction brought by PBMT is not statistically significant. We also found out that the distribution of types of keystrokes is very different in post-editing compared to translation from scratch. While the first results in considerably fewer content keywords, it increases notably the amount of navigation and erase keystrokes.", "title": "Post-editing a Novel with Neural Machine Translation", "authors": "Antonio Toral and Andy Way", "id": 55}], [{"abstract": "This study compares and explores ways to integrate machine learning and rule-based microtext classification techniques. The performance of each method is analyzed and reported, both when used standalone and when used in combination with the other method. The goal of this analysis is to effectively combine the two approaches. We aim to find what is similar and what is distinct between these two paradigms in order to come up with a hybrid methodology that will yield a higher performance, preferably both in precision and recall. We use gold standard data (tweets) released under the First Workshop on the Exploitation of Social Media for Emergency Relief and Preparedness (SMERP 2017) and the FIRE 2016 Microblog track: Information Extraction from Microblogs for training and testing our approaches. These data sets have a naturally unbalanced class distribution and tweets may be annotated with multiple labels. The first level of comparison was performed on the predicted topics by each approach. The results showed that each approach performs well on a distinct part of the test set. This observation was confirmed by having a decrease in precision when we use intersection of the predictions as the final prediction from each approach. The union of the predictions yielded an average precision and recall and 2% higher F1-score. We explored combining the two approaches at an earlier stage than the result level as well. We used the predictions of the rule-based system as training data for the machine learning system. Moreover, we excluded the tweets that do not have the same predicted value by the rule based system and the machine learning classifier trained on the SMERP data", "title": "Comparing and Integrating Machine Learning and Rule Based Microtext Classification", "authors": "Ali H\u00fcrriyeto\u011flu, Nelleke Oostdijk and Antal van Den Bosch", "id": 73}, {"abstract": "It is well-known that written text carries a good deal of non-verbal infor- mation related to the author\u2019s identity and social factors, such as age, gender and personality. However, it is less clear to what extent behavioral biometric measures such as typist data transmit such information. In this talk I will investigate typist behavior to study the predictiveness of authorship and author profiles. Experiments on two datasets of increasing size are used, including a dataset to predict age and gender from keystroke dynamics. Preliminary results show that keystroke-based features lead to significantly higher accuracies for authorship than text-based measures. For user attribute prediction, the best approach is to combine the two, suggesting that extra-linguistic factors are disclosed to a larger degree in written text while author identity is better transmitted in typing behavior.", "title": "What can we learn from behavioral biometric data?", "authors": "Barbara Plank", "id": 37}, {"abstract": "Skilled\u200b word\u200b reading\u200b can\u200b be\u200b seen\u200b as\u200b knowing\u200b how\u200b to\u200b map\u200b orthographic\u200b word\u200b forms\u200b to\u200b their phonological\u200b and\u200b semantic\u200b counterparts.\u200b Crucially,\u200b this\u200b mapping\u200b is\u200b highly\u200b ambiguous,\u200b and causes\u200b inhibition\u200b in\u200b tasks\u200b where\u200b the\u200b ambiguity\u200b plays\u200b a role\u200b of\u200b import\u200b (Kawamoto\u200b & Zemblidge, 1992). Because\u200b of\u200b these\u200b inhibitory\u200b effects,\u200b computational\u200b models\u200b of\u200b word\u200b reading\u200b have\u200b assumed\u200b a sharp\u200b division\u200b between\u200b a word\u2019s\u200b Orthographic,\u200b Semantic\u200b and\u200b Phonological\u200b representations, making\u200b the\u200b assumption\u200b that\u200b the\u200b inhibitory\u200b effects\u200b are\u200b the\u200b result\u200b of\u200b an\u200b imperfect\u200b mapping between\u200b these\u200b representations\u200b (Dijkstra\u200b & Van\u200b Heuven,\u200b 2002;\u200b Harm\u200b & Seidenberg,\u200b 2004).\u200b This approach\u200b has\u200b several\u200b downsides:\u200b First,\u200b the\u200b links\u200b between\u200b the\u200b different\u200b levels\u200b of\u200b representation have\u200b to\u200b be\u200b pre-defined.\u200b Second,\u200b by\u200b assuming\u200b three\u200b distinct\u200b levels\u200b of\u200b representation,\u200b it\u200b is\u200b unclear which\u200b of\u200b these\u200b three\u200b distinct\u200b levels\u200b is\u200b actually\u200b used\u200b in\u200b reading.\u200b Third,\u200b research,\u200b especially\u200b in\u200b field of\u200b bilingualism,\u200b has\u200b shown\u200b that\u200b differences\u200b in\u200b task\u200b demands\u200b and\u200b list\u200b composition\u200b lead\u200b to different\u200b responses,\u200b which\u200b is\u200b something\u200b these\u200b models\u200b have\u200b trouble\u200b accounting\u200b for. We\u200b propose\u200b an\u200b alternative\u200b model,\u200b which\u200b we\u200b call\u200b Global\u200b Space,\u200b that\u200b doesn\u2019t\u200b assume\u200b the existence\u200b of\u200b three\u200b separate\u200b modes\u200b of\u200b representation,\u200b but\u200b instead\u200b represents\u200b words\u200b in\u200b a single vector\u200b space,\u200b which\u200b consists\u200b of\u200b the\u200b concatenation\u200b of\u200b all\u200b three\u200b forms\u200b of\u200b representation.\u200b Words are\u200b thus\u200b represented\u200b by\u200b the\u200b concatenation\u200b of\u200b an\u200b orthographic,\u200b phonological\u200b and\u200b semantic representation,\u200b and\u200b word\u200b reading\u200b is\u200b then\u200b reduced\u200b to\u200b the\u200b reconstruction\u200b of\u200b the\u200b phonological\u200b and semantic\u200b parts\u200b of\u200b the\u200b vector\u200b given\u200b the\u200b orthographic\u200b part\u200b of\u200b the\u200b vector. This\u200b alleviates\u200b the\u200b problems\u200b above:\u200b the\u200b model\u200b no\u200b longer\u200b makes\u200b explicit\u200b theoretical\u200b assumptions about\u200b how\u200b the\u200b different\u200b representations\u200b are\u200b linked\u200b to\u200b each\u200b other,\u200b while\u200b also\u200b gaining\u200b a single representation\u200b for\u200b each\u200b word.\u200b In\u200b addition,\u200b several\u200b observed\u200b effects\u200b in\u200b inhibition\u200b or\u200b facilitation through\u200b differing\u200b task\u200b demands\u200b can\u200b be\u200b explained\u200b by\u200b a repartitioning\u200b of\u200b the\u200b space,\u200b e.g.,\u200b if\u200b a task requires\u200b a phonological\u200b response,\u200b the\u200b model\u200b can\u200b calculate\u200b the\u200b activation\u200b over\u200b the\u200b orthographic and\u200b phonological\u200b parts\u200b of\u200b the\u200b space,\u200b while\u200b leaving\u200b out\u200b the\u200b semantic\u200b part\u200b of\u200b the\u200b space,\u200b thus yielding\u200b different\u200b responses\u200b in\u200b different\u200b tasks. As\u200b a theoretical\u200b model,\u200b Global\u200b Space\u200b requires\u200b that\u200b incomplete\u200b vectors,\u200b e.g.,\u200b the\u200b orthographic vector\u200b by\u200b itself,\u200b can\u200b reliably\u200b be\u200b translated\u200b into\u200b stable\u200b internal\u200b representations.\u200b In\u200b order\u200b to\u200b count as\u200b stable,\u200b these\u200b internal\u200b representations\u200b should\u200b be\u200b translatable\u200b back\u200b to\u200b their\u200b full\u200b explicit representations.\u200b As\u200b there\u200b are\u200b several\u200b models\u200b that\u200b meet\u200b these\u200b criteria,\u200b e.g.,\u200b Autoencoders, modified\u200b SOM,\u200b Variational\u200b Autoencoders,\u200b and\u200b Hopfield\u200b Networks,\u200b empirical\u200b research\u200b needs\u200b to be\u200b carried\u200b out\u200b to\u200b verify\u200b which\u200b of\u200b these\u200b models\u200b has\u200b the\u200b best\u200b fit.For\u200b this\u200b presentation,\u200b we\u200b implement\u200b a version\u200b of\u200b the\u200b model\u200b using\u200b a Modified\u200b Self-Organizing Map\u200b (SOM)\u200b (Kohonen,\u200b 1998),\u200b outfitted\u200b with\u200b a softmax\u200b activation\u200b function.\u200b The\u200b softmax\u200b causes the\u200b map\u200b response\u200b to\u200b be\u200b expressed\u200b as\u200b a distribution\u200b over\u200b a quantized\u200b version\u200b of\u200b the\u200b training\u200b set, which\u200b allows\u200b words\u200b to\u200b be\u200b expressed\u200b as\u200b distributed\u200b representations\u200b over\u200b the\u200b SOM.", "title": "Global Space: A Reconstructive Model of Word Reading", "authors": "St\u00e9phan Tulkens, Dominiek Sandra and Walter Daelemans", "id": 35}, {"abstract": "n speech translation the first step often consists of automatic speech recognition (ASR), which outputs an unsegmented stream of words. Translating this stream of words, using off-the-shelf machine translation (MT) results in a lower translation quality compared to translating punctuated input. We compare different strategies and techniques to deal with this problem. We compare different punctuation strategies: Preprocessing inserts punctuation in the source text and allows to use an MT system trained on a normal (i.e. punctuated) parallel corpus. Implicit insertion inserts punctuating during the translation, and requires a dedicated MT system. Postprocessing inserts punctuation in the output of the MT system, which is trained on unpunctuated data. We predict punctuation using language modeling techniques, such as n-grams and long short-term memories (LSTM), sequence labeling LSTMs (unidirectional and bidirectional), and monolingual phrase-based, hierarchical and neural MT, and intrinsically evaluate punctuation prediction accuracy. For actual translation phrase-based, hierarchical and neural MT are investigated. We set up an experiment in which we combine all these strategies, punctuation prediction methods and machine translation methods, resulting in 145 experimental conditions. We observe that for punctuation prediction, phrase-based statistical MT and neural MT reach similar results, and are best used as a preprocessing step which is followed by neural MT to perform the actual translation. Implicit punctuation insertion by a dedicated neural MT system, trained on unpunctuated source and punctuated target, yields similar results.", "title": "145 ways to insert punctuation for speech translation", "authors": "Vincent Vandeghinste, Lyan Verwimp, Joris Pelemans and Patrick Wambacq", "id": 6}], [{"abstract": "Along with its tremendous benefits, the advent of technology has also brought about so many serious problems. Recent years have seen cyberbullying having taken its tragic toll on internet users. Cyberbullying is often described as \u201cwillful and repeated harm inflicted through the use of computers, cell phones, and other electronic devices\u201d. Cyberbullying incidents have showed a steady increasing trend. This trend was proven by 2 surveys in 2013 and 2017 with more than 10,000 participants in the latter. The survey showed that 7 in 10 youths are victims of cyberbullying, 37% of which experience it on a highly frequent basis. Six extremely catastrophic incidents of cyberbullying presses a huge demand for tackling this problem. Little research have been carried out using machine learning to detect bullying messages in many different social network domains such as Formstring.com, Youtube.com and Twitter.com. One of the challenges these works have in common is the moderate amount of publicly available datasets due to privacy concerns, and those available are surprisingly small in positive samples (bullying messages). Therefore, the results obtained did not seem to be satisfied, and the classification model from one domain might not be applicable to others. Another difficulty is that each dataset has its own ground truth as it was annotated differently. For example, some datasets were annotated with only label \u201cbullying\u201d and \u201cnon-bullying\u201d message, others also annotated messages from the bullies, the victims and the supporters (someone who tries to help the victims). Thus, it makes it more troublesome to perform cross domain machine learning in this task. It is reasonable to assume that internet users tend to use their same ways of texting across social media domains. Therefore, a model should be able to classify text messages regardless of the social networking sites where they were posted. Given such challenges, this work will propose a technique to apply transfer learning from one domain to another within the context of classifying cyberbullying. Using training data across different domains will help to address the problem of having too few positive data samples in separate available datasets, thereby generally improving classification accuracy.", "title": "Transfer Learning on Cyberbullying Classification", "authors": "Long Tran", "id": 80}, {"abstract": "This presentation introduces the task of microportrait extraction and applies it to identify whether Muslims are stereotyped, and if so, in what way, by the Dutch media. The core idea behind this research is that world views and perspectives are not only expressed by specific opinions, but also in more implicit ways: by what information we decide to share about an individual, event or topic and how we express this information. For instance, Vermeulen (p.c.) shows in a limited corpus study that citizens of Moroccan origin are quickly called \u2018a thief\u2019 (generic property of the person), whereas white Dutch per- petrators \u2018have stolen something\u2019 (incidental behavior). Microportrait extraction provides the technology to investigate whether such differences occur systematically. A microportrait provides all the information that a single document provides on a specific entity, concept or event. For entities, this includes the labels used to refer to them, the properties assigned to them (through, e.g. copula structures and adjectival modifiers) and the roles they play in activities (semantic roles related to events). By extracting microportraits from large amounts of data, we can investigate which labels, properties and activities tend to go occur, i.e. establish whether articles indeed talk about Moroccan thieves and others \u2018having stolen something\u2019. We created a pipeline consisting of Alpino (Bouma et al., 2001) for parsing Dutch text a new implementation of a coreference resolution system for Dutch, based on Stanford Multi-Pass Sieve for Coreference Resolution (Raghunathan et al., 2010), and a set of manually developed rules to link labels to properties and specific roles in activities. We extracted microportraits from 15,573 articles from various Dutch media. In the next step, we investigated which descriptions occurred more often than expected when an individual was labeled explicitly as a Muslim or as being Dutch (Ruigrok et al., 2017). The results show that the label Dutch is mainly used to refer to people playing (and particularly winning) in sports. When talking about Muslims, papers specifically refer to whether they are moderate or fundamental. We also observe several correlations with people labeled as Muslim and violence. In this presentation, we outline our methodology for microportrait extraction and present the results of our study on stereotyping in the Dutch media. A detailed evaluation of the accuracy of our microportrait extraction system is currently ongoing. We validated the method by verifying 1,058 descriptions from randomly selected articles. Manual inspection revealed that 98.1% of the descriptions was indeed found in the text and 87.2% of the descriptions was ordered in the correct group. These results seems suspiciously high, but this is mainly due to the the far majority of descriptions being expressed by a basic substructure in a sentence that the Alpino parser analyzes correctly despite possibly making errors in the rest of the sentence. We present the latest status of our evaluation corpus together with the results of the microportrait extraction algorithm.", "title": "Microportrait detection for identifying stereotypes in Dutch News", "authors": "Antske Fokkens, Nel Ruigrok, Sarah Gagenstein, Wouter Van Atteveldt and Camiel Beukeboom", "id": 68}, {"abstract": "Learning from past incidents has a great importance for disaster managers. Estimation of the outcomes beforehand can improve preparations for the next incidents. To make this a less labour-intensive task, we aim to automate extracting information from past events. We focus on extracting critical information about flooding events from newspaper articles as our use case. We treat this information extraction task as a sequential labelling task and train a supervised machine learning algorithm, namely Conditional Random Fields [1], to achieve our goal. However, supervised learning requires manually annotated training data, which is very expensive and time-consuming to obtain. To reduce the need for manual annotation, Active Learning [2], a human-in-the-loop method, is explored. We obtain improvement on f1-score up to 25% and observe that Active Learning drastically reduces the effort required by annotation.", "title": "Sequential Labelling with Active Learning to Extract Information about Disasters", "authors": "M. Erkan Basar", "id": 77}, {"abstract": "Elastic-substitution decoding (ESD), first introduced by Chiang (2010), can be important for obtaining good results when applying labels to enrich hierarchical statistical machine translation (SMT). However, an efficient implementation is essential for scalable application. We describe how to achieve this, contributing essential details that were missing in the original exposition. We compare ESD to strict matching and show its superiority for both reordering and syntactic labels. To overcome the sub-optimal performance due to the late evaluation of features marking label substitution types, we increase the diversity of the rules explored during cube pruning initialization with respect to labels their labels. This approach gives significant improvements over basic ESD and performs favorably compared to extending the search by increasing the cube pruning pop-limit. Finally, we look at combining multiple labels. The combination of reordering labels and target-side boundary-tags yields a significant improvement in terms of the word-order sensitive metrics Kendall reordering score and METEOR. This confirms our intuition that the combination of reordering labels and syntactic labels can yield improvements over either label by itself, despite increased sparsity.", "title": "Elastic-substitution decoding for Hierarchical SMT: efficiency, richer search and double labels", "authors": "Gideon Maillette de Buy Wenniger, Khalil Sima'An and Andy Way", "id": 85}], [{"abstract": "Tweet collections comprise a relatively new document type. The form, motivation behind their generation, and intended function of the tweets are more diverse than traditional document types such as essays or news articles. Understanding this diversity is essential for extracting relevant information from tweets. However, the Twitter platform does not provide any kind of infrastructure other than hashtags, which is limited by the number of users who know about it, to organize tweets. Thus, collecting and analyzing tweets introduces various challenges (Imran et al., 2015). Using one or more key terms and/or a geographical area to collect tweets is prone to cause the final collection to be incomplete or unbalanced (Olteanu et al., 2014), which in turn decreases our ability to leverage the available informa tion. In order to alleviate some of the problems that users experience, we developed Relevancer which aims to support experts in analyzing tweet sets collected via imprecise queries in the context of high-impact events. 4 Experts can define their information need with up-to-date information in terms of automatically detected information threads (H\u00fcrriyeto\u011flu et al., 2016a), and use these annotations to organize unlabeled tweets. The main observations and contributions that are integrated in Relevancer to help experts to come to grips with event-related event collections are: (i) almost every event-related tweet collection comprises tweets about similar but irrelevant events (H\u00fcrriyeto\u011flu et al., 2016a); (ii) by taking into account the temporal distribution of the tweets about an event it is possible to achieve an increase in the quality of the information thread detection and a decrease in computation time (H\u00fcrriyeto\u011flu et al., 2016b); and (iii) the use of inflection-aware key terms can decrease the degree of ambiguity (H\u00fcrriyeto\u011flu et al., 2016c). At the end of the analysis process, the experts understands the data and is able to use his understanding in an operational setting to classify new tweets. The worst case performance of the classifier is significantly better than a majority class based baseline. The tool is supported by a web interface that can potentially be used to monitor and improve the performance in a particular use-case in real time.", "title": "Relevant Information Detection on Twitter", "authors": "Ali H\u00fcrriyeto\u011flu, Nelleke Oostdijk, Antal van Den Bosch and Mustafa Erkan Basar", "id": 3}, {"abstract": "Recent initiatives such as the TextLink network 1 have created an impetus for the creation of new resources (and unification of previously existing resources) related to discourse structure of text. Discourse-annotated corpora, as well as lexicons of discourse connectives, are becoming available for an increasing number of languages. However, computational discourse analysis is still quite limited by the number of languages for which discourse parsers or large enough resources exist. We aim to contribute to the language diversity in discourse analysis by proposing a novel way to create discourse lexicons for multiple languages in which the items are categorized by the discourse relations they convey according to the Penn Discourse Treebank (PDTB) tags (The PDTB Research Group, 2008). Our approach is indebted to Lopes et al.\u2019s (2015) phrase tables of discourse connectives for all the languages in the Europarl parallel corpus (Koehn, 2005). The merit of our work is in converting these phrase tables to weighted lexicons by extrapolating the discourse relation category weights of the English connectives in the PDTB 2.0 corpus (Prasad et al., 2008) to these other languages, weighted by the correlation strength between English and the target language of the connectives in the phrase table. One of our interests is how the discourse structure of a text, based on such lexicons, can be used as features for gender profiling experiments. We use our weighted lexicons to create features and also evaluate our approach as such, extrinsically, by comparing its performance with that of a knowledge-based discourse lexicon (DiMLex) for German (Stede, 2002). We perform gender profiling experiments on five datasets, varying in three languages (English, Dutch, German) and two genres (news and blogs). All datasets were preprocessed to be balanced for gender. Standard features, such as character and word n-grams, and discourse features were both experimented with. Four variants of discourse features were tested. Three of these take discourse categories into account on three different levels of specificity, e.g. comparison.contrast.opposition illustrates the three levels. Our assumption is that abstracting over the specific connectives, to discourse categories of the text, might result in a better representation of the discourse of the text. The fourth variant is a mere count-based approach of all the connectives in the discourse lexicon. All experiments are conducted using tenfold cross-validation with three different learning algorithms (SGDClassifier, LogisticRegression, RandomForestClassifier) from sklearn (Pedregosa et al., 2011). We find that our discourse features perform quite well (0.57-0.64 F-score) when used for gender profiling on the three blog corpora, but hardly on the news corpora (0.50-0.54 F-score). The overall performance of the news corpora when using n-grams is also decidedly lower (0.62-0.68 F-score) than for blogs (0.88-0.92 F-score). Our results on the German dataset show that the discourse features from our own generated lexicon significantly outperform the discourse features from the knowledge-based DiMLex lexicon. This validates the reasoning of our lexicon generation approach thus making it a viable approach for all other languages for which no (or few) resources exist.", "title": "Discourse lexicons for multiple languages and their use for gender profiling", "authors": "Ben Verhoeven and Walter Daelemans", "id": 34}, {"abstract": "The most common method for evaluating automatic translations is to compare the automatic translation to a manually verified reference translation and to compute a BLEU score that expresses the overlap between the two versions (Papineni et al., 2002). As it is time and effort consuming to construct these manual translations, we aimed to develop a method that can evaluate the automatic translation without any human effort. Such an implicit evaluation technique only focuses on a verification of correct translation of the main concepts and entities in the translation. These experiments are conducted within the TraMOOC project (http://tramooc.eu/) that aims to automatically translate online course material of MOOCs to 11 different languages (Kordoni et al., 2016). In this study we compare two methods for the automatic detection of topics and entities: entity linking as implemented in the Illinois Wikifier (Cheng and Roth, 2013) and BabelFy (Moro et al., 2014) a tool that performs both entity linking and word sense disambiguation. We explain how both entity linking methods work and how we integrated them into a tool for implicit translation evaluation. This tool computes, for a given English source text and its translation in the target language, an Entity translation recall score that expresses the number of correctly translated entities in comparison to the overall number of detected entities. We experimented on samples of educational material in three different languages: Dutch, Portuguese and Russian.", "title": "To Wikify of to Babelfy for Implicit Translation Evaluation?", "authors": "Iris Hendrickx, Maarten van Gompel and Antal van Den Bosch", "id": 44}]], "title": "Parallel session no. 3", "sessions": [{"chair": "", "title": "Social media and winner STIL Thesis Prize", "location": "Kleine zaal"}, {"chair": "", "title": "Authorship and Discourse", "location": "Leeuwzaal"}, {"chair": "Orph\u00e9e De Clercq", "title": "Named Entity Recognition and Term Extraction", "location": "Soci\u00ebteitskamer"}, {"chair": "Sien Moens", "title": "Translation", "location": "Annazaal"}], "start": "15:55"}, {"title": "Drinks", "location": "Foyer begane grond", "start": "17:15", "end": "18:15"}]