[{"location": "Centrale Hal", "start": "8:30", "title": "Registration and Coffee", "end": "9:30"}, {"description": "", "start": "10:10", "title": "", "sessions": [{"location": "Kleine zaal  ", "title": "Emotion and Sentiment"}, {"location": "Leeuwzaal", "title": "Language generation"}, {"location": "Soci\u00ebteitskamer", "title": "Semantics"}, {"location": "Keizer Karel Foyer", "title": "Information Retrieval"}, {"location": "Annazaal", "title": "Shared Task and Discussion on Open data"}], "subslots": [[[{"authors": "Mario Giulianelli and Dani\u00ebl de Kok", "abstract": "The task of emotion classification has traditionally been addressed using two different ap- proaches: (1) in lexicon-based approaches, a classifier relies on a lexicon that stores, for each word, the emotions the word evokes; (2) in corpus-based approaches, a classifier relies on features extracted from a training set that consists of pairs of sentences and emotion distributions. Both of the approaches have different strengths\u2014lexicon-based approaches typically have a wider coverage of emotion-bearing words, whereas corpus-based approaches learn to use contextual clues. It should not come as a surprise that these approaches have been used jointly, to exploit the strengths of both (Strapparava and Mihalcea, 2008). However, a combination of the two approaches to emotion classification still suffers from the relatively limited size of the available vocabularies. In this work, we introduce a novel variant of the Label Propagation algorithm (Zhu and Ghahramani, 2002) to extend the coverage of an existing emotion lexicon. In order to do so, we construct a fully connected graph wherein words are vertices and the edges are weighted by distributional word representations. The vertices that correspond to words that occur in an emotion lexicon are initialized using the emotion distribution indicated in the lexicon. The label propagation algorithm is then used to derive emotion distributions for words that do not occur in the lexicon. In our experiments, we compare four emotion classifiers: (1) the model of Mohammad and Kiritchenko (2015); (2) a bidirectional LSTM model; (3) a bidirectional LSTM model using an emotion lexicon; (4) a bidirectional LSTM model using the extended emotion lexicon derived through label propagation. Our results show that the model that uses the expanded emotion lexicon consistently outperforms the other models (e.g. it provides an absolute improvement of 6.3 in micro-averaged F1 score compared to Mohammad and Kiritchenko (2015) on the Hashtag Emotion Corpus (Mohammad and Kiritchenko, 2015). Finally, we introduce another variant of label propagation, batched label propagation, that makes label propagation tractable for large vocabularies.", "title": "Semi-supervised emotion lexicon expansion with label propagation"}], [{"authors": "Cynthia Van Hee, Els Lefever and V\u00e9ronique Hoste", "abstract": "While common sense and connotative knowledge come natural to most people, computers still struggle to perform well on tasks for which such extra-textual information is required. Automatic approaches to sentiment analysis and irony detection have revealed that the lack of such world knowledge undermines classification performance. In this paper, we therefore address the challenge of modelling implicit or prototypical sentiment in the framework of automatic irony detection. Starting from manually annotated connoted situation phrases (e.g. flight delays, sitting the whole day at the doctor's office), we defined the implicit sentiment held towards such situations automatically by using both a lexico-semantic knowledge base and a data-driven method. We further investigate how such implicit sentiment information affects irony detection by assessing a state-of-the-art irony classifier before and after it is informed with implicit sentiment information.", "title": "Using common sense to detect irony on Twitter"}], [{"authors": "Orph\u00e9e De Clercq and Veronique Hoste", "abstract": "When producing text, people often use shorter or alternative linguistic structures to refer to previously mentioned elements. Though mostly short text is being produced online, people use the same technique when venting opinions on various aspects of a certain product or experience. Whenever discourse entities are mentioned more than once and appear in different discourse contexts they are said to corefer. While it has been claimed that anaphora or coreference resolution plays an important role in opinion mining, it is not clear to what extent coreference resolution actually boosts performance, if at all. In this presentation, we investigate the potential added value of coreference resolution for the aspect-based sentiment analysis of restaurant reviews written in Dutch. We focus on the task of aspect category classification and investigate whether including coreference information prior to classification to resolve implicit aspect mentions is beneficial. Because coreference resolution is not a solved task in NLP, we rely on both automatically-derived and gold-standard coreference relations, allowing us to investigate the true upper bound. By training a classifier on a combination of lexical and semantic features, we show that resolving coreferential relations prior to classification is beneficial in a joint optimization setup on the training data. When validating these optimal models, however, the system is able to achieve a satisfying performance on a held-out test set, regardless of whether coreference information was included or not.", "title": "OMG het was zooo lekker! Can fine-grained sentiment analysis benefit from coreference resolution?"}], [{"authors": "Florian Kunneman, Liesbeth Mollema, Antal van den Bosch, Mattijs Lambooij, Albert Wong and Hester de Melker", "abstract": "We present a case study on the categorization of Twitter messages that refer to vaccination, a much-discussed topic on social media. In order to assist the Dutch Rijksinstituut voor Volksgezondheid en Milieu (RIVM) in monitoring this discussion, we aimed to build a system that can detect the stance towards vaccination in these messages. Especially a rapid spread of rumours about the negative effects of vaccination can pose a threat to this institute, which is responsible for the Dutch vaccination program and thereby has interest in a wide vaccination coverage. A system that can detect a sudden increase of messages with a negative stance can alarm the institute and enable them to take timely action if the information that is shared seems to be based on falsive information. Identifying the stance towards vaccination in messages is comparable to identifying the polarity of a message, in the sense that messages are flagged as positive, negative or neutral. However, a tweet that conveys a negative stance towards vaccination might be positive in sentiment and vice versa. Hence, off-the-shelf tools for sentiment analysis do not seem applicable to this problem. Alternatively, we collect Twitter messages that mention a vaccination-related key term and have them annotated by means of three categorizations. First, the annotator is asked whether a message is relevant or irrelevant. This is an important distinction, as the word 'vaccin' might be used as a metafor in a different context or refer to animal vaccination, making the tweet irrelevant. Second, the annotator is asked whether the tweet conveys a positive, negative or neutral stance towards vaccination, or whether the stance is unclear to the reader. Third, the tweet is categorized into one of the fine-grained sentiments 'frustration', 'concern', 'fear', 'information' and 'other'. This latter categorization might assist the detection of tweets with a negative stance. The resulting categorizations are used to train and test a machine learning classifier on detecting tweets with a negative stance. We experimented with a strict and lax usage of the annotated tweets, as well as different categorizations and classifiers. We will present an overview of the different approaches and compare their predictions to those of an off-the-shelf sentiment analysis tool. Based on these outcomes, we will discuss the challenges of the task and avenues for future work.", "title": "Monitoring stance towards vaccination in Twitter messages"}]], [[{"authors": "Bram Bulte, Leen Sevens and Vincent Vandeghinste", "abstract": "We discuss the design, development and evaluation of an automated lexical simplification tool for Dutch. Such text simplification tools are useful for a wide range of target populations (e.g. people with an intellectual disability, second language learners, aphasics and children). The ultimate aim of these tools is to adapt texts in such a way that they are easier to read and understand, while maintaining the original meaning as much as possible. To our knowledge, automated text simplification in Dutch has not been researched yet. We use a basic pipeline approach to tackle the problem of lexical simplification. Sentences are first pre-processed (i.e. tokenized, POS tagged and lemmatized) using TreeTagger (Schmid, 1994) and word sense disambiguation is performed using a tool based on support vector machines and trained on the data of the DutchSemCor project (Vossen et al., 2012). This tool links the identified word senses to lexical items in the Cornetto database (Vossen et al., 2013). The difficulty of each token in the input sentence is then estimated using two resources: (a) aggregated data coming from psycholinguistic studies into the average age of acquisition of Dutch words (Brysbaert et al., 2014), and (b) frequency information of Dutch tokens calculated on the basis of a large-scale corpus (of over 1000 million tokens) combining different sources, such as Subtitles2016 (Lison & Tiedemann, 2016), EUBookshop (Skadi\u0146\u0161 et al., 2014), DGT, Europarl and Wikipedia (Tiedemann, 2012), CGN Flemish (Oostdijk et al., 2002) and SONAR500 (Oostdijk et al., 2013). Cornetto is used to identify synonyms of words that have been identified as being difficult. Potential replacements are retrieved, ranked and selected, and a parsed version of the SONAR500 corpus is used to perform reverse lemmatization. The correct inflectional form of the replacement word is selected by matching the Treetagger-tag of the lemma with the SoNaR-tag, and retrieving the corresponding form. A neural language model is used to verify whether the selected replacement word fits the local context. A small development and test set, each consisting of 150 sentences taken from the Flemish newspaper De Standaard, are used to tune the system\u2019s parametersand evaluate its output. In this study, a basic form of human evaluation is performed. Words in the test set are divided into two categories: (a) potentially difficult words, and (b) all other words. It is then verified whether the lexical simplification system changed the appropriate words and whether the proposed changes constitute an improvement in terms of decreasing the degree of difficulty, while maintaining the meaning and respecting grammar rules. We present quantitative results for different versions of the system that focus either on maximizing precision or recall, or on balancing both, and we show how the lexical simplification tool can be integrated in a full-fledged text simplification system for Dutch (Sevens et al., 2017). We expect that the presented research will lead to further developments in the field of augmentative and alternative communication.", "title": "Automating lexical simplification in Dutch"}], [{"authors": "Leen Sevens, Vincent Vandeghinste, Lyan Verwimp, Ineke Schuurman, Patrick Wambacq and Frank Van Eynde", "abstract": "In today\u2019s digital age, people with limited reading and writing skills have trouble partaking in online activities. Not being able to access or use information technology is a major form of social exclusion. We present a Pictograph-to-Text translation system for people with an intellectual disability. It provides help in constructing Dutch textual messages, by allowing the user to input a series of pictographs, and translates these messages into natural language text. The main challenge in translating from pictograph languages to natural language text is the fact that a pictograph-for-word correspondence will almost never provide an acceptable output. Pictographs are underspecified, both semantically and grammatically. In the second place, the pictograph input to translation could be ambiguous and unpredictable with respect to pictograph order. Our baseline system for Pictograph-to-Text translation (Sevens et al. 2015) generates natural language from pictographs using language models and does not use any grammatical information in the translation process. When a pictograph is selected, its connected WordNet synset is retrieved, and from this synset, the system retrieves all the synonyms it contains. For each of these synonyms, reverse lemmatisation is applied. The reverse lemmatiser retrieves the full inflectional paradigm of each lemma. Each of these surface forms is a hypothesis for the language model. We propose two types of language models. In our n-gram-based approach, the system performs beam search decoding on an n-gram language model (n\u22645), trained with the CMU toolkit (Clarkson & Rosenfeld 1997) on a Dutch corpus of over 1100M tokens. In our Long Short-Term Memory-based approach, we train a language model with Tensorflow (Abadi et al. 2016) on the Flemish part of the CGN corpus (3.8M tokens) (Oostdijk et al. 2002) and re-rank the natural language hypotheses. The evaluations of the baseline system show that using language models for finding the most likely combination of textual representations is already an improvement over the initial baseline (i.e., pictograph file names), but there is ample room for improvement. In recent experiments, we apply machine translation techniques. Since a parallel corpus of pictograph sequences and well-formed written Dutch text is not available, we explore different approaches toward the creation of a suitable parallel corpus. In our first approach, we automatically translate a large corpus of monolingual Dutch SoNaR subtitles (27.6M tokens) (Oostdijk et al. 2013) into pictographs using the Text-to-Pictograph translation tool (Vandeghinste et al. 2015). In our second approach, we lemmatise the subtitle corpus, and remove all words that are not content words, thus creating a source language corpus that resembles pictograph input. Our phrase-based statistical machine translation approach toward Pictograph-to-Text translation uses the Moses decoder (Koehn et al. 2007), while our neural machine translation approach makes use of the open-source system OpenNMT (Klein et al. 2017). We build different models using a variety of training conditions, including factored models that include part-of-speech and lemma information, and evaluate all systems using automated metrics and human evaluations (adequacy, fluency, and ranking). Our first experiments indicate that the machine translation approaches outperform the baseline system.", "title": "Pictograph-to-Text Translation for Augmented and Alternative Communication"}], [{"authors": "Andreas Madsack and Johanna Heininger", "abstract": "Natural Language Generation (NLG) is especially interesting for companies with structured data and the need for large amounts of texts or regularly updated texts based on this data. Another important field of application is the demand to provide multilingual content that is usually solved by translation of text. We present a rule-based NLG system with a cross-language abstractional layer that makes it feasible to write texts in multiple languages (currently in 22 languages) at the same time. This cross-language abstraction works independently from data source language or grammatical differences in the output. The system is controlled by a language agnostic markup language that is used on our SaaS platform to generate texts. This markup can be easily edited by a technical editor in the web browser without coding skills or previous knowledge in computational linguistics. In the talk we would like to present our solution for grammatical agreement exemplary for Dutch. One of our clients is able to generate 250.000 different texts about holiday homes in German and Dutch with the same configuration.", "title": "Content Generation in German and Dutch from Structured Data"}], [{"authors": "Tim Van de Cruys", "abstract": "Recurrent neural networks have established themselves as a state of the art method for language modeling. Advanced architectures, such as LSTM-style networks, are able to precisely model the sentence\u2019s history in order to capture long-range dependencies. These recurrent neural network models, however, are less suitable for topically coherent text generation: they are able to generate grammatically correct text, but are unable to properly modulate the topical content. In this research, we investigate whether a recurrent neural network architecture might be augmented with topical information from a non-negative matrix factorization model (Lee and Seung, 1999), in order to generate text that is syntactically sound as well as topically coherent. The model\u2019s results are evaluated quantitatively as well as qualitatively, and compared to recent neural network architectures that aim to incorporate topical information using different means (Cao et al., 2015; Lau et al., 2017).", "title": "Generating Topically Coherent Text with Recurrent Neural Networks"}]], [[{"authors": "Johan Bos, Lasha Abzianidze, Hessel Haagsma and Rik van Noord", "abstract": "Recently several corpora annotated with meaning representations have been developed. Most of this work has been concentrated on English. We present ongoing efforts on building a corpus of Dutch sentences annotated with formal meaning representations. Although there are several resources addressing lexical semantics for Dutch (Postma et al., 2016), as far as we know this is the first effort to produce meaning representations for Dutch that combine predicateargument structure with lexical semantics and typical semantic phenomena such as negation, quantification, modality, and tense. In this paper we outline the method that we use to build the corpus, and present the first results. The method that we employ is based on annotation projection using a parallel English\u2013Dutch corpus (Evang and Bos, 2016). In a nutshell, this works as follows. First we produce a semantic analysis for English with NLP tools tailored for this task (Evang et al., 2013; Lewis and Steedman, 2014; Bos, 2008). Next, taking advantage of automatically aligned words, we produce the syntactic and semantic analysis from English to Dutch. If needed, we manually correct the analysis. The syntactic analysis, both for English and Dutch, is based on Combinatory Categorial Grammar (CCG, Steedman 2001). In order to give a lexical meaning to a word, we use language-independent semantic tagging (Bjerva et al., 2016; Abzianidze and Bos, 2017). Given a word and its lexical CCG category and semantic tag, we produce a lexical meaning in the form of a lambda-expression. As a semantic representation we adopt Discourse Representation Theory (Kamp and Reyle, 1993) with a neo-Davidsonian analysis of events, where the inventory of VerbNet roles (Bonial et al., 2011) provide the thematic roles and WordNet synsets form the predicate symbols. We present a corpus that includes over 300 Dutch documents with gold standard annotations and semantic representations, together with an English counterpart, and in some cases, a German and/or Italian translation as well (http://pmb.let.rug.nl/explorer). Semantic representations of different translations of the same documents can be compared using our semantic matching system D-Match, which detects the degree of overlap between graph structures of meaning representations and calculates standard measures like precision, recall and F1-score. The latter scalar measure can be seen as a complement to a more coarse-grained categorical measure (e.g., equivalence, entailment, neutral, and contradiction) that is based on logical inference over meaning representations. Using the scalar or categorical semantic measure, one can automatically detect potentially bad/good or literal/non-literal translations. During the presentation, we will (i) discuss challenges of the annotation projection from English to Dutch, (ii) show how several semantic phenomena, including quantification, modals, negation and tense, are transferred into semantic representations of Dutch translations and (iii) present semantic similarity scores for several English-Dutch translations.", "title": "Towards Wide-coverage Compositional Semantics for Dutch"}], [{"authors": "Mathias Coeckelbergs", "abstract": "Within the practice of Topic Modeling, an often heard complaint is the difficulty and inherent subjectivity of evaluating the appropriateness of topics for the document collection they model. On the other hand, word embeddings have proven their worth discerning semantic coherence of a given vocabulary, giving rise to the famous examples of word analogies found without previous world knowledge. The research question we want to address in this paper is to what extent word embeddings can help to discern concepts automatically in topics, and how they can place interpretational boundaries which decreases the subjectivity of interpretation. For this, we start our exposition by giving a short overview of both topic modeling and word vectors, their assets and drawbacks, and show in which way they could be of mutual benefit to each other. Next, we present the results of filtering the keyword clusters found by Latent Dirichlet Allocation, the most widely used topic modeling algorithm, by two sets of pre-trained word embeddings, Word2Vec and GloVe respectively. The corpus serving as case study is a collection of economic documents of the European Commission, spanning from 1958 to 1982. We find that some topics correspond to concepts found by the word embeddings, but that most are collocations of two or more concepts. In this way, we show how topic modeling is a suitable method for extracting the elements of what a text is about, and that word embeddings add conceptual structure to this. We use the embeddings in two ways, one is to find the most related word vectors to a given word, to indicate in which topics closely related words can be found. The other method starts from the topics themselves and puts a hierarchical order on the terms by iteratively extracting the word which according to its vectorial representation has the least features in common with the other words. To round off, we present an overview of the benefits of combining the word of topic modeling with that of word embeddings, rather than resorting to either of both.", "title": "From Topic Modeling to Concept Identification. The Added Value of Word Embeddings to Discern Semantic Coherence in Keyword Clusters."}], [{"authors": "Anna Feldman, Jing Peng and Katsiaryna Aharodnik", "abstract": "Idioms add color to language. Without idioms language would be dull and unexciting. Idioms reflect on our culture and values. Different languages and cultures use different types of idioms. Thus, for example, in American English, one bites the bullet while in Russian, one squeezes the teeth; in American English, one puts a fly in the ointment whereas in Russian one adds a spoon of tar to the barrel of honey. Both Russians and Americans shed crocodile tears. Many Natural Language Processing applications, such as machine translation (MT), natural language understanding (NLU), sentiment and emotion analysis could improve their performance if idioms could be detected automatically with good accuracy. It turns out that a large number of expressions are ambiguous between their idiomatic and literal interpretation and their status (idiomatic vs. literal) can only be determined in context (e.g., sales hits the roof vs. hit the roof of the car). This paper addresses the problem of determining if an expression is literal or idiomatic in a specific context. Below we describe our approach. Our approach is based on two hypotheses: (1) words in a given text segment that are representatives of the local context are likely to associate strongly with a literal expression in the segment, in terms of projection of word vectors onto the vector representing the literal expression; (2) the context word distribution for a literal expression in word vector space will be different from the distribution for an idiomatic one (similarly to Firth, 1957; Katz and Giesbrecht, 2006)). To address the first hypothesis, we propose to exploit recent advances in vector space representation to capture the difference between local contexts (Mikolov et al., 2013a; Mikolov et al., 2013b). A word can be represented by a vector of fixed dimensionality q that best predicts its surrounding words in a sentence or a document (Mikolov et al., 2013a; Mikolov et al., 2013b). To address the second hypothesis, we propose to capture local context distributions in terms of scatter matrices in a space spanned by word vectors (Mikolov et al., 2013a; Mikolov et al., 2013b). Given two distributions represented by two scatter matrices S1 and S2, a number of measures can be used to compute the distance between S1 and S2, such as Choernoff and Bhattacharyya distances (Fukunaga, 1990). We carried out an empirical study evaluating the performance of the proposed techniques and compared our results with a number of previous approaches, including the supervised version of the CONTEXT technique described in Fazly et al. 2009 and the Gaussian Mixture Model as described in Li & Sporleder 2010. We use two different languages to evaluate the performance of our system \u2013 Russian and English. Our results are promising. Our methods outperform the previous approaches we re-implemented and evaluated on our datasets.", "title": "IDEA: Idiom Detection \u2013 an Easy Approach"}], [{"authors": "Lasha Abzianidze", "abstract": "Knowledge acquisition is considered as one of major bottlenecks for systems that attempt to automatically detect semantic relations between natural language sentences (Dagan et al., 2013, p. 7). One can employ lexical knowledge resources, e.g., WordNet (Miller, 1995) or FrameNet (Baker et al., 1998), but they are never enough. On the other hand, rule-based textual entailment systems are brittle since a single error in a meaning representation, a missing inference rule, or a lack of piece of lexical knowledge can lead to the failure of capturing a correct semantic relation. In this abstract, I use a rule-based, tableau theorem prover for natural language, called LangPro (Abzianidze, 2015, 2017), and the recognizing textual entailment (RTE) dataset SICK (Marelli et al., 2014), to show how lexical knowledge can be automatically induced from a training set with the help of proof trees built by the prover. For this task we choose the SICK dataset as its problems heavily depend on lexical knowledge. The tableau prover builds proof trees that try to refute a certain semantic relations. Refutation fails if there are no open branches (i.e., all constructed situations are impossible), otherwise refutation succeeds and an open branch in the tableau serves as a counterexample for the semantic relation. For example, the tableau in Figure 1a tries to refute that someone is holding a hedgehog is not entailing someone is holding a small animal ( SICK -4974): it starts to find a situation where the premise 1 is true (T) and the conclusion 2 is false (F). The branch (in blue) is not closed since the prover doesn\u2019t have the information that hedgehog is small animal (see 4 and 8 ). Hence it succeeded to refute the entailment as it found an open branch and failed to classify the problem as entailment. But if the abductive reasoning\u2014inference to the best explanation\u2014is used, based on the open branch in Figure 1a, it is straightforward to identify that the lexical knowledge, hedgehog entailing small animal, is sufficient to close the branch and hence correctly classify SICK -4974 as entailment. Whenever a problem like SICK -4974 occurs in a training set, based on its gold entailment label, LangPro can build a tableau that would have all branches closed if there were no missing lexical knowledge. Then induction of lexical knowledge can be reduced on finding a piece of information that causes all branches of the tableau to close. Let\u2019s explain the latter scenario on the non-trivial entailment problem: ( SICK -5397) a man is removing some food from a box vs the man is putting chicken into the container. Given the gold label CONTRADICTION of the problem, the prover can build the tableau that attempts to refute the contradiction relation in SICK -5397. The built tableau need to have all branches closed, i.e., refutation of the contradiction relation must fail, to comply with the gold label. For this concrete example, the automatically built tableau is open, where the nodes in the only open branch are shown in Figure 1b. Given that chicken entails food based on WordNet, c and f entities can co-refer, which gives us an opportunity to close the branch due to 4 and 8 based on the assumption that put (into) and remove (from) are semantically inconsistent. During the presentation it will be shown to what extent the induced knowledge improves the results of the prover over SICK.", "title": "Lexical Knowledge Acquisition with Theorem Proving"}]], [[{"authors": "Tjerk de Greef and Pieter van Boxtel", "abstract": "Legal intelligence is a Solr based specialised search engine for Dutch legal professionals, with 70% market share and 100,000+ users. A Google for Dutch lawyers, if you will. Over the years, we have optimised search for our specific group of users by adding domain specific features. This abstract highlights some of these features thereby shining a commercial light on academic topics. Good legal search starts classically by building an inverted index based on appropriate lemmatization and the application of thesauri while indexing and searching, using a combination of domain driven text analysis and query parsing/expanding. Using a set of data driven regular expressions that accounts for writing variations beyond official guidelines 3 , Legal Intelligence recognizes topics from the legal vocabulary, synonyms of courts, references to law articles, legal publications,\u200b \u200band\u200b \u200bmany\u200b \u200bothers.\u200b \u200bThis\u200b \u200bapproach\u200b \u200bresults\u200b \u200bin\u200b \u200ba\u200b \u200bwell\u200b \u200bbalanced\u200b \u200brecall\u200b \u200band\u200b \u200bprecision. For any commercial product, performance and agility are key. As such, the application of thesauri while indexing and searching is based by a custom query parser and analysis chain, including a token filter that takes care of advanced Finite State Transducer based thesaurus matching 4 . The talk discusses some unexpected challenges while implementing. In addition, we will also explain how we are using Solr\u2019s function queries in order to give our users the best possible ranking of search results. For agility goals, the talk will also address our approach to test driven development 5 accounting for an automated test suite for these search related features maintaining\u200b \u200bthe\u200b \u200bquality\u200b \u200bof\u200b \u200bsearch\u200b \u200bwhile\u200b \u200benabling\u200b \u200brapid\u200b \u200bdeployments\u200b \u200bat\u200b \u200bthe\u200b \u200bsame\u200b \u200btime. But search is never stable. New content comes in every day and more search techniques becomes available at each dawn. Therefore Legal Intelligence looks beyond the horizon of excellent search. Therefore this talk will also address two machine learning approaches. The first addresses meta-data enrichment of documents, specifically using supervised classification to add law areas to legal documents. Having such additional meta-data is quintessential for faceted search and alerting mechanism on law area. Using unsupervised learning, the second approach addresses a capability to filter out non legal relevant documents. Known as \u2018the Garbage Collector, this has become an important requirement since clients prefer to search through their own content. But often, the internal content is messy and also including invoices, progress reports\u200b \u200band\u200b \u200beven\u200b \u200bstaff\u200b \u200bassessment\u200b \u200breports\u200b \u200bwhich\u200b \u200ball\u200b \u200blead\u200b \u200bto\u200b \u200bnoise\u200b \u200bin\u200b \u200bsearch\u200b \u200bresults.", "title": "Lawyers that Search? Thesaurus driven legal search using custom analysis and query parsing and beyond"}], [{"authors": "Suzan Verberne", "abstract": "Popularity plays an important role in ranking search results, especially for general interest, high-frequency queries: the more a search result is clicked on by users, the higher it will be ranked in new searches. But when the information searched becomes more specific, popularity becomes less important and less useful as a ranking criterion. Less important because for highly specific queries, relevance is more searcher- specific than for general-domain search; less useful because the amount of click data available from other users is limited for specific search queries. An alternative for search tasks addressing highly specialized topics is to employ the user\u2019s own data for result ranking. Academic search is a key example of domain-specific search. Although academic search is generally defined as a recall-oriented task, precision is also an issue due to ambiguity of search queries \u2013 query terms commonly have different mean- ings across scientific domains. Consider for example the term \u2018search behaviour\u2019, which can refer to (according to Google Scholar): prey search behaviour, job search behaviour, the search behaviour of soccer goalkeepers, and of course information search behaviour. A large body of previous work exists on user profiling in Information Retrieval, but relatively little work addresses personalized academic search. Approaches to user profiling and personalization often incorporate ontological information from a reference ontology. An alternative is to collect a set of documents that are known to be relevant to the user, and generate a user profile from those documents. We propose an academic user profile that consists of topic-specific terms, stored in a graph. Storing the profile as a graph has a number of advantages. First, it is flexible with respect to other node types and relation types to be added. Second, it is a visualization of knowledge that is interpretable by the user. Third, it offers the possibility to view relational characteristics of individual nodes. In the presentation we explain how we generate the graph model from texts authored by the searcher. We show what the proposed personal graph model looks like for academic authors and how the model can be used to to re-rank the scientific publications retrieved by a search engine. We found that re-ranking with the personal graph gives a small but significant improvement over the baseline ranking. In future research, we will address the extension of the author\u2013topic graph with other types of relational information, such as author and conference/journal nodes, citation relations between documents, and behavioral data such as queries and clicks on documents.", "title": "User profiling with personal graphs"}], [{"authors": "Hugo de Vos", "abstract": "Automatic thesaurus generation is a desired technique for the reason that a thesaurus is a useful tool in NLP, but manually making a thesaurus is expensive and time-consuming. In this project, the process of thesaurus generation was divided up in two parts: term extraction and relation extraction. Term extraction being the process of automatically finding candidate terms for a legal thesaurus and relation extraction is the process of finding which terms are hypernyms of each other. For term extraction different termhood measures are used: Log Likelihood, Kullback Leibler Divergence and the measure as assigned by the TExSIS tool. For relation extraction, different classifiers are trained to classify whether two terms have a hypernym-relation based on word embeddings. In this presentation, I will briefly present my methods and results. I will also discuss the challenges for NLP in the legal domain compared to challenges in other domains for example the biomedical domain. During this research, I experienced first hand that methods from other domains can not be adopted to the legal domain, for the main reason that Law is a domain that differs largely per country. On the contrary, the legal domain provides with great opportunities, for example because large quantities of text are freely available.", "title": "Methods For Automatically Generating a Legal Thesaurus and discussing NLP in the Legal Domain"}], [{"authors": "Subhradeep Kayal, Sameer Chivukula and Sophia Katrenko", "abstract": "Automatic extraction of definitional sentences from text content is often useful in many other tasks such as ontology generation, question answering, and building glossaries and dictionaries. Definition classification can be modelled as a supervised sequence classification problem, and sequence-learning neural networks (namely Long Short-term Memory Networks or LSTMs) have been used successfully to perform this task. In this paper, we propose a simple multi-view extension to the network architecture, to combine the learnings from raw text as well as its corresponding part-of-speech sequence, for both LSTMs as well as Convolutional Neural Networks (CNNs), showing that such a simple multi-view fusion mechanism consistently learns better than a single-view network, as well as furthers the current state-of-the-art F1-score by 5.8%, by obtaining the highest F1-score of 97%.", "title": "Classifying Definitional Sentences using Multi-view Neural Network Architectures"}]], [[{"authors": "Ineke Schuurman, Leen Sevens, Vincent Vandeghinste", "abstract": "May 2018, the new personal data protection \u2013privacy! \u2013 rules (GDPR) will become valid in the EU and all its member states. On the other hand, making your research data available as Open Data is becoming more and more important, for example when applying for a grant (NWO, FWO, EU, ...). Not making them \u2019Open\u2019 is to be justified very well.      What does this mean for linguistic research? Especially when dealing with children, elderly people, people with an intellectual disability, migrants, ... . Or with social media? And what when someone decides that his or her contribution to a corpus can no longer be used? Thus: How are GDPR and Open Data to be reconciled? How are you dealing with such issues? Any tips, tricks, ideas, solutions? Or more topics that should be taken care of? *** One of the pros for all of us of working with Open Data, is that data      will be reusable. For example, we ourselves are working on data written by people with an intellectual disability (ID), data with lots of specific errors (spelling, grammar), or just consisting of pictos. These data can only be used by us. That\u2019s a pity, as it is difficult to get such data, especially digital, for example from social media. Some issues we would like to discuss with you in this session, esp. when you are working with user-generated data (social media, web forums, online reviews, ...). * in general, how would you ask permission from everybody involved? Depending on the type of resource, people may be using nicknames, ... * how would you deal with users with some communicative issues, in general or wrt the language in question (apply text normalization, translation, ...? If so: on request or in general?) * and what with informed consent in such, from the user and/or a carer, guardian, ... And what about the ethical aspects? * how would you act when people recall their permission? * in such a case: what about older versions of your data already made available to other reseachers? * which metadata would you be make available to other researchers? All, or ...? Would you allow them to see the real names, etc? We presume that all people appearing in the data collection will be anonymized or pseudonymized (unless the names of public figures are involved.) !! We will try and get answers to issues arising during this session after\u00a0CLIN 2018 (for example during our ISI-NLP 2 workshop at LREC).", "title": "Open Data, Social Media and other User Generated Text: some topics for a discussion session"}]]], "end": false}, {"location": "Centrale hal", "start": "10:50", "title": "Coffee Break", "end": "11:10"}, {"location": "Kleine zaal", "start": "11:15", "title": "Welcome", "end": "11:25"}, {"location": "Kleine zaal", "start": "11:25", "title": "STIL Thesis Prize", "end": "11:35"}, {"location": "Kleine zaal", "start": "11:35", "title": "Invited speaker: Andy Way", "end": "12:40", "subslots":{"title":"TBA", "abstract":"tba"}}, {"location": "Keizer Karel Foyer", "start": "12:40", "title": "Lunch", "end": "13:30"}, {"location": "Centrale Hal", "start": "13:30", "title": "Poster Session", "end": "14:30"}, {"description": "", "start": "15:10", "title": "", "sessions": [{"location": "Kleine zaal", "title": "Deep learning"}, {"location": "Leeuwzaal", "title": "Language variation"}, {"location": "Soci\u00ebteitskamer", "title": "Parsing and Grammar"}, {"location": "Annazaal ", "title": "NLP for historical text"}], "subslots": [[[{"authors": "Madhumita Sushil, Simon Suster, Kim Luyckx and Walter Daelemans", "abstract": "Representation learning techniques have been used extensively within and outside the clinical domain to learn the semantics of words, phrases, and documents. We apply such techniques to create a patient semantic space where \u201csimilar\u201d patients should have similar vectors. We aim to capture patient similarity on multiple dimensions which would encapsulate a holistic view of the patients. To address the cases where there is a dearth of labeled data despite the abundance of unlabeled data, we create unsupervised dense patient representations from clinical notes in the freely available MIMIC-III database. We explore the usage of two neural representation learning architectures\u2014a stacked denoising autoencoder (SDAE), and a paragraph vector architecture\u2014for unsupervised learning. We evaluate these representations by predicting patient mortality, the primary diagnostic and procedural categories for the patients, and the gender of the patients. We compare their performance with sparse representations obtained from the bag-of-words and the bag-of-medical-concepts models. We observe that the generalized representations outperform the sparse representations when we have very few positive instances to learn from, and there is an absence of strong lexical features. It is critical to understand model behavior when statistical outputs influence clinical decisions. However, dense representations often capture semantics at a loss of interpretability. To interpret the dense representations learned by the SDAE, we calculate the mean squared reconstruction error of every feature after training the first layer of the SDAE. We observe a very high positive correlation between feature frequency and the reconstruction error, which we believe may be due to the high entropy of the frequent terms. Further, we calculate feature sensitivity across networks to extract the most influential features for the classification output on using the SDAE representations as the input. We find that the classifiers give high importance to sensible frequent features for most of the tasks. Although we input a bag-of-words representation to the SDAE, co-occurrence of different terms is reflected in the extracted features. We further observe that there is a minimal overlap between the sets of important features for the different tasks, which reflects the task-independent nature of the representations.", "title": "Unsupervised dense patient representations with interpretable classification decisions"}], [{"authors": "Eva Vanmassenhove and Andy Way", "abstract": "Neural Machine Translation (NMT) models have recently become the state-of-the art in the field of Machine Translation (Bahdanau et al. 2014, Cho et al. 2014, Kalchbrenner et al. 2014, Sutskever et al. 2014). Compared to Statistical Machine Translation (SMT), the previous state- of-the-art, NMT performs particularly well when it comes to word-reorderings and translations involving morphologically rich languages (Bentivogli et al. 2016). Although NMT seems to partially learn or generalize some patterns related to syntax from the raw, sentence-aligned parallel data, more complex phenomena (e.g. prepositional-phrase attachment) remain problematic (Bentivogli et al. 2016). More recent work showed that explicitly modeling extra syntactic information into an NMT system on the source (and/or target) side improves the translation quality 1 : Sennrich and Haddow (2016) integrated morphological information, POS-tags and dependency labels in the form of features on the source side of the NMT model while Nadejde et al. (2017) introduced syntactic information in the form of CCG supertags on both the source and the target side. Moreover, Nadejde et al. (2017) showed that a shared embedding space, where syntax information and words are tightly coupled, is more effective than multitask training. When integrating linguistic information into an MT system, following the central role assigned to syntax by many linguists, the focus has been mainly on the integration of syntactic features. Although there has been some work on semantic features for SMT (Banchs and Costa-Juss\u00e0 2011), so far, no work has been done on enriching NMT systems with more general semantic features at the word-level. This might be explained by the fact that NMT models already have means of learning word-embeddings since words are represented in a common vector space. However, making some level of semantics more explicitly available at the word level can provide the translation system with a higher level of abstraction and generalization beneficial to learn more complex constructions. Furthermore, a combination of both syntactic and semantic features would provide the NMT system with a way of learning semantico-syntactic patterns. To apply semantic abstractions at the word-level that enable a characterisation beyond that what can be superficially derived, coarse-grained semantic classes can be used. Inspired by Named Entity Recognition (NER) which provides such abstractions for a limited set of words, supersense- tagging uses an inventory of more general semantic classes (for nouns and verbs) for domain-independent settings (Schneider and Smith 2015). We investigate the effect of integrating super-sense features (26 for nouns, 15 for verbs) into an NMT system. To obtain these features, we used theAMALGrAM 2.0 tool (Schneider et al. 2014, Schneider and Smith 2015) which analyses the input sentence for multi-word expressions as well as noun and verb supersenses. The features are integrated using the framework of Sennrich et al. (2016), replicating the tags for every subword unit obtained by byte-pair encoding (BPE). We further experiment with a combination of semantic supersenses and syntactic supertag features (CCG syntactic categories (Steedman 2000) using EasySRL (Lewis et al. 2015)) and less complex features such as POS-tags, assuming that supersense-tags have the potential to be useful especially in combination with syntactic information.", "title": "SuperNMT: Integrating Supersense and Supertag Features into Neural Machine Translation"}], [{"authors": "Chao Li, Carsten van Weelden, Luigi Lorato, Carsten L. Hansen, Mihai Rotaru and Jakub Zavrel", "abstract": "Distributed\u200b word\u200b embeddings\u200b offer\u200b continuous\u200b vector\u200b representations\u200b that\u200b can\u200b capture\u200b rich contextual\u200b semantics.They\u200b have\u200b become\u200b the\u200b standard\u200b component\u200b in\u200b downstream\u200b NLP\u200b systems, and\u200b help\u200b models\u200b with\u200b modern\u200b neural\u200b network\u200b architectures\u200b achieve\u200b state\u200b of\u200b the\u200b art\u200b results\u200b on most\u200b NLP\u200b tasks.\u200b In\u200b addition\u200b to\u200b improving\u200b the\u200b performance\u200b on\u200b monolingual\u200b NLP\u200b tasks,\u200b shared representations\u200b of\u200b words\u200b across\u200b languages\u200b offers\u200b intriguing\u200b possibilities.\u200b Crosslingual embedding\u200b can\u200b potentially\u200b enrich\u200b the\u200b monolingual\u200b learning\u200b in\u200b languages\u200b with\u200b less\u200b training\u200b data. For\u200b example,\u200b we\u200b could\u200b borrow\u200b data\u200b from\u200b a language\u200b with\u200b sufficient\u200b training\u200b data\u200b to\u200b help\u200b train\u200b a model\u200b in\u200b another\u200b language\u200b where\u200b only\u200b limited\u200b resources\u200b are\u200b available.\u200b And\u200b one\u200b step\u200b further,\u200b we might\u200b be\u200b able\u200b to\u200b deploy\u200b models\u200b trained\u200b in\u200b one\u200b language\u200b to\u200b work\u200b on\u200b problems\u200b in\u200b other languages. Motivated\u200b by\u200b these\u200b insights,\u200b this\u200b project\u200b studied\u200b techniques\u200b for\u200b crosslingual\u200b embedding,\u200b and used\u200b them\u200b in\u200b a recurrent\u200b neural\u200b network\u200b sequence\u200b labelling\u200b model.\u200b Pioneer\u200b works in\u200b this fields\u200b focused\u200b on\u200b bilingual\u200b word\u200b embeddings\u200b from\u200b a pair\u200b of\u200b languages.\u200b Recently,\u200b there\u200b are\u200b also studies\u200b to\u200b align\u200b real\u200b crosslingual\u200b embeddings.\u200b In\u200b line\u200b with\u200b these\u200b studies,\u200b we\u200b propose\u200b a canonical\u200b correlation\u200b analysis\u200b (CCA)\u200b based\u200b transformation\u200b method\u200b to\u200b learn\u200b shared\u200b semantic representations\u200b by\u200b aligning\u200b word\u200b embeddings\u200b from\u200b multiple\u200b languages. The\u200b embedding\u200b is\u200b then\u200b used\u200b in\u200b the\u200b input\u200b layer\u200b of\u200b a bidirectional\u200b LSTM\u200b network.\u200b While\u200b our previous\u200b HMM/CRF\u200b based\u200b model\u200b has\u200b used\u200b hand-engineered\u200b features\u200b that\u200b are\u200b hard\u200b to\u200b be generalized\u200b across\u200b languages,\u200b the\u200b neural\u200b network\u200b model\u200b is\u200b able\u200b to\u200b learn\u200b relevant\u200b features automatically\u200b with\u200b better\u200b generalization,\u200b and\u200b enables\u200b rapid\u200b development\u200b of\u200b new\u200b language models\u200b at\u200b lower\u200b cost. In\u200b the\u200b present\u200b work,\u200b we\u200b aligned\u200b word\u200b embeddings\u200b of\u200b 4 languages\u200b (English,\u200b German,\u200b French, and\u200b Dutch).\u200b We\u200b evaluated\u200b the\u200b quality\u200b of\u200b the\u200b crosslingual\u200b embeddings\u200b by\u200b checking\u200b the neighbouring\u200b words\u200b of\u200b a \u200bword\u200b from\u200b another\u200b language,\u200b for\u200b example,\u200b the\u200b top\u200b 5 similar\u200b French words\u200b of\u200b a given\u200b English\u200b word.\u200b The\u200b experiment\u200b also\u200b shows\u200b that\u200b using\u200b the\u200b crosslingual embedding,\u200b a model\u200b trained\u200b on\u200b only\u200b 10%\u200b of\u200b the\u200b German\u200b training\u200b data\u200b combined\u200b with\u200b the\u200b English data\u200b performs\u200b similar\u200b to\u200b the\u200b baseline\u200b model\u200b trained\u200b purely\u200b on\u200b 100%\u200b German\u200b training\u200b data.", "title": "Towards multilingual sequence labelling using cross-lingual word embeddings"}]], [[{"authors": "Hans Van Halteren and Nelleke Oostdijk", "abstract": "t is generally accepted that the form of a text is influenced by various factors, foremost among which are a) who wrote it (author), b) what it is about (topic), and c) which form was chosen to optimally reach the intended effect in the intended target audience (communicative situation, more or less also covered by the terms genre/register). For each of these three factors, there are NLP tasks to identify them for a specific text. From these topic detection and authorship recognition are most likely known to the CLIN audience. Genre recognition has drawn less attention so far, and is often limited to high level distinctions, with broad classes like fiction and academic writing. However, that genre/register do influence language use in a text will be clear to anyone attempting authorship recognition, where both genre/register and topic are strong confounding factors in the desired text classification. We would like to advance all three classification tasks by mapping which classification features are bound to which of the three factors, and to which degree. Before attacking this ultimate question, however, we first need some more clarity regarding genre/register. Especially, we would like to employ much more fine-grained groupings, such as romance fiction and medical texts. This means we have to determine whether such labels lead to groups of texts which have enough internal consistency, and enough contrast with other groups, that they can be recognized. In this talk, we start our investigation with romance fiction, a genre for which we most expect to find clear stylistic properties. Using texts from the British National Corpus, we look at various stylistic features, and investigate how these differ between romance fiction and other texts, but also how they vary within the genre, e.g. between authors.", "title": "Fine-grained genre/register recognition: romance fiction in the BNC"}], [{"authors": "Agnes Tellings, Nelleke Oostdijk, Iris Monster, Franc Grootjen and Antal van den Bosch", "abstract": "This presentation introduces BasiScript, a 9-million-word corpus of contemporary Dutchtexts written by primary school children. The data were collected over three years with manychildren contributing texts throughout this period. Each word token in the corpus isannotated with the correct orthographical form, the associated lemma and the part-of-speech.The most frequent polysemous words have been annotated for word meaning, while allwords in the lexicon have been annotated for corpus and subcorpora frequency, distribution,length, family size, family frequency, orthographic neighborhood size, and orthographicneighborhood frequency. Images of the texts are available. The present article first describesthe corpus. Then it presents some results from an initial exploration of the corpus, namely, acomparison of BasiScript with BasiLex (a Dutch corpus with texts primary school childrenread) by means of frequency profiling, and an analysis of spelling errors children makeregarding diphthongs and verb forms.", "title": "BasiScript: a 9 million word corpus of texts written in primary school"}], [{"authors": "Florian Kunneman, Liesbeth Mollema, Antal van den Bosch, Mattijs Lambooij, Albert Wong and Hester de Melker", "abstract": "We present a case study on the categorization of Twitter messages that refer to vaccination, a much-discussed topic on social media. In order to assist the Dutch Rijksinstituut voor Volksgezondheid en Milieu (RIVM) in monitoring this discussion, we aimed to build a system that can detect the stance towards vaccination in these messages. Especially a rapid spread of rumours about the negative effects of vaccination can pose a threat to this institute, which is responsible for the Dutch vaccination program and thereby has interest in a wide vaccination coverage. A system that can detect a sudden increase of messages with a negative stance can alarm the institute and enable them to take timely action if the information that is shared seems to be based on falsive information. Identifying the stance towards vaccination in messages is comparable to identifying the polarity of a message, in the sense that messages are flagged as positive, negative or neutral. However, a tweet that conveys a negative stance towards vaccination might be positive in sentiment and vice versa. Hence, off-the-shelf tools for sentiment analysis do not seem applicable to this problem. Alternatively, we collect Twitter messages that mention a vaccination-related key term and have them annotated by means of three categorizations. First, the annotator is asked whether a message is relevant or irrelevant. This is an important distinction, as the word 'vaccin' might be used as a metafor in a different context or refer to animal vaccination, making the tweet irrelevant. Second, the annotator is asked whether the tweet conveys a positive, negative or neutral stance towards vaccination, or whether the stance is unclear to the reader. Third, the tweet is categorized into one of the fine-grained sentiments 'frustration', 'concern', 'fear', 'information' and 'other'. This latter categorization might assist the detection of tweets with a negative stance. The resulting categorizations are used to train and test a machine learning classifier on detecting tweets with a negative stance. We experimented with a strict and lax usage of the annotated tweets, as well as different categorizations and classifiers. We will present an overview of the different approaches and compare their predictions to those of an off-the-shelf sentiment analysis tool. Based on these outcomes, we will discuss the challenges of the task and avenues for future work.", "title": "Monitoring stance towards vaccination in Twitter messages"}]], [[{"authors": "Rob van der Goot and Gertjan Van Noord", "abstract": "Whereas parser performance on news texts keeps getting closer to human performance, parsers still perform drastically worse out-of-domain. Multiple approaches have already been explored for domain adaptation; up-training, weighing of training data and integrating disfluency detection are some common approaches. In this work we will focus on another approach: normalization. More concretely, we examine different approaches on integrating normalization into a neural network dependency parser. We will focus on Twitter data; we annotated a small treebank using the Universal Dependencies format for evaluation purposes. Our baseline parser is the UUParser(de Lhoneux et al, 2017), which is an Arc-Hybrid BiLSTM parser. This parser exploits character embeddings, and includes an option to initialize with pre-trained word embeddings. We use word2vec to train word embeddings on a big Twitter corpus. Both the character embeddings and the pre-trained model increased the performance of the parser. In a domain adaptation setup, where we train on Wall Street Journal and test on Twitter, the performance improvement is even bigger. This is probably mainly an effect of solving the unknown word problem, which is also adressed by a normalization-based approach. This leads to our main research question: can we make use of normalization to increase performance beyond the use of character level models and pre-trained word embeddings? We use an existing normalization model, which does normalization on the word level. It generates candidates using the Aspell spell checker, word embeddings and a lookup list. Features are directly taken from the generation step and supplemented with N-gram probabilities. A random forest classifier is then trained to predict the probability that a candidate belongs to the \u2018correct\u2019 class; this enables the system to output a top-n list of candidates and their probabilities. When using normalization as a straightforward pre-processing component, we observe a small increase in LAS. However, the normalization component makes mistakes, these propagate directly to the parser. And even if we would have access to a perfect normalization sequence, it might still be informative to take the original token into account during parsing. To fully exploit the potential of the normalization model we combine the vectors of the top-n normalization candidates into one vector. We weight the vector of each candidate by the probably from the normalization model, and then sum the vectors of all candidates.It should be noted that this approach can also be generalized to other neural network parsers, and even to other tasks. In our initial experiments normalization improved parser performance, even when using charactel embeddings, and pre-trained word embeddings. The in- tegration of multiple normalization candidates improved performance even further, indicating that the top-n candidates are also informative. More in-depth evaluation will be included in the presentation.", "title": "Lexical Normalization for Neural Network Parsing"}], [{"authors": "Christoph Aurnhammer", "abstract": "This work addresses the phenomenon of (ir)regularity in the past tense forms of verbs, a subject widely studied in linguistics (Pinker and Ullman, 2002; Pinker, 2001; Prasada and Pinker, 1993; Bybee and Moder, 1983). It is a common assumption, claimed by, among others, Kim et al. (1991), that the formation of either a regular or an irregular preterite form is exclusively due to phonological or orthographic form. The works of Baayen and Moscoso del Prado Mart\u0131\u0301n (2005) and Tabak et al. (2005) allow to question this assumption by stressing the additional importance of semantic density for past tense regularity. Inspired by their work, I present a memory-based model of past tense regularity of Dutch simplex verbs. The computational model investigates the importance of phonological similarity and semantic density of verbs for the regularity of their past tense forms. The dataset of Baayen and Moscoso del Prado Mart\u0131\u0301n (2005) is modeled using memory-based classification. This is implemented by the IB1 algorithm in TiMBL (Daelemans et al., 2004). The classifier is presented with, first, phonological transcriptions of verbs, second, semantic density measures and, last, a combination of the two. Using the combined feature set results in the highest classification accuracy (85,8%). Classification turns out most difficult on the irregular class. An analysis of classification errors shows that adding semantic density information resolves erroneous regularization. The computational model thus replicates the findings of Baayen and Moscoso del Prado Mart\u0131\u0301n (2005) and Tabak et al. (2005) by demonstrating the importance of semantic density in Dutch verb's past tense regularity. Beyond that, it demonstrates the potential of purely memory-based language processing to model regular -- irregular distinctions.", "title": "Memory Based Modelling of Dutch Past Tense Regularity"}], [{"authors": "Tom Vanallemeersch", "abstract": "Translation memories (TM) are tools which store sentences and their translation and allow for retrieving fuzzy matches based on similarity. In translation workflows, TMs are often used independently from tools for machine translation (MT), in spite of accumulating evidence that the integration of both tool types can lead to gains in translation efficiency. In the SCATE project (Smart Computer-Aided Translation Environment), we develop a method for integrating TM and MT which constrains MT output by pre-translating parts of the input sentence using consistently aligned sub-segments stemming from one or more TM matches. Pre-translation takes place using the XML markup functionality in the Moses toolkit for statistical MT. Our integrated system adopts two sliders to associate a fuzzy match score range with one of three strategies: the first one selects MT output, the second one selects the translation of the best fuzzy match (TM output), and the third one chooses the output resulting from pre-translation. In case of high-scoring fuzzy matches, an additional strategy \u201crepairs\u201d the translation of a match through small changes. Our first baseline consists of pure MT output for all test sentences, our second baseline of pure TM output. We tune our integrated system using a hill climber and evaluate translations by means of three metrics (BLEU, METEOR and TER). We perform tests on the TM of the Directorate-General for Translation of the European Commission, for ten language pairs involving different families (English to Dutch, German, French, Polish and Hungarian, and vice versa). Some of these pairs are known to be challenging for MT due to differences in word order and richness of the morphological system. For all language pairs, we observe significantly better scores when applying our integrated system. It has been incorporated into the prototype of the SCATE project, which demonstrates an interface for professional translators and presents different types of translation suggestions. The integration of TM and MT being work in progress, we discuss further research related to the construction and evaluation of a neural MT system for the above TM and language pairs.", "title": "MT rules: balancing words and trees"}]], [[{"authors": "Camil Staps", "abstract": "Due to the small corpus of biblical Hebrew, dating texts based on linguistic properties is tedious. Young, Rezetko, and Ehrensv\u00e4rd (2008) collected 88 grammatical properties from literature that could be typical of late biblical Hebrew, but conclude that they are (almost) all fuzzy and/or rare, making it hard, if not impossible, to date texts based on them. We translate these properties to queries to the ETCBC treebank of the Hebrew Bible (Roorda et al. 2017) and train a classifier on them to distinguish early and late biblical Hebrew. This approach allows us to consider many linguistic properties at once, which helps resolve issues of fuzziness and scarceness. Considering the size of the corpus (ca. 420,000 words), we cannot expect a classifier to date texts with very high granularity or accuracy. However, inspecting the models after training provides insights into the patterns behind language variation. Working with small corpora is part and parcel of the study of many dead languages. Com- putational methods can prove useful, but must be used carefully to avoid over-fitting. While developing and applying them, linguistic expertise must be consulted continuously. We show what methods, in particular sliding windowing and leave-one-out cross validation, can help with scarcity of data and rare features. We discuss which pitfalls one has to avoid to keep the analysis sound.", "title": "Combining grammatical features for dating texts in small-corpus languages"}], [{"authors": "Katrien Depuydt, Maarten Van Gompel, Hennie Brugman, Jesse de Does and Gosse Bouma", "abstract": "The Nederlab projects makes available a large diachronic corpus of Dutch, from the 6th until the 21st century in a virtual research environ- ment which allows extensive search of the texts. The corpus is based on available digital text material. The text material is converted into one format, FoLiA (van Gompel, 2012), and special attention is given to uniform metadata. To try to enhance the quality of the OCR\u2019ed material in the corpus, automatic post-correction is applied with TICCL (Reynaert, 2010). To allow for better searching, the text material is annotated with Part of Speech information (PoS) and lemma. Another annotation layer is the links from named entities to an external resource like Wikipedia. In this talk we will focus on the linguistic annotation with PoS and lemma and more in particular, the various challenges we have been facing, tools for linguistic annotation need training and training material. We deliver the Nederlab enrichment pipeline is delivered as part of PICCL. One of the main challenges is that historical training material is scarce and hard to come by in sufficient quantity, and even more so in the case of Nederlab, where a very large time span is covered. From the start of the project, it was therefore decided to work with Frog (Van den Bosch et al., 2007) trained on a corpus of approximately 11 million word forms annotated with the tagset for the Corpus Gesproken Nederlands (van Eynde, 2004) and to see how we could improve the tagging of historical Dutch by adding a modern Dutch translation. The experiment was performed on 17th Cen- tury Dutch [Sang, 2016]. Exploratory evaluation of the results of this ap- proach brought to light several issues: a) the CGN tagset is not entirely suitable for historical Dutch b) our current tokenisers cannot deal with the fact that words in historical Dutch have spaces, and c) the approach by which tagging is improved by translation of historical Dutch is only applicable for text material until 1800. After having made an inventory of existing training material and a division into different language phases, it was decided to go for a mixed approach for linguistic annotation, implying conversion and training of existing golden standards. But it was also decided to produce enough evaluation material for each language phase for the annotation with main PoS and lemma, so as to be able to assess the quality of the approach taken. The evaluation material will be made publicly available. In this way, we hope to come to solid recommendations for future work.", "title": "Nederlab: Progress and challenges in linguistic enrichment of historical Dutch Texts"}], [{"authors": "Marieke van Erp, Jesse De Does, Thomas van Goethem and Katrien Depuydt", "abstract": "he\u200b availability\u200b of\u200b large\u200b text\u200b archives\u200b such\u200b as\u200b Delpher\u200b present\u200b humanities\u200b researchers\u200b with\u200b a wealth\u200b of information.\u200b However,\u200b such\u200b archives\u200b do\u200b not\u200b provide\u200b researchers\u200b with\u200b a precisely\u200b scoped\u200b collection\u200b that fits\u200b their\u200b research\u200b question.\u200b The\u200b current\u200b research\u200b practice\u200b involves\u200b sending\u200b a query\u200b to\u200b an\u200b archive interface,\u200b click\u200b every\u200b document\u200b link,\u200b read\u200b the\u200b document\u200b and\u200b record\u200b whether\u200b it\u200b is\u200b relevant\u200b to\u200b their research\u200b question\u200b or\u200b not.\u200b Automating\u200b this\u200b step\u200b would\u200b leave\u200b researchers\u200b with\u200b more\u200b time\u200b to\u200b analyse\u200b only relevant\u200b documents\u200b to\u200b answer\u200b their\u200b research\u200b question. The\u200b CLARIAH\u200b SERPENS\u200b pilot\u200b project\u200b aims\u200b to\u200b aid\u200b historical\u200b ecologists\u200b in\u200b finding\u200b information\u200b from\u200b the Dutch\u200b National\u200b Library\u2019s\u200b newspaper\u200b collection\u200b by\u200b employing\u200b semantic\u200b web\u200b and\u200b language\u200b technology. Historical\u200b ecology\u200b is\u200b an\u200b interdisciplinary\u200b research\u200b field\u200b that\u200b investigates\u200b the\u200b interactions\u200b between humans\u200b and\u200b their\u200b environment.\u200b The\u200b research\u200b in\u200b SERPENS\u200b is\u200b driven\u200b by\u200b the\u200b question\u200b how\u200b the\u200b public perception\u200b of\u200b certain\u200b species\u200b changes\u200b over\u200b time\u200b (Lenders, 2014).\u200b To\u200b this\u200b end,\u200b we\u200b obtained\u200b a set\u200b of\u200b documents\u200b from the\u200b Dutch\u200b National\u200b Library\u2019s\u200b newspaper\u200b collection\u200b by\u200b querying\u200b for\u200b \u201cLynx\u201d\u200b and\u200b \u201cBunzing\u201d\u200b (European polecat).\u200b However,\u200b we\u200b find\u200b that\u200b many\u200b returned\u200b articles\u200b do\u200b not\u200b concern\u200b animals\u200b but\u200b for\u200b example\u200b a magazine\u200b or\u200b ship\u200b whose\u200b name\u200b is\u200b derived\u200b from\u200b an\u200b animal\u200b name. During\u200b the\u200b first\u200b phase\u200b of\u200b the\u200b project,\u200b we\u200b identified\u200b the\u200b issues\u200b that\u200b complicate\u200b collecting\u200b relevant documents,\u200b we\u200b defined\u200b relevant\u200b document\u200b categories\u200b and\u200b performed\u200b a manual\u200b annotation\u200b of\u200b 7,902 documents\u200b to\u200b classify\u200b documents\u200b into\u200b one\u200b of\u200b these\u200b categories\u200b (\u200bvan\u200b Erp et al., 2017). In\u200b our\u200b first\u200b classification\u200b experiments,\u200b we\u200b focus\u200b on\u200b identification\u200b of\u200b OCR\u200b problems\u200b using\u200b a lexicon-based method\u200b as\u200b well\u200b as\u200b automatic\u200b classification\u200b experiments\u200b using\u200b the\u200b twelve\u200b categories\u200b defined\u200b in\u200b (\u200bvan\u200b Erp et al., 2017).\u200b In our\u200b first\u200b series\u200b of\u200b experiments,\u200b we\u200b achieve\u200b F1\u200b measures\u200b of\u200b 77.62\u200b (fastText)\u200b and\u200b 86.24\u200b (mallet)\u200b but\u200b more research\u200b is\u200b needed\u200b to\u200b overcome\u200b the\u200b class\u200b imbalance\u200b and\u200b classify\u200b the\u200b most\u200b important\u200b categories\u200b to ecological\u200b historians\u200b correctly. In\u200b the\u200b coming\u200b months,\u200b we\u200b will\u200b test\u200b unsupervised\u200b techniques\u200b such\u200b as\u200b LDA\u200b to\u200b try\u200b to\u200b strengthen\u200b our\u200b grip on\u200b the\u200b material,\u200b and\u200b test\u200b our\u200b techniques\u200b on\u200b a larger\u200b set\u200b of\u200b species.", "title": "Good lynx, bad lynx: document enrichment for historical ecologists"}]]], "end": false}, {"location": "Keizer Karel Foyer", "start": "15:30", "title": "Coffee and beverages", "end": "15:55"}, {"description": "", "start": "16:55", "title": "", "sessions": [{"location": "Kleine zaal", "title": "Social media and winner STIL Thesis Prize"}, {"location": "Leeuwzaal", "title": "Authorship and Discourse"}, {"location": "Soci\u00ebteitskamer", "title": "Named Entity Recognition and Term Extraction"}, {"location": "Annazaal", "title": "Translation"}], "subslots": [[[{"authors": "Long Tran", "abstract": "Along with its tremendous benefits, the advent of technology has also brought about so many serious problems. Recent years have seen cyberbullying having taken its tragic toll on internet users. Cyberbullying is often described as \u201cwillful and repeated harm inflicted through the use of computers, cell phones, and other electronic devices\u201d. Cyberbullying incidents have showed a steady increasing trend. This trend was proven by 2 surveys in 2013 and 2017 with more than 10,000 participants in the latter. The survey showed that 7 in 10 youths are victims of cyberbullying, 37% of which experience it on a highly frequent basis. Six extremely catastrophic incidents of cyberbullying presses a huge demand for tackling this problem. Little research have been carried out using machine learning to detect bullying messages in many different social network domains such as Formstring.com, Youtube.com and Twitter.com. One of the challenges these works have in common is the moderate amount of publicly available datasets due to privacy concerns, and those available are surprisingly small in positive samples (bullying messages). Therefore, the results obtained did not seem to be satisfied, and the classification model from one domain might not be applicable to others. Another difficulty is that each dataset has its own ground truth as it was annotated differently. For example, some datasets were annotated with only label \u201cbullying\u201d and \u201cnon-bullying\u201d message, others also annotated messages from the bullies, the victims and the supporters (someone who tries to help the victims). Thus, it makes it more troublesome to perform cross domain machine learning in this task. It is reasonable to assume that internet users tend to use their same ways of texting across social media domains. Therefore, a model should be able to classify text messages regardless of the social networking sites where they were posted. Given such challenges, this work will propose a technique to apply transfer learning from one domain to another within the context of classifying cyberbullying. Using training data across different domains will help to address the problem of having too few positive data samples in separate available datasets, thereby generally improving classification accuracy.", "title": "Transfer Learning on Cyberbullying Classification"}], [{"authors": "Ali H\u00fcrriyeto\u011flu, Nelleke Oostdijk, Antal van Den Bosch and Mustafa Erkan Basar", "abstract": "Tweet collections comprise a relatively new document type. The form, motivation behind their generation, and intended function of the tweets are more diverse than traditional document types such as essays or news articles. Understanding this diversity is essential for extracting relevant information from tweets. However, the Twitter platform does not provide any kind of infrastructure other than hashtags, which is limited by the number of users who know about it, to organize tweets. Thus, collecting and analyzing tweets introduces various challenges (Imran et al., 2015). Using one or more key terms and/or a geographical area to collect tweets is prone to cause the final collection to be incomplete or unbalanced (Olteanu et al., 2014), which in turn decreases our ability to leverage the available informa tion. In order to alleviate some of the problems that users experience, we developed Relevancer which aims to support experts in analyzing tweet sets collected via imprecise queries in the context of high-impact events. 4 Experts can define their information need with up-to-date information in terms of automatically detected information threads (H\u00fcrriyeto\u011flu et al., 2016a), and use these annotations to organize unlabeled tweets. The main observations and contributions that are integrated in Relevancer to help experts to come to grips with event-related event collections are: (i) almost every event-related tweet collection comprises tweets about similar but irrelevant events (H\u00fcrriyeto\u011flu et al., 2016a); (ii) by taking into account the temporal distribution of the tweets about an event it is possible to achieve an increase in the quality of the information thread detection and a decrease in computation time (H\u00fcrriyeto\u011flu et al., 2016b); and (iii) the use of inflection-aware key terms can decrease the degree of ambiguity (H\u00fcrriyeto\u011flu et al., 2016c). At the end of the analysis process, the experts understands the data and is able to use his understanding in an operational setting to classify new tweets. The worst case performance of the classifier is significantly better than a majority class based baseline. The tool is supported by a web interface that can potentially be used to monitor and improve the performance in a particular use-case in real time.", "title": "Relevant Information Detection on Twitter"}], [{"authors": "Ali H\u00fcrriyeto\u011flu and Piet Daas", "abstract": "The usefulness of the content on social media for a particular statistical use depends on the characteristics of the users that contribute to generating it. Thus, the level of our understanding of the user characteristics affects the quality and the completeness, or selectivity, of the outcome. As such, every social media use case needs to be aware of the characteristics of the users in the tweets collected and analyzed. Especially when social media messages are used as an input source for official Dutch statistics, the composition of the population producing these messages should -by definition-be located in the Netherlands. The language in which a tweet is written is an important criterion for that, but the language of a tweet as detected by Twitter is not enough. The study reported here focusses on classifying users that write tweets in the Dutch language, in one of its two main dialects: 1) the language used in the Netherlands (simply called Dutch from hereon) and 2) the language used in Flanders (Flemish). So, to be absolutely clear, this study only focusses on discerning Twitter messages written in Flemish from those written by inhabitants of the Netherlands; no other dialects were included here. The study consists of two steps. The first step is to use the data in the location field of a Twitter user profile to determine the location of a user. This is often a municipality. The goal of the first step is the creation of a set of users for which it is certain that they are located in either the Netherlands or Flanders. The next step is to use the text of the tweets to identify the dialect of a user. Our sample is all users from 2016 on TwiNL (Tjong Kim Sang and van den Bosch, 2013). Place recognition was performed using Pyparsing (McGuire, 2007) on the profile location of a Twitter user. We developed grammars that enable disambiguation of place names by using language specific rules. The rules contain definitions about the context of a place name in the text as well. The context is defined in terms of separator punctuation, conjunctives, determiners, directions or affixes. The result of all of this is the identification of 1,484,427 unique location strings, of which 54,301 could be parsed and were either located in the Netherlands or Belgium. The recognized place names occur in the location field of 825,373 (14%) of the users that have a location string on their profile. This is 9% of the whole population. We trained a support vector machine (SVM) classifier using the Corpus Gesproken Nederlands (CGN) and tweet text data that is collected from users that states their location on their profile. We extracted unigram and bigram bag-of-words (BoW) features and weight them with a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus (TF-IDF). The classifier accuracy and F1 on a test set that is created based on user profile location are 0.97 and 0.98 respectively. In addition to the performance metrics, we will present a detailed performance and error analysis of the results of location parsing and tweet text classification.", "title": "Using Location and Dialect to Classify Twitter Users from the Netherlands and Flanders"}], [{"authors": "Ali H\u00fcrriyeto\u011flu, Nelleke Oostdijk and Antal van Den Bosch", "abstract": "This study compares and explores ways to integrate machine learning and rule-based microtext classification techniques. The performance of each method is analyzed and reported, both when used standalone and when used in combination with the other method. The goal of this analysis is to effectively combine the two approaches. We aim to find what is similar and what is distinct between these two paradigms in order to come up with a hybrid methodology that will yield a higher performance, preferably both in precision and recall. We use gold standard data (tweets) released under the First Workshop on the Exploitation of Social Media for Emergency Relief and Preparedness (SMERP 2017) and the FIRE 2016 Microblog track: Information Extraction from Microblogs for training and testing our approaches. These data sets have a naturally unbalanced class distribution and tweets may be annotated with multiple labels. The first level of comparison was performed on the predicted topics by each approach. The results showed that each approach performs well on a distinct part of the test set. This observation was confirmed by having a decrease in precision when we use intersection of the predictions as the final prediction from each approach. The union of the predictions yielded an average precision and recall and 2% higher F1-score. We explored combining the two approaches at an earlier stage than the result level as well. We used the predictions of the rule-based system as training data for the machine learning system. Moreover, we excluded the tweets that do not have the same predicted value by the rule based system and the machine learning classifier trained on the SMERP data", "title": "Comparing and Integrating Machine Learning and Rule Based Microtext Classification"}]], [[{"authors": "Rianne Conijn, Simon Knight, Roberto Martinez and Menno van Zaanen", "abstract": "Writing academic texts is a skill that students often find hard to acquire. Whereas several tools exist that support low-level text writing aspects, such as spelling correction systems, not many tools are available that support students in constructing the high-level structure of academic texts, or the sentence-level rhetorical structures from which arguments are constructed. To help students improve their writing, it is important to provide them with feedback that goes beyond low-level surface properties such as spelling, grammar, and mechanics, by giving feedback on high-level, textual structure and coherence. In order to develop a feedback system that helps students write coherent, well-structured academic texts, we first need to know what \u2018good\u2019 academic texts look like. For this, we collected texts written by students in two disciplines: Law and Science. These texts have been analyzed at the level of rhetorical moves, which show the function of sentences in the text. The extracted rhetorical moves are: attitude, background, contrast, contribution, emphasis, novelty, question, surprise, trend, and very important sentence. This type of information does not aim to describe the overall structure of the text, but indicates the relationships between and functions of the sentences in the text. Sequences of rhetorical moves of a text can indicate the relationship between sentences, which provides information on the overall structure of the text. Information found in these sequences may be a stronger indicator of well structured and coherent texts. Most work on analyzing and evaluating writing products has focused on frequencies of textual features. While this may provide interesting and useful information, it does not allow for the uncovering of higher level sequential information as context is not taken into account. In this presentation, we will introduce a technique that identifies patterns (sequences of rhetorical moves) that can be found in texts from the Law and Science disciplines. The technique relies on notions from the field of information retrieval. Based on the information found by the technique, we show that we can identify differences in structure of the text between the different disciplines. The \u2018differential\u2019 patterns that characterize each of the two disciplines can be explained from the nature of the information that is described in the texts. The ultimate aim of the technique is to provide insight into the patterns of rhetorical moves that are preferred in texts in a specific discipline. This allows for the development of an automatic feedback system that can help students with the high-level structure in the texts they write.", "title": "Patterns of rhetorical structure in student\u2019s writing in different text genres"}], [{"authors": "Barbara Plank", "abstract": "It is well-known that written text carries a good deal of non-verbal infor- mation related to the author\u2019s identity and social factors, such as age, gender and personality. However, it is less clear to what extent behavioral biometric measures such as typist data transmit such information. In this talk I will investigate typist behavior to study the predictiveness of authorship and author profiles. Experiments on two datasets of increasing size are used, including a dataset to predict age and gender from keystroke dynamics. Preliminary results show that keystroke-based features lead to significantly higher accuracies for authorship than text-based measures. For user attribute prediction, the best approach is to combine the two, suggesting that extra-linguistic factors are disclosed to a larger degree in written text while author identity is better transmitted in typing behavior.", "title": "What can we learn from behavioral biometric data?"}], [{"authors": "Antske Fokkens, Ruigrok Nel, Sarah Gagenstein, Wouter Van Atteveldt and Beukeboom Camiel", "abstract": "This presentation introduces the task of microportrait extraction and applies it to identify whether Muslims are stereotyped, and if so, in what way, by the Dutch media. The core idea behind this research is that world views and perspectives are not only expressed by specific opinions, but also in more implicit ways: by what information we decide to share about an individual, event or topic and how we express this information. For instance, Vermeulen (p.c.) shows in a limited corpus study that citizens of Moroccan origin are quickly called \u2018a thief\u2019 (generic property of the person), whereas white Dutch per- petrators \u2018have stolen something\u2019 (incidental behavior). Microportrait extraction provides the technology to investigate whether such differences occur systematically. A microportrait provides all the information that a single document provides on a specific entity, concept or event. For entities, this includes the labels used to refer to them, the properties assigned to them (through, e.g. copula structures and adjectival modifiers) and the roles they play in activities (semantic roles related to events). By extracting microportraits from large amounts of data, we can investigate which labels, properties and activities tend to go occur, i.e. establish whether articles indeed talk about Moroccan thieves and others \u2018having stolen something\u2019. We created a pipeline consisting of Alpino (Bouma et al., 2001) for parsing Dutch text a new implementation of a coreference resolution system for Dutch, based on Stanford Multi-Pass Sieve for Coreference Resolution (Raghunathan et al., 2010), and a set of manually developed rules to link labels to properties and specific roles in activities. We extracted microportraits from 15,573 articles from various Dutch media. In the next step, we investigated which descriptions occurred more often than expected when an individual was labeled explicitly as a Muslim or as being Dutch (Ruigrok et al., 2017). The results show that the label Dutch is mainly used to refer to people playing (and particularly winning) in sports. When talking about Muslims, papers specifically refer to whether they are moderate or fundamental. We also observe several correlations with people labeled as Muslim and violence. In this presentation, we outline our methodology for microportrait extraction and present the results of our study on stereotyping in the Dutch media. A detailed evaluation of the accuracy of our microportrait extraction system is currently ongoing. We validated the method by verifying 1,058 descriptions from randomly selected articles. Manual inspection revealed that 98.1% of the descriptions was indeed found in the text and 87.2% of the descriptions was ordered in the correct group. These results seems suspiciously high, but this is mainly due to the the far majority of descriptions being expressed by a basic substructure in a sentence that the Alpino parser analyzes correctly despite possibly making errors in the rest of the sentence. We present the latest status of our evaluation corpus together with the results of the microportrait extraction algorithm.", "title": "Microportrait detection for identifying stereotypes in Dutch News"}], [{"authors": "Ben Verhoeven and Walter Daelemans", "abstract": "Recent initiatives such as the TextLink network 1 have created an impetus for the creation of new resources (and unification of previously existing resources) related to discourse structure of text. Discourse-annotated corpora, as well as lexicons of discourse connectives, are becoming available for an increasing number of languages. However, computational discourse analysis is still quite limited by the number of languages for which discourse parsers or large enough resources exist. We aim to contribute to the language diversity in discourse analysis by proposing a novel way to create discourse lexicons for multiple languages in which the items are categorized by the discourse relations they convey according to the Penn Discourse Treebank (PDTB) tags (The PDTB Research Group, 2008). Our approach is indebted to Lopes et al.\u2019s (2015) phrase tables of discourse connectives for all the languages in the Europarl parallel corpus (Koehn, 2005). The merit of our work is in converting these phrase tables to weighted lexicons by extrapolating the discourse relation category weights of the English connectives in the PDTB 2.0 corpus (Prasad et al., 2008) to these other languages, weighted by the correlation strength between English and the target language of the connectives in the phrase table. One of our interests is how the discourse structure of a text, based on such lexicons, can be used as features for gender profiling experiments. We use our weighted lexicons to create features and also evaluate our approach as such, extrinsically, by comparing its performance with that of a knowledge-based discourse lexicon (DiMLex) for German (Stede, 2002). We perform gender profiling experiments on five datasets, varying in three languages (English, Dutch, German) and two genres (news and blogs). All datasets were preprocessed to be balanced for gender. Standard features, such as character and word n-grams, and discourse features were both experimented with. Four variants of discourse features were tested. Three of these take discourse categories into account on three different levels of specificity, e.g. comparison.contrast.opposition illustrates the three levels. Our assumption is that abstracting over the specific connectives, to discourse categories of the text, might result in a better representation of the discourse of the text. The fourth variant is a mere count-based approach of all the connectives in the discourse lexicon. All experiments are conducted using tenfold cross-validation with three different learning algorithms (SGDClassifier, LogisticRegression, RandomForestClassifier) from sklearn (Pedregosa et al., 2011). We find that our discourse features perform quite well (0.57-0.64 F-score) when used for gender profiling on the three blog corpora, but hardly on the news corpora (0.50-0.54 F-score). The overall performance of the news corpora when using n-grams is also decidedly lower (0.62-0.68 F-score) than for blogs (0.88-0.92 F-score). Our results on the German dataset show that the discourse features from our own generated lexicon significantly outperform the discourse features from the knowledge-based DiMLex lexicon. This validates the reasoning of our lexicon generation approach thus making it a viable approach for all other languages for which no (or few) resources exist.", "title": "Discourse lexicons for multiple languages and their use for gender profiling"}]], [[{"authors": "Camiel Colruyt", "abstract": "Neural\u200b translation\u200b systems\u200b typically\u200b use\u200b a \u200b limited\u200b vocabulary\u200b in\u200b the\u200b target\u200b language\u200b to\u200b reduce computational\u200b demand\u200b during\u200b training.\u200b Because\u200b of\u200b this,\u200b neural\u200b models\u200b tend\u200b to\u200b generate more\u200b unknown\u200b words\u200b ('<unk>')\u200b during\u200b translation\u200b than\u200b statistical\u200b or\u200b phrase-based\u200b models. Neural\u200b network\u200b translation\u200b has\u200b repeatedly\u200b shown\u200b its\u200b worth\u200b in\u200b recent\u200b years,\u200b but\u200b vocabulary limitations\u200b and\u200b the\u200b unknown\u200b word\u200b problem\u200b remain\u200b a \u200b hurdle\u200b for\u200b further\u200b improvement. Named\u200b entities\u200b contribute\u200b a \u200b large\u200b portion\u200b of\u200b these\u200b unknown\u200b words;\u200b robust\u200b named\u200b entity resolution\u200b can\u200b immediately\u200b improve\u200b any\u200b neural\u200b system.\u200b This\u200b work\u200b explores\u200b the\u200b feasibility\u200b of\u200b a full-size\u200b neural\u200b model\u200b augmented\u200b with\u200b dedicated\u200b named\u200b entity\u200b resolution\u200b components\u200b for English\u200b to\u200b Chinese\u200b translation. We\u200b focus\u200b on\u200b the\u200b transcription\u200b of\u200b personal\u200b and\u200b place\u200b names\u200b from\u200b English\u200b into\u200b Chinese. Resolving\u200b such\u200b entities\u200b in\u200b language\u200b pairs\u200b using\u200b different\u200b writing\u200b systems\u200b is\u200b challenging,\u200b but important\u200b for\u200b a \u200b commercial\u200b translation\u200b system.\u200b Specifically,\u200b we\u200b show\u200b a \u200b method\u200b to automatically\u200b transcribe\u200b English\u200b names\u200b (or\u200b any\u200b name\u200b in\u200b the\u200b Latin\u200b alphabet)\u200b into\u200b Chinese characters.\u200b This\u200b approach\u200b consists\u200b of\u200b two\u200b components:\u200b a \u200b neural\u200b network\u200b trained\u200b for character-based\u200b transcription\u200b and\u200b a \u200b lookup\u200b dictionary\u200b or\u200b gazetteer.\u200b The\u200b gazetteer\u200b handles names\u200b not\u200b fit\u200b to\u200b be\u200b transcribed\u200b automatically. We\u200b derive\u200b a \u200b list\u200b of\u200b English-Chinese\u200b name\u200b pairs\u200b from\u200b a \u200b corpus\u200b of\u200b wikipedia\u200b article\u200b titles\u200b using a\u200b custom-made\u200b method.\u200b Part\u200b of\u200b this\u200b list\u200b becomes\u200b training\u200b data\u200b for\u200b the\u200b transcription\u200b network, the\u200b other\u200b becomes\u200b the\u200b gazetteer\u200b list.\u200b We\u200b find\u200b our\u200b transcription\u200b network\u200b generates\u200b useable transcriptions\u200b despite\u200b its\u200b simplicity. We\u200b train\u200b three\u200b full-size\u200b neural\u200b models\u200b using\u200b the\u200b OpenNMT\u200b framework:\u200b one\u200b baseline,\u200b one where\u200b named\u200b entities\u200b have\u200b been\u200b normalized,\u200b and\u200b one\u200b where\u200b entities\u200b consisting\u200b of\u200b multiple tokens\u200b are\u200b concatenated.\u200b Random\u200b artefact\u200b generation\u200b in\u200b the\u200b normalized\u200b model\u200b point\u200b to undesirable\u200b interference\u200b between\u200b our\u200b normalization\u200b methods\u200b and\u200b OpenNMT\u200b parameters;\u200b we correct\u200b for\u200b this.\u200b We\u200b augment\u200b both\u200b model\u200b with\u200b our\u200b phrase\u200b table\u200b system\u200b and\u200b transcription module\u200b and\u200b compare\u200b the\u200b results.", "title": "Improving named entity resolution for English to Chinese neural machine translation"}], [{"authors": "Hans Paulussen and Dirk De Hertog", "abstract": "This talk describes the need for and the issues involved in the creation of a gold standard for Named Entity Recognition (NER), limited to spatio-temporal relations as applied to descriptions of multilingual picture collections. The gold standard will be used for training NER-tools as part of the UGESCO project which aims at developing geo-temporal (meta)data extraction routines and enrichment tools to extend and link existing scientific collection items and facilitate spatio-temporal collection mapping for interactive querying. The enrichment will thus for instance enable to select pictures from the same location at different periods. The challenges we discuss are related to the nature and the type of texts used for picture descrip- tions and the types of enrichments required for geo-temporal named entity recognition. As such, the NER-task differs in various points from previous NER projects. The main issue is the brevity and the structure of the photo descriptions. Unlike ordinary running text, text samples describing pictures are usually short texts, lacking co-textual information, necessary to disambiguate the words. Description fields often have a reduced syntactic structure (e.g. subject missing, use of infinitives) which requires specific training and/or adaptation of the existing NLP tools, which are usually trained on full sentence texts having a verb and subject. Text brevity also has an influence in multilingual contexts. The database of photo descriptions covers text samples from different languages. In order to select language specific text samples, language identification is necessary. However, existing language identification tools expect longer stretches of text. Moreover, also within the description, languages can switch at sentence or word level. Due to the short text length of the descriptions and the language switch in the descriptions, language identification is hampered. The gold standard will consist of a Dutch and a French dataset, so that we can analyse in which way geo-temporal entities in the two languages are constructed. Previous NER research tasks have mainly focused on broad NE categories (i.e. persons, organisations and locations) found in mainly journalistic texts (Grishman and Sundheim 1996). In this project, the focus is on time and location. In fact, in this project, we are not only looking for general locations (such as names of cities, regions or countries), but also for points-of-interests, including specific buildings and roadways and important landmarks (Lee et al. 2006, Desmet and Hoste 2014). In other words, we limit the general NE categories, but expand the granularity of the subcategories of time and location. In the case of timestamps, the TIMEX categories are used. In the case of location, the following subcategories are used: place, street, road, waterway, building, space and monument. This subcategorization is motivated by the fact the project aims to improve fine-grained selection of locations. The granularity of locations has an influence on the type of annotations used. Whereas general NE categories cover mainly proper nouns, the location subcategories require multi word units, not limited to proper nouns. The granularity also requires a protocol defining how to delimit the boundaries of the multi word unit, which in turn has an influence on the evaluation of the NER tools. For example, \u201cchausse de Louvain\u201d could be interpreted as one unit and as such refers to a road, or only the proper noun could be selected, and then refers to a general location.", "title": "Geo-temporal named entity recognition in multilingual picture collections"}], [{"authors": "St\u00e9phan Tulkens, Dominiek Sandra and Walter Daelemans", "abstract": "Skilled\u200b word\u200b reading\u200b can\u200b be\u200b seen\u200b as\u200b knowing\u200b how\u200b to\u200b map\u200b orthographic\u200b word\u200b forms\u200b to\u200b their phonological\u200b and\u200b semantic\u200b counterparts.\u200b Crucially,\u200b this\u200b mapping\u200b is\u200b highly\u200b ambiguous,\u200b and causes\u200b inhibition\u200b in\u200b tasks\u200b where\u200b the\u200b ambiguity\u200b plays\u200b a role\u200b of\u200b import\u200b (Kawamoto\u200b & Zemblidge, 1992). Because\u200b of\u200b these\u200b inhibitory\u200b effects,\u200b computational\u200b models\u200b of\u200b word\u200b reading\u200b have\u200b assumed\u200b a sharp\u200b division\u200b between\u200b a word\u2019s\u200b Orthographic,\u200b Semantic\u200b and\u200b Phonological\u200b representations, making\u200b the\u200b assumption\u200b that\u200b the\u200b inhibitory\u200b effects\u200b are\u200b the\u200b result\u200b of\u200b an\u200b imperfect\u200b mapping between\u200b these\u200b representations\u200b (Dijkstra\u200b & Van\u200b Heuven,\u200b 2002;\u200b Harm\u200b & Seidenberg,\u200b 2004).\u200b This approach\u200b has\u200b several\u200b downsides:\u200b First,\u200b the\u200b links\u200b between\u200b the\u200b different\u200b levels\u200b of\u200b representation have\u200b to\u200b be\u200b pre-defined.\u200b Second,\u200b by\u200b assuming\u200b three\u200b distinct\u200b levels\u200b of\u200b representation,\u200b it\u200b is\u200b unclear which\u200b of\u200b these\u200b three\u200b distinct\u200b levels\u200b is\u200b actually\u200b used\u200b in\u200b reading.\u200b Third,\u200b research,\u200b especially\u200b in\u200b field of\u200b bilingualism,\u200b has\u200b shown\u200b that\u200b differences\u200b in\u200b task\u200b demands\u200b and\u200b list\u200b composition\u200b lead\u200b to different\u200b responses,\u200b which\u200b is\u200b something\u200b these\u200b models\u200b have\u200b trouble\u200b accounting\u200b for. We\u200b propose\u200b an\u200b alternative\u200b model,\u200b which\u200b we\u200b call\u200b Global\u200b Space,\u200b that\u200b doesn\u2019t\u200b assume\u200b the existence\u200b of\u200b three\u200b separate\u200b modes\u200b of\u200b representation,\u200b but\u200b instead\u200b represents\u200b words\u200b in\u200b a single vector\u200b space,\u200b which\u200b consists\u200b of\u200b the\u200b concatenation\u200b of\u200b all\u200b three\u200b forms\u200b of\u200b representation.\u200b Words are\u200b thus\u200b represented\u200b by\u200b the\u200b concatenation\u200b of\u200b an\u200b orthographic,\u200b phonological\u200b and\u200b semantic representation,\u200b and\u200b word\u200b reading\u200b is\u200b then\u200b reduced\u200b to\u200b the\u200b reconstruction\u200b of\u200b the\u200b phonological\u200b and semantic\u200b parts\u200b of\u200b the\u200b vector\u200b given\u200b the\u200b orthographic\u200b part\u200b of\u200b the\u200b vector. This\u200b alleviates\u200b the\u200b problems\u200b above:\u200b the\u200b model\u200b no\u200b longer\u200b makes\u200b explicit\u200b theoretical\u200b assumptions about\u200b how\u200b the\u200b different\u200b representations\u200b are\u200b linked\u200b to\u200b each\u200b other,\u200b while\u200b also\u200b gaining\u200b a single representation\u200b for\u200b each\u200b word.\u200b In\u200b addition,\u200b several\u200b observed\u200b effects\u200b in\u200b inhibition\u200b or\u200b facilitation through\u200b differing\u200b task\u200b demands\u200b can\u200b be\u200b explained\u200b by\u200b a repartitioning\u200b of\u200b the\u200b space,\u200b e.g.,\u200b if\u200b a task requires\u200b a phonological\u200b response,\u200b the\u200b model\u200b can\u200b calculate\u200b the\u200b activation\u200b over\u200b the\u200b orthographic and\u200b phonological\u200b parts\u200b of\u200b the\u200b space,\u200b while\u200b leaving\u200b out\u200b the\u200b semantic\u200b part\u200b of\u200b the\u200b space,\u200b thus yielding\u200b different\u200b responses\u200b in\u200b different\u200b tasks. As\u200b a theoretical\u200b model,\u200b Global\u200b Space\u200b requires\u200b that\u200b incomplete\u200b vectors,\u200b e.g.,\u200b the\u200b orthographic vector\u200b by\u200b itself,\u200b can\u200b reliably\u200b be\u200b translated\u200b into\u200b stable\u200b internal\u200b representations.\u200b In\u200b order\u200b to\u200b count as\u200b stable,\u200b these\u200b internal\u200b representations\u200b should\u200b be\u200b translatable\u200b back\u200b to\u200b their\u200b full\u200b explicit representations.\u200b As\u200b there\u200b are\u200b several\u200b models\u200b that\u200b meet\u200b these\u200b criteria,\u200b e.g.,\u200b Autoencoders, modified\u200b SOM,\u200b Variational\u200b Autoencoders,\u200b and\u200b Hopfield\u200b Networks,\u200b empirical\u200b research\u200b needs\u200b to be\u200b carried\u200b out\u200b to\u200b verify\u200b which\u200b of\u200b these\u200b models\u200b has\u200b the\u200b best\u200b fit.For\u200b this\u200b presentation,\u200b we\u200b implement\u200b a version\u200b of\u200b the\u200b model\u200b using\u200b a Modified\u200b Self-Organizing Map\u200b (SOM)\u200b (Kohonen,\u200b 1998),\u200b outfitted\u200b with\u200b a softmax\u200b activation\u200b function.\u200b The\u200b softmax\u200b causes the\u200b map\u200b response\u200b to\u200b be\u200b expressed\u200b as\u200b a distribution\u200b over\u200b a quantized\u200b version\u200b of\u200b the\u200b training\u200b set, which\u200b allows\u200b words\u200b to\u200b be\u200b expressed\u200b as\u200b distributed\u200b representations\u200b over\u200b the\u200b SOM.", "title": "Global Space: A Reconstructive Model of Word Reading"}], [{"authors": "M. Erkan Basar", "abstract": "Learning from past incidents has a great importance for disaster managers. Estimation of the outcomes beforehand can improve preparations for the next incidents. To make this a less labour-intensive task, we aim to automate extracting information from past events. We focus on extracting critical information about flooding events from newspaper articles as our use case. We treat this information extraction task as a sequential labelling task and train a supervised machine learning algorithm, namely Conditional Random Fields [1], to achieve our goal. However, supervised learning requires manually annotated training data, which is very expensive and time-consuming to obtain. To reduce the need for manual annotation, Active Learning [2], a human-in-the-loop method, is explored. We obtain improvement on f1-score up to 25% and observe that Active Learning drastically reduces the effort required by annotation.", "title": "Sequential Labelling with Active Learning to Extract Information about Disasters"}]], [[{"authors": "Antonio Toral", "abstract": "Translation plays a key role in interlingual communication. With the advent of big data there is a growing demand for translation, which cannot be dealt with by human translators alone. An efficient solution is machine-assisted translation, in which machine translation (MT) assists the human translator. However, machine-assisted translation of literary texts such as novels remains challenging because literary translation must preserve not only the meaning of the source text but also its reading experience. As a result, novels are still translated manually. While the perceived wisdom is that MT is of no use for novels, empirical evidence to support this thinking is thin and fragmented and the topic remains largely unexplored. Due to the emergence of a radically new paradigm to automated translation, neural MT (NMT), which has raised the expectations of what MT can offer, we deem it timely to revisit the question of whether MT can be useful to assist with the translation of literary text. This gives us the opportunity to test NMT on a particularly challenging use case, which we hope should contribute to shed further light on the real potential of this newly introduced approach to MT. This work builds upon our recent study (Toral, 2017), in which we built an NMT system tailored to novels for the English-to-Catalan language direction and evaluated it on a set of 12 widely known novels against a system based on the \"classic\" statistical phrase-based approach (PBMT). NMT outperformed PBMT for every novel, resulting overall in 11% relative improvement according to the BLEU evaluation metric. In this work we use this NMT system to assist professional translators. To this end we use the post-editing workflow, so the novel is first translated automatically using an MT system, and subsequently this MT output is edited by translators. Our case study is Warbreaker, a popular fantasy novel originally written in English, which we translate into Catalan. We translated one chapter of the novel with the NMT and PBMT systems built in our previous work. In the post- editing experiment, two professional translators translate subsets of this novel (contiguous passages of 10 sentences) under three conditions: from scratch (the norm in the novel translation industry), post-editing PBMT and post-editing NMT. We record all the keystrokes as well as the time taken to translate each sentence. Based on these measurements we aim to answer two questions regarding (i) temporal and (ii) technical effort, i.e. whether post-editing results in (i) faster translations, and (ii) a smaller number of keystrokes, compared to translating from scratch. Both MT approaches significantly reduce the translation time required, the reduction achieved by NMT being considerably bigger than that by PBMT. In addition, NMT results in a significant reduction in keystrokes, while the reduction brought by PBMT is not statistically significant. We also found out that the distribution of types of keystrokes is very different in post-editing compared to translation from scratch. While the first results in considerably fewer content keywords, it increases notably the amount of navigation and erase keystrokes.", "title": "Post-editing a Novel with Neural Machine Translation"}], [{"authors": "Vincent Vandeghinste, Lyan Verwimp, Joris Pelemans and Patrick Wambacq", "abstract": "n speech translation the first step often consists of automatic speech recognition (ASR), which outputs an unsegmented stream of words. Translating this stream of words, using off-the-shelf machine translation (MT) results in a lower translation quality compared to translating punctuated input. We compare different strategies and techniques to deal with this problem. We compare different punctuation strategies: Preprocessing inserts punctuation in the source text and allows to use an MT system trained on a normal (i.e. punctuated) parallel corpus. Implicit insertion inserts punctuating during the translation, and requires a dedicated MT system. Postprocessing inserts punctuation in the output of the MT system, which is trained on unpunctuated data. We predict punctuation using language modeling techniques, such as n-grams and long short-term memories (LSTM), sequence labeling LSTMs (unidirectional and bidirectional), and monolingual phrase-based, hierarchical and neural MT, and intrinsically evaluate punctuation prediction accuracy. For actual translation phrase-based, hierarchical and neural MT are investigated. We set up an experiment in which we combine all these strategies, punctuation prediction methods and machine translation methods, resulting in 145 experimental conditions. We observe that for punctuation prediction, phrase-based statistical MT and neural MT reach similar results, and are best used as a preprocessing step which is followed by neural MT to perform the actual translation. Implicit punctuation insertion by a dedicated neural MT system, trained on unpunctuated source and punctuated target, yields similar results.", "title": "145 ways to insert punctuation for speech translation"}], [{"authors": "Gideon Maillette de Buy Wenniger, Khalil Sima'An and Andy Way", "abstract": "Elastic-substitution decoding (ESD), first introduced by Chiang (2010), can be important for obtaining good results when applying labels to enrich hierarchical statistical machine translation (SMT). However, an efficient implementation is essential for scalable application. We describe how to achieve this, contributing essential details that were missing in the original exposition. We compare ESD to strict matching and show its superiority for both reordering and syntactic labels. To overcome the sub-optimal performance due to the late evaluation of features marking label substitution types, we increase the diversity of the rules explored during cube pruning initialization with respect to labels their labels. This approach gives significant improvements over basic ESD and performs favorably compared to extending the search by increasing the cube pruning pop-limit. Finally, we look at combining multiple labels. The combination of reordering labels and target-side boundary-tags yields a significant improvement in terms of the word-order sensitive metrics Kendall reordering score and METEOR. This confirms our intuition that the combination of reordering labels and syntactic labels can yield improvements over either label by itself, despite increased sparsity.", "title": "Elastic-substitution decoding for Hierarchical SMT: efficiency, richer search and double labels"}], [{"authors": "Iris Hendrickx, Maarten van Gompel and Antal van Den Bosch", "abstract": "The most common method for evaluating automatic translations is to compare the automatic translation to a manually verified reference translation and to compute a BLEU score that expresses the overlap between the two versions (Papineni et al., 2002). As it is time and effort consuming to construct these manual translations, we aimed to develop a method that can evaluate the automatic translation without any human effort. Such an implicit evaluation technique only focuses on a verification of correct translation of the main concepts and entities in the translation. These experiments are conducted within the TraMOOC project (http://tramooc.eu/) that aims to automatically translate online course material of MOOCs to 11 different languages (Kordoni et al., 2016). In this study we compare two methods for the automatic detection of topics and entities: entity linking as implemented in the Illinois Wikifier (Cheng and Roth, 2013) and BabelFy (Moro et al., 2014) a tool that performs both entity linking and word sense disambiguation. We explain how both entity linking methods work and how we integrated them into a tool for implicit translation evaluation. This tool computes, for a given English source text and its translation in the target language, an Entity translation recall score that expresses the number of correctly translated entities in comparison to the overall number of detected entities. We experimented on samples of educational material in three different languages: Dutch, Portuguese and Russian.", "title": "To Wikify of to Babelfy for Implicit Translation Evaluation?"}]]], "end": false}, {"location": "Keizer Karel Foyer", "start": "17:15", "title": "Drinks", "end": "18:15"}]